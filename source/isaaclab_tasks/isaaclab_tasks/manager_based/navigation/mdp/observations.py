# Copyright (c) 2022-2025, The Isaac Lab Project Developers (https://github.com/isaac-sim/IsaacLab/blob/main/CONTRIBUTORS.md).
# All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

# Copyright (c) 2025, The Nav-Suite Project Developers (https://github.com/leggedrobotics/nav-suite/blob/main/CONTRIBUTORS.md).
# All rights reserved.
#
# SPDX-License-Identifier: Apache-2.0

from __future__ import annotations

import torch
from typing import TYPE_CHECKING

from isaaclab.managers import SceneEntityCfg
from isaaclab.sensors import Camera, RayCasterCamera, TiledCamera

from .actions import NavigationSE2Action

if TYPE_CHECKING:
    from isaaclab.envs import ManagerBasedEnv, ManagerBasedRLEnv


"""
Sensors.
"""


def camera_image(
    env: ManagerBasedEnv,
    sensor_cfg: SceneEntityCfg,
    data_type: str = "distance_to_image_plane",
    flatten: bool = False,
    nan_fill_value: float | None = None,
) -> torch.Tensor:
    """Camera image Observations.

    The camera image observation from the given sensor w.r.t. the asset's root frame.
    Also removes nan/inf values and sets them to the maximum distance of the sensor

    Args:
        env: The environment object.
        sensor_cfg: The name of the sensor.
        data_type: The type of data to extract from the sensor. Default is "distance_to_image_plane".
        flatten: If True, the image will be flattened to 1D. Default is False.
        nan_fill_value: The value to fill nan/inf values with. If None, the maximum distance of the sensor will be used.

    Returns:
        The image data."""
    # extract the used quantities (to enable type-hinting)
    sensor: Camera | RayCasterCamera | TildCamera = env.scene.sensors[sensor_cfg.name]

    img = sensor.data.output[data_type].clone()

    if data_type == "distance_to_image_plane":
        if nan_fill_value is None:
            nan_fill_value = (
                sensor.cfg.max_distance if isinstance(sensor, RayCasterCamera) else sensor.cfg.spawn.clipping_range[1]
            )
        img = torch.nan_to_num(img, nan=nan_fill_value, posinf=nan_fill_value, neginf=0.0)

    # if type torch.uint8, convert to float and scale between 0 and 1
    if img.dtype == torch.uint8:
        img = img.to(torch.float32) / 255.0

    if flatten:
        return img.flatten(start_dim=1)
    else:
        # reorder the image to [BS, C, H, W] if it is not already in that shape
        if img.shape[-1] == 1 or img.shape[-1] == 3:
            img = img.permute(0, 3, 1, 2)

        return img


"""
Actions.
"""


def last_low_level_action(
    env: ManagerBasedRLEnv, action_term: str, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")
) -> torch.Tensor:
    """The last low-level action."""
    action_term: NavigationSE2Action = env.action_manager._terms[action_term]
    return action_term.low_level_actions[:, asset_cfg.joint_ids]


"""
Commands.
"""


def vel_commands(env: ManagerBasedRLEnv, action_term: str) -> torch.Tensor:
    """The velocity command generated by the planner and given as input to the step function"""
    action_term: NavigationSE2Action = env.action_manager._terms[action_term]
    return action_term.processed_actions
