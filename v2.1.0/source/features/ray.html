

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

    <title>Ray Job Dispatch and Tuning &#8212; Isaac Lab Documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/twemoji.css" />
    <link rel="stylesheet" type="text/css" href="/opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages/sphinxcontrib/icon/node_modules/@fortawesome/fontawesome-free/css/all.min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.4cbf315f70debaebd550c87a6162cf0f.min.css" />
  
  <!-- So that users can add custom icons -->
  <script src="../../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/sphinx_highlight.js"></script>
    <script src="https://unpkg.com/twemoji@latest/dist/twemoji.min.js"></script>
    <script src="../../_static/twemoji.js"></script>
    <script src="/opt/hostedtoolcache/Python/3.11.13/x64/lib/python3.11/site-packages/sphinxcontrib/icon/node_modules/@fortawesome/fontawesome-free/js/all.min.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script src="../../_static/design-tabs.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'source/features/ray';</script>
    <link rel="icon" href="../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Reproducibility and Determinism" href="reproducibility.html" />
    <link rel="prev" title="Multi-GPU and Multi-Node Training" href="multi_gpu.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="2.1.0" />
    <meta name="docbuild:last-update" content="Oct 17, 2025"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/NVIDIA-logo-white.png" class="logo__image only-light" alt=""/>
    <img src="../../_static/NVIDIA-logo-black.png" class="logo__image only-dark pst-js-only" alt=""/>
  
  
    <p class="title logo__title">Isaac Lab Documentation</p>
  
</a></div>
        <div class="sidebar-primary-item">
<nav class="bd-links bd-docs-nav">
    <div class="bd-toc-item navbar-nav">
      <ul class="nav bd-sidenav">
        <li class="toctree-l1 has-children" style="display: flex; justify-content: center; align-items: center; flex-direction: column;">
          <div  style ="text-align:center;">
            <label for="version-select" style="font-weight: bold; display: block;">Version</label>
          </div>
          <select id="version-select" class="version-dropdown" style="margin: 0 auto; display: block;" onchange="location = this.value;">
            <option value="../../../release/2.3.0/source/features/ray.html" >release/2.3.0</option>
            <option value="../../../main/source/features/ray.html" >main</option>
            <option value="../../../release/2.1.0/source/features/ray.html" >release/2.1.0</option>
            <option value="../../../release/2.2.0/source/features/ray.html" >release/2.2.0</option>
            <option value="../../../v2.2.1/source/features/ray.html" >v2.2.1</option>
            <option value="../../../v2.2.0/source/features/ray.html" >v2.2.0</option>
            <option value="../../../v2.1.1/source/features/ray.html" >v2.1.1</option>
            <option value="ray.html" selected>v2.1.0</option>
            <option value="../../../v2.0.2/source/features/ray.html" >v2.0.2</option>
            <option value="../../../v2.0.1/source/features/ray.html" >v2.0.1</option>
            <option value="../../../v2.0.0/source/features/ray.html" >v2.0.0</option>
            <option value="../../../v1.4.1/source/features/ray.html" >v1.4.1</option>
            <option value="../../../v1.4.0/source/features/ray.html" >v1.4.0</option>
            <option value="../../../v1.3.0/index.html" >v1.3.0</option>
            <option value="../../../v1.2.0/index.html" >v1.2.0</option>
            <option value="../../../v1.1.0/index.html" >v1.1.0</option>
            <option value="../../../v1.0.0/index.html" >v1.0.0</option>
          </select>
        </li>
      </ul>
    </div>
</nav>
</div>
        <div class="sidebar-primary-item"><ul class="navbar-icon-links"
    aria-label="Quick Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/isaac-sim/IsaacLab" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-square-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://developer.nvidia.com/isaac-sim" title="Isaac Sim" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><img src="https://img.shields.io/badge/IsaacSim-5.1.0-silver.svg" class="icon-link-image" alt="Isaac Sim"/></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://img.shields.io/github/stars/isaac-sim/IsaacLab?color=fedcba" title="Stars" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><img src="https://img.shields.io/github/stars/isaac-sim/IsaacLab?color=fedcba" class="icon-link-image" alt="Stars"/></a>
        </li>
</ul></div>
        <div class="sidebar-primary-item">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../setup/ecosystem.html">Isaac Lab Ecosystem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../setup/quickstart.html">Quickstart Guide</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../setup/installation/index.html">Local Installation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../setup/installation/pip_installation.html">Pip installation (recommended for Ubuntu 22.04 and Windows)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../setup/installation/binaries_installation.html">Binary installation (recommended for Ubuntu 20.04)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../setup/installation/isaaclab_pip_installation.html">Advanced installation (Isaac Lab pip)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../setup/installation/asset_caching.html">Asset caching</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../setup/installation/cloud_installation.html">Running Isaac Lab in the Cloud</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Overview</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../overview/developer-guide/index.html">Developer’s Guide</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../overview/developer-guide/vs_code.html">Setting up Visual Studio Code</a></li>

<li class="toctree-l2"><a class="reference internal" href="../overview/developer-guide/repo_structure.html">Repository organization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../overview/developer-guide/development.html">Extension Development</a></li>
<li class="toctree-l2"><a class="reference internal" href="../overview/developer-guide/template.html">Build your Own Project or Task</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../overview/core-concepts/index.html">Core Concepts</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../overview/core-concepts/task_workflows.html">Task Design Workflows</a></li>
<li class="toctree-l2"><a class="reference internal" href="../overview/core-concepts/actuators.html">Actuators</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../overview/core-concepts/sensors/index.html">Sensors</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../overview/core-concepts/sensors/camera.html">Camera</a></li>
<li class="toctree-l3"><a class="reference internal" href="../overview/core-concepts/sensors/contact_sensor.html">Contact Sensor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../overview/core-concepts/sensors/frame_transformer.html">Frame Transformer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../overview/core-concepts/sensors/imu.html">Inertial Measurement Unit (IMU)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../overview/core-concepts/sensors/ray_caster.html">Ray Caster</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../overview/core-concepts/motion_generators.html">Motion Generators</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../overview/environments.html">Available Environments</a></li>

<li class="toctree-l1 has-children"><a class="reference internal" href="../overview/reinforcement-learning/index.html">Reinforcement Learning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../overview/reinforcement-learning/rl_existing_scripts.html">Reinforcement Learning Scripts</a></li>
<li class="toctree-l2"><a class="reference internal" href="../overview/reinforcement-learning/rl_frameworks.html">Reinforcement Learning Library Comparison</a></li>
<li class="toctree-l2"><a class="reference internal" href="../overview/reinforcement-learning/performance_benchmarks.html">Performance Benchmarks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../overview/reinforcement-learning/training_guide.html">Debugging and Training Guide</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../overview/teleop_imitation.html">Teleoperation and Imitation Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../overview/showroom.html">Showroom Demos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../overview/simple_agents.html">Simple Agents</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Features</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="hydra.html">Hydra Configuration System</a></li>
<li class="toctree-l1"><a class="reference internal" href="multi_gpu.html">Multi-GPU and Multi-Node Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../overview/core-concepts/sensors/camera.html">Tiled Rendering</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Ray Job Dispatch and Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="reproducibility.html">Reproducibility and Determinism</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Resources</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../tutorials/index.html">Tutorials</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/00_sim/create_empty.html">Creating an empty scene</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/00_sim/spawn_prims.html">Spawning prims into the scene</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/00_sim/launch_app.html">Deep-dive into AppLauncher</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/01_assets/add_new_robot.html">Adding a New Robot to Isaac Lab</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/01_assets/run_rigid_object.html">Interacting with a rigid object</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/01_assets/run_articulation.html">Interacting with an articulation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/01_assets/run_deformable_object.html">Interacting with a deformable object</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/02_scene/create_scene.html">Using the Interactive Scene</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/03_envs/create_manager_base_env.html">Creating a Manager-Based Base Environment</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/03_envs/create_manager_rl_env.html">Creating a Manager-Based RL Environment</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/03_envs/create_direct_rl_env.html">Creating a Direct Workflow RL Environment</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/03_envs/register_rl_env_gym.html">Registering an Environment</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/03_envs/run_rl_training.html">Training with an RL Agent</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/03_envs/modify_direct_rl_env.html">Modifying an existing Direct RL Environment</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/03_envs/policy_inference_in_usd.html">Policy Inference in USD Environment</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/04_sensors/add_sensors_on_robot.html">Adding sensors on a robot</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/05_controllers/run_diff_ik.html">Using a task-space controller</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/05_controllers/run_osc.html">Using an operational space controller</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../how-to/index.html">How-to Guides</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../how-to/import_new_asset.html">Importing a New Asset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../how-to/write_articulation_cfg.html">Writing an Asset Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../how-to/make_fixed_prim.html">Making a physics prim fixed in the simulation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../how-to/multi_asset_spawning.html">Spawning Multiple Assets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../how-to/save_camera_output.html">Saving rendered images and 3D re-projection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../how-to/estimate_how_many_cameras_can_run.html">Find How Many/What Cameras You Should Train With</a></li>
<li class="toctree-l2"><a class="reference internal" href="../how-to/configure_rendering.html">Configuring Rendering Settings</a></li>
<li class="toctree-l2"><a class="reference internal" href="../how-to/draw_markers.html">Creating Visualization Markers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../how-to/wrap_rl_env.html">Wrapping environments</a></li>
<li class="toctree-l2"><a class="reference internal" href="../how-to/add_own_library.html">Adding your own learning library</a></li>
<li class="toctree-l2"><a class="reference internal" href="../how-to/record_animation.html">Recording Animations of Simulations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../how-to/record_video.html">Recording video clips during training</a></li>
<li class="toctree-l2"><a class="reference internal" href="../how-to/master_omniverse.html">Mastering Omniverse for Robotics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../how-to/cloudxr_teleoperation.html">Setting up CloudXR Teleoperation</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../deployment/index.html">Container Deployment</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../deployment/docker.html">Docker Guide</a></li>
<li class="toctree-l2"><a class="reference internal" href="../deployment/cluster.html">Cluster Guide</a></li>
<li class="toctree-l2"><a class="reference internal" href="../deployment/run_docker_example.html">Running an example with Docker</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../policy_deployment/index.html">Sim2Real Deployment of Policies Trained in Isaac Lab</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../policy_deployment/00_hover/hover_policy.html">Training &amp; Deploying HOVER Policy</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Migration Guides</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../migration/migrating_from_isaacgymenvs.html">From IsaacGymEnvs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration/migrating_from_omniisaacgymenvs.html">From OmniIsaacGymEnvs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration/migrating_from_orbit.html">From Orbit</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Source API</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../api/index.html">API Reference</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../api/lab/isaaclab.app.html">isaaclab.app</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/lab/isaaclab.actuators.html">isaaclab.actuators</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/lab/isaaclab.assets.html">isaaclab.assets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/lab/isaaclab.controllers.html">isaaclab.controllers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/lab/isaaclab.devices.html">isaaclab.devices</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/lab/isaaclab.envs.html">isaaclab.envs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/lab/isaaclab.managers.html">isaaclab.managers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/lab/isaaclab.markers.html">isaaclab.markers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/lab/isaaclab.scene.html">isaaclab.scene</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/lab/isaaclab.sensors.html">isaaclab.sensors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/lab/isaaclab.sim.html">isaaclab.sim</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/lab/isaaclab.terrains.html">isaaclab.terrains</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/lab/isaaclab.utils.html">isaaclab.utils</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/lab/isaaclab.envs.mdp.html">isaaclab.envs.mdp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/lab/isaaclab.envs.ui.html">isaaclab.envs.ui</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/lab/isaaclab.sensors.patterns.html">isaaclab.sensors.patterns</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/lab/isaaclab.sim.converters.html">isaaclab.sim.converters</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/lab/isaaclab.sim.schemas.html">isaaclab.sim.schemas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/lab/isaaclab.sim.spawners.html">isaaclab.sim.spawners</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/lab_rl/isaaclab_rl.html">isaaclab_rl</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/lab_tasks/isaaclab_tasks.utils.html">isaaclab_tasks.utils</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">References</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../refs/reference_architecture/index.html">Reference Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../refs/additional_resources.html">Additional Resources</a></li>
<li class="toctree-l1"><a class="reference internal" href="../refs/contributing.html">Contribution Guidelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../refs/troubleshooting.html">Tricks and Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../refs/migration.html">Migration Guide (Isaac Sim)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../refs/issues.html">Known Issues</a></li>
<li class="toctree-l1"><a class="reference internal" href="../refs/release_notes.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../refs/changelog.html">Extensions Changelog</a></li>
<li class="toctree-l1"><a class="reference internal" href="../refs/license.html">License</a></li>
<li class="toctree-l1"><a class="reference internal" href="../refs/bibliography.html">Bibliography</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Project Links</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference external" href="https://github.com/isaac-sim/IsaacLab">GitHub</a></li>
<li class="toctree-l1"><a class="reference external" href="https://docs.isaacsim.omniverse.nvidia.com/latest/index.html">NVIDIA Isaac Sim</a></li>
<li class="toctree-l1"><a class="reference external" href="https://nvidia-omniverse.github.io/PhysX/physx/5.4.1/index.html">NVIDIA PhysX</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/isaac-sim/IsaacLab" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/isaac-sim/IsaacLab/edit/main/docs/source/features/ray.rst" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/isaac-sim/IsaacLab/issues/new?title=Issue%20on%20page%20%2Fsource/features/ray.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/source/features/ray.rst" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.rst</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Ray Job Dispatch and Tuning</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#docker-based-local-quickstart">Docker-based Local Quickstart</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#remote-clusters">Remote Clusters</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#kuberay-setup">KubeRay Setup</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ray-clusters-without-kubernetes-setup">Ray Clusters (Without Kubernetes) Setup</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#shared-steps-between-kuberay-and-pure-ray-part-i">Shared Steps Between KubeRay and Pure Ray Part I</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#kuberay-clusters-only">KubeRay Clusters Only</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ray-clusters-only-without-kubernetes">Ray Clusters Only (Without Kubernetes)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dispatching-steps-shared-between-kuberay-and-pure-ray-part-ii">Dispatching Steps Shared Between KubeRay and Pure Ray Part II</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#kubernetes-cluster-cleanup">Kubernetes Cluster Cleanup</a></li>
</ul>
</li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="ray-job-dispatch-and-tuning">
<h1>Ray Job Dispatch and Tuning<a class="headerlink" href="#ray-job-dispatch-and-tuning" title="Permalink to this heading">#</a></h1>
<p>Isaac Lab supports <a class="reference external" href="https://docs.ray.io/en/latest/index.html">Ray</a> for streamlining dispatching multiple training jobs (in parallel and in series),
and hyperparameter tuning, both on local and remote configurations.</p>
<p>This <a class="reference external" href="https://youtu.be/z7MDgSga2Ho?feature=shared">independent community contributed walkthrough video</a>
demonstrates some of the core functionality of the Ray integration covered in this overview. Although there may be some
differences in the codebase (such as file names being shortened) since the creation of the video,
the general workflow is the same.</p>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>This functionality is experimental, and has been tested only on Linux.</p>
</div>
<nav class="contents local" id="table-of-contents">
<p class="topic-title">Table of Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#overview" id="id2">Overview</a></p></li>
<li><p><a class="reference internal" href="#docker-based-local-quickstart" id="id3">Docker-based Local Quickstart</a></p></li>
<li><p><a class="reference internal" href="#remote-clusters" id="id4">Remote Clusters</a></p>
<ul>
<li><p><a class="reference internal" href="#kuberay-setup" id="id5">KubeRay Setup</a></p></li>
<li><p><a class="reference internal" href="#ray-clusters-without-kubernetes-setup" id="id6">Ray Clusters (Without Kubernetes) Setup</a></p></li>
<li><p><a class="reference internal" href="#shared-steps-between-kuberay-and-pure-ray-part-i" id="id7">Shared Steps Between KubeRay and Pure Ray Part I</a></p></li>
<li><p><a class="reference internal" href="#kuberay-clusters-only" id="id8">KubeRay Clusters Only</a></p></li>
<li><p><a class="reference internal" href="#ray-clusters-only-without-kubernetes" id="id9">Ray Clusters Only (Without Kubernetes)</a></p></li>
<li><p><a class="reference internal" href="#dispatching-steps-shared-between-kuberay-and-pure-ray-part-ii" id="id10">Dispatching Steps Shared Between KubeRay and Pure Ray Part II</a></p>
<ul>
<li><p><a class="reference internal" href="#kubernetes-cluster-cleanup" id="id11">Kubernetes Cluster Cleanup</a></p></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
<section id="overview">
<h2><a class="toc-backref" href="#id2" role="doc-backlink">Overview</a><a class="headerlink" href="#overview" title="Permalink to this heading">#</a></h2>
<p>The Ray integration is useful for the following.</p>
<ul class="simple">
<li><p>Dispatching several training jobs in parallel or sequentially with minimal interaction.</p></li>
<li><p>Tuning hyperparameters; in parallel or sequentially with support for multiple GPUs and/or multiple GPU Nodes.</p></li>
<li><p>Using the same training setup everywhere (on cloud and local) with minimal overhead.</p></li>
<li><p>Resource Isolation for training jobs (resource-wrapped jobs).</p></li>
</ul>
<p>The core functionality of the Ray workflow consists of two main scripts that enable the orchestration
of resource-wrapped and tuning aggregate jobs. In resource-wrapped aggregate jobs, each sub-job and its
resource requirements are defined manually, enabling resource isolation.
For tuning aggregate jobs, individual jobs are generated automatically based on a hyperparameter
sweep configuration.</p>
<p>Both resource-wrapped and tuning aggregate jobs dispatch individual jobs to a designated Ray
cluster, which leverages the cluster’s resources (e.g., a single workstation node or multiple nodes)
to execute these jobs with workers in parallel and/or sequentially.</p>
<p>By default, jobs use all available resources on each available GPU-enabled node for each sub-job worker. This can be changed through
specifying the <code class="docutils literal notranslate"><span class="pre">--num_workers</span></code> argument for resource-wrapped jobs, or <code class="docutils literal notranslate"><span class="pre">--num_workers_per_node</span></code>
for tuning jobs, which is especially critical for parallel aggregate
job processing on local/virtual multi-GPU machines. Tuning jobs assume homogeneous node resource composition for nodes with GPUs.</p>
<p>The two following files contain the core functionality of the Ray integration.</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-icon"><svg version="1.1" width="1.0em" height="1.0em" class="sd-octicon sd-octicon-code" viewBox="0 0 16 16" aria-hidden="true"><path d="m11.28 3.22 4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734L13.94 8l-3.72-3.72a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215Zm-6.56 0a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042L2.06 8l3.72 3.72a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L.47 8.53a.75.75 0 0 1 0-1.06Z"></path></svg></span><span class="sd-summary-text">scripts/reinforcement_learning/ray/wrap_resources.py</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Copyright (c) 2022-2025, The Isaac Lab Project Developers.</span>
<span class="c1"># All rights reserved.</span>
<span class="c1">#</span>
<span class="c1"># SPDX-License-Identifier: BSD-3-Clause</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">argparse</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">ray</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">util</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.util.scheduling_strategies</span><span class="w"> </span><span class="kn">import</span> <span class="n">NodeAffinitySchedulingStrategy</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">This script dispatches sub-job(s) (individual jobs, use :file:`tuner.py` for tuning jobs)</span>
<span class="hll"><span class="sd">to worker(s) on GPU-enabled node(s) of a specific cluster as part of an resource-wrapped aggregate</span>
</span><span class="hll"><span class="sd">job. If no desired compute resources for each sub-job are specified,</span>
</span><span class="hll"><span class="sd">this script creates one worker per available node for each node with GPU(s) in the cluster.</span>
</span><span class="hll"><span class="sd">If the desired resources for each sub-job is specified,</span>
</span><span class="hll"><span class="sd">the maximum number of workers possible with the desired resources are created for each node</span>
</span><span class="hll"><span class="sd">with GPU(s) in the cluster. It is also possible to split available node resources for each node</span>
</span><span class="hll"><span class="sd">into the desired number of workers with the ``--num_workers`` flag, to be able to easily</span>
</span><span class="hll"><span class="sd">parallelize sub-jobs on multi-GPU nodes. Due to Isaac Lab requiring a GPU,</span>
</span><span class="hll"><span class="sd">this ignores all CPU only nodes such as loggers.</span>
</span><span class="hll">
</span><span class="hll"><span class="sd">Sub-jobs are matched with node(s) in a cluster via the following relation:</span>
</span><span class="hll"><span class="sd">sorted_nodes = Node sorted by descending GPUs, then descending CPUs, then descending RAM, then node ID</span>
</span><span class="hll"><span class="sd">node_submitted_to = sorted_nodes[job_index % total_node_count]</span>
</span><span class="hll">
</span><span class="hll"><span class="sd">To check the ordering of sorted nodes, supply the ``--test`` argument and run the script.</span>
</span><span class="hll">
</span><span class="hll"><span class="sd">Sub-jobs are separated by the + delimiter. The ``--sub_jobs`` argument must be the last</span>
</span><span class="hll"><span class="sd">argument supplied to the script.</span>
</span><span class="hll">
</span><span class="hll"><span class="sd">If there is more than one available worker, and more than one sub-job,</span>
</span><span class="hll"><span class="sd">sub-jobs will be executed in parallel. If there are more sub-jobs than workers, sub-jobs will</span>
</span><span class="hll"><span class="sd">be dispatched to workers as they become available. There is no limit on the number</span>
</span><span class="hll"><span class="sd">of sub-jobs that can be near-simultaneously submitted.</span>
</span><span class="hll">
</span><span class="hll"><span class="sd">This script is meant to be executed on a Ray cluster head node as an aggregate cluster job.</span>
</span><span class="hll"><span class="sd">To submit aggregate cluster jobs such as this script to one or more remote clusters,</span>
</span><span class="hll"><span class="sd">see :file:`../submit_isaac_ray_job.py`.</span>
</span><span class="hll">
</span><span class="hll"><span class="sd">KubeRay clusters on Google GKE can be created with :file:`../launch.py`</span>
</span><span class="hll">
</span><span class="hll"><span class="sd">Usage:</span>
</span><span class="hll">
</span><span class="hll"><span class="sd">.. code-block:: bash</span>
</span><span class="hll"><span class="sd">    # **Ensure that sub-jobs are separated by the ``+`` delimiter.**</span>
</span><span class="hll"><span class="sd">    # Generic Templates-----------------------------------</span>
</span><span class="hll"><span class="sd">    ./isaaclab.sh -p scripts/reinforcement_learning/ray/wrap_resources.py -h</span>
</span><span class="hll"><span class="sd">    # No resource isolation; no parallelization:</span>
</span><span class="hll"><span class="sd">    ./isaaclab.sh -p scripts/reinforcement_learning/ray/wrap_resources.py</span>
</span><span class="hll"><span class="sd">    --sub_jobs &lt;JOB0&gt;+&lt;JOB1&gt;+&lt;JOB2&gt;</span>
</span><span class="hll"><span class="sd">    # Automatic Resource Isolation; Example A: needed for parallelization</span>
</span><span class="hll"><span class="sd">    ./isaaclab.sh -p scripts/reinforcement_learning/ray/wrap_resources.py \</span>
</span><span class="hll"><span class="sd">    --num_workers &lt;NUM_TO_DIVIDE_TOTAL_RESOURCES_BY&gt; \</span>
</span><span class="hll"><span class="sd">    --sub_jobs &lt;JOB0&gt;+&lt;JOB1&gt;</span>
</span><span class="hll"><span class="sd">    # Manual Resource Isolation; Example B:  needed for parallelization</span>
</span><span class="hll"><span class="sd">    ./isaaclab.sh -p scripts/reinforcement_learning/ray/wrap_resources.py --num_cpu_per_worker &lt;CPU&gt; \</span>
</span><span class="hll"><span class="sd">    --gpu_per_worker &lt;GPU&gt; --ram_gb_per_worker &lt;RAM&gt; --sub_jobs &lt;JOB0&gt;+&lt;JOB1&gt;</span>
</span><span class="hll"><span class="sd">    # Manual Resource Isolation; Example C: Needed for parallelization, for heterogeneous workloads</span>
</span><span class="hll"><span class="sd">    ./isaaclab.sh -p scripts/reinforcement_learning/ray/wrap_resources.py --num_cpu_per_worker &lt;CPU&gt; \</span>
</span><span class="hll"><span class="sd">    --gpu_per_worker &lt;GPU1&gt; &lt;GPU2&gt; --ram_gb_per_worker &lt;RAM&gt; --sub_jobs &lt;JOB0&gt;+&lt;JOB1&gt;</span>
</span><span class="hll"><span class="sd">    # to see all arguments</span>
</span><span class="hll"><span class="sd">    ./isaaclab.sh -p scripts/reinforcement_learning/ray/wrap_resources.py -h</span>
</span><span class="hll"><span class="sd">&quot;&quot;&quot;</span>
</span><span class="hll">
</span>
<span class="k">def</span><span class="w"> </span><span class="nf">wrap_resources_to_jobs</span><span class="p">(</span><span class="n">jobs</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">args</span><span class="p">:</span> <span class="n">argparse</span><span class="o">.</span><span class="n">Namespace</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Provided a list of jobs, dispatch jobs to one worker per available node,</span>
<span class="sd">    unless otherwise specified by resource constraints.</span>

<span class="sd">    Args:</span>
<span class="sd">        jobs: bash commands to execute on a Ray cluster</span>
<span class="sd">        args: The arguments for resource allocation</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">ray</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">():</span>
        <span class="n">ray</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">address</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">ray_address</span><span class="p">,</span> <span class="n">log_to_driver</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">job_results</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">gpu_node_resources</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">get_gpu_node_resources</span><span class="p">(</span><span class="n">include_id</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">include_gb_ram</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">any</span><span class="p">([</span><span class="n">args</span><span class="o">.</span><span class="n">gpu_per_worker</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">cpu_per_worker</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">ram_gb_per_worker</span><span class="p">])</span> <span class="ow">and</span> <span class="n">args</span><span class="o">.</span><span class="n">num_workers</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Either specify only num_workers or only granular resources(GPU,CPU,RAM_GB).&quot;</span><span class="p">)</span>

    <span class="n">num_nodes</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">gpu_node_resources</span><span class="p">)</span>
    <span class="c1"># Populate arguments</span>
    <span class="n">formatted_node_resources</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;gpu_per_worker&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">gpu_node_resources</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s2">&quot;GPU&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_nodes</span><span class="p">)],</span>
        <span class="s2">&quot;cpu_per_worker&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">gpu_node_resources</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s2">&quot;CPU&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_nodes</span><span class="p">)],</span>
        <span class="s2">&quot;ram_gb_per_worker&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">gpu_node_resources</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s2">&quot;ram_gb&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_nodes</span><span class="p">)],</span>
        <span class="s2">&quot;num_workers&quot;</span><span class="p">:</span> <span class="n">args</span><span class="o">.</span><span class="n">num_workers</span><span class="p">,</span>  <span class="c1"># By default, 1 worker por node</span>
    <span class="p">}</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">fill_in_missing_resources</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">resources</span><span class="o">=</span><span class="n">formatted_node_resources</span><span class="p">,</span> <span class="n">policy</span><span class="o">=</span><span class="nb">min</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[INFO]: Number of GPU nodes found: </span><span class="si">{</span><span class="n">num_nodes</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">test</span><span class="p">:</span>
        <span class="n">jobs</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;nvidia-smi&quot;</span><span class="p">]</span> <span class="o">*</span> <span class="n">num_nodes</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">job</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">jobs</span><span class="p">):</span>
        <span class="n">gpu_node</span> <span class="o">=</span> <span class="n">gpu_node_resources</span><span class="p">[</span><span class="n">i</span> <span class="o">%</span> <span class="n">num_nodes</span><span class="p">]</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[INFO]: Submitting job </span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2"> of </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">jobs</span><span class="p">)</span><span class="si">}</span><span class="s2"> with job &#39;</span><span class="si">{</span><span class="n">job</span><span class="si">}</span><span class="s2">&#39; to node </span><span class="si">{</span><span class="n">gpu_node</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;[INFO]: Resource parameters: GPU: </span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">gpu_per_worker</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="sa">f</span><span class="s2">&quot; CPU: </span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">cpu_per_worker</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2"> RAM </span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">ram_gb_per_worker</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[INFO] For the node parameters, creating </span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">num_workers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2"> workers&quot;</span><span class="p">)</span>
        <span class="n">num_gpus</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">gpu_per_worker</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">/</span> <span class="n">args</span><span class="o">.</span><span class="n">num_workers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">num_cpus</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">cpu_per_worker</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">/</span> <span class="n">args</span><span class="o">.</span><span class="n">num_workers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">memory</span> <span class="o">=</span> <span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">ram_gb_per_worker</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span> <span class="o">/</span> <span class="n">args</span><span class="o">.</span><span class="n">num_workers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[INFO]: Requesting </span><span class="si">{</span><span class="n">num_gpus</span><span class="si">=}</span><span class="s2"> </span><span class="si">{</span><span class="n">num_cpus</span><span class="si">=}</span><span class="s2"> </span><span class="si">{</span><span class="n">memory</span><span class="si">=}</span><span class="s2"> id=</span><span class="si">{</span><span class="n">gpu_node</span><span class="p">[</span><span class="s1">&#39;id&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">job</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">remote_execute_job</span><span class="o">.</span><span class="n">options</span><span class="p">(</span>
            <span class="n">num_gpus</span><span class="o">=</span><span class="n">num_gpus</span><span class="p">,</span>
            <span class="n">num_cpus</span><span class="o">=</span><span class="n">num_cpus</span><span class="p">,</span>
            <span class="n">memory</span><span class="o">=</span><span class="n">memory</span><span class="p">,</span>
            <span class="n">scheduling_strategy</span><span class="o">=</span><span class="n">NodeAffinitySchedulingStrategy</span><span class="p">(</span><span class="n">gpu_node</span><span class="p">[</span><span class="s2">&quot;id&quot;</span><span class="p">],</span> <span class="n">soft</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
        <span class="p">)</span><span class="o">.</span><span class="n">remote</span><span class="p">(</span><span class="n">job</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Job </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">test</span><span class="p">)</span>
        <span class="n">job_results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">job</span><span class="p">)</span>

    <span class="n">results</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">job_results</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">result</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">results</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[INFO]: Job </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2"> result: </span><span class="si">{</span><span class="n">result</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;[INFO]: All jobs completed.&quot;</span><span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">&quot;Submit multiple jobs with optional GPU testing.&quot;</span><span class="p">)</span>
    <span class="n">parser</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">add_resource_arguments</span><span class="p">(</span><span class="n">arg_parser</span><span class="o">=</span><span class="n">parser</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--ray_address&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;the Ray address.&quot;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--test&quot;</span><span class="p">,</span>
        <span class="n">action</span><span class="o">=</span><span class="s2">&quot;store_true&quot;</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="p">(</span>
            <span class="s2">&quot;Run nvidia-smi test instead of the arbitrary job,&quot;</span>
            <span class="s2">&quot;can use as a sanity check prior to any jobs to check &quot;</span>
            <span class="s2">&quot;that GPU resources are correctly isolated.&quot;</span>
        <span class="p">),</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--sub_jobs&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
        <span class="n">nargs</span><span class="o">=</span><span class="n">argparse</span><span class="o">.</span><span class="n">REMAINDER</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;This should be last wrapper argument. Jobs separated by the + delimiter to run on a cluster.&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">sub_jobs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">jobs</span> <span class="o">=</span> <span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">sub_jobs</span><span class="p">)</span>
        <span class="n">formatted_jobs</span> <span class="o">=</span> <span class="n">jobs</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;+&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">formatted_jobs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[INFO]: Isaac Ray Wrapper received jobs </span><span class="si">{</span><span class="n">formatted_jobs</span><span class="si">=}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">wrap_resources_to_jobs</span><span class="p">(</span><span class="n">jobs</span><span class="o">=</span><span class="n">formatted_jobs</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details><details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-icon"><svg version="1.1" width="1.0em" height="1.0em" class="sd-octicon sd-octicon-code" viewBox="0 0 16 16" aria-hidden="true"><path d="m11.28 3.22 4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734L13.94 8l-3.72-3.72a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215Zm-6.56 0a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042L2.06 8l3.72 3.72a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L.47 8.53a.75.75 0 0 1 0-1.06Z"></path></svg></span><span class="sd-summary-text">scripts/reinforcement_learning/ray/tuner.py</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Copyright (c) 2022-2025, The Isaac Lab Project Developers.</span>
<span class="c1"># All rights reserved.</span>
<span class="c1">#</span>
<span class="c1"># SPDX-License-Identifier: BSD-3-Clause</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">argparse</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">importlib.util</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sys</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">time</span><span class="w"> </span><span class="kn">import</span> <span class="n">sleep</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">ray</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">util</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray</span><span class="w"> </span><span class="kn">import</span> <span class="n">air</span><span class="p">,</span> <span class="n">tune</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.tune.search.optuna</span><span class="w"> </span><span class="kn">import</span> <span class="n">OptunaSearch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.tune.search.repeater</span><span class="w"> </span><span class="kn">import</span> <span class="n">Repeater</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="hll"><span class="sd">This script breaks down an aggregate tuning job, as defined by a hyperparameter sweep configuration,</span>
</span><span class="hll"><span class="sd">into individual jobs (shell commands) to run on the GPU-enabled nodes of the cluster.</span>
</span><span class="hll"><span class="sd">By default, one worker is created for each GPU-enabled node in the cluster for each individual job.</span>
</span><span class="hll"><span class="sd">To use more than one worker per node (likely the case for multi-GPU machines), supply the</span>
</span><span class="hll"><span class="sd">num_workers_per_node argument.</span>
</span><span class="hll">
</span><span class="hll"><span class="sd">Each hyperparameter sweep configuration should include the workflow,</span>
</span><span class="hll"><span class="sd">runner arguments, and hydra arguments to vary.</span>
</span><span class="hll">
</span><span class="hll"><span class="sd">This assumes that all workers in a cluster are homogeneous. For heterogeneous workloads,</span>
</span><span class="hll"><span class="sd">create several heterogeneous clusters (with homogeneous nodes in each cluster),</span>
</span><span class="hll"><span class="sd">then submit several overall-cluster jobs with :file:`../submit_job.py`.</span>
</span><span class="hll"><span class="sd">KubeRay clusters on Google GKE can be created with :file:`../launch.py`</span>
</span><span class="hll">
</span><span class="hll"><span class="sd">To report tune metrics on clusters, a running MLFlow server with a known URI that the cluster has</span>
</span><span class="hll"><span class="sd">access to is required. For KubeRay clusters configured with :file:`../launch.py`, this is included</span>
</span><span class="hll"><span class="sd">automatically, and can be easily found with with :file:`grok_cluster_with_kubectl.py`</span>
</span><span class="hll">
</span><span class="hll"><span class="sd">Usage:</span>
</span><span class="hll">
</span><span class="hll"><span class="sd">.. code-block:: bash</span>
</span><span class="hll">
</span><span class="hll"><span class="sd">    ./isaaclab.sh -p scripts/reinforcement_learning/ray/tuner.py -h</span>
</span><span class="hll">
</span><span class="hll"><span class="sd">    # Examples</span>
</span><span class="hll"><span class="sd">    # Local</span>
</span><span class="hll"><span class="sd">    ./isaaclab.sh -p scripts/reinforcement_learning/ray/tuner.py --run_mode local \</span>
</span><span class="hll"><span class="sd">    --cfg_file scripts/reinforcement_learning/ray/hyperparameter_tuning/vision_cartpole_cfg.py \</span>
</span><span class="hll"><span class="sd">    --cfg_class CartpoleTheiaJobCfg</span>
</span><span class="hll"><span class="sd">    # Remote (run grok cluster or create config file mentioned in :file:`submit_job.py`)</span>
</span><span class="hll"><span class="sd">    ./isaaclab.sh -p scripts/reinforcement_learning/ray/submit_job.py \</span>
</span><span class="hll"><span class="sd">    --aggregate_jobs tuner.py \</span>
</span><span class="hll"><span class="sd">    --cfg_file hyperparameter_tuning/vision_cartpole_cfg.py \</span>
</span><span class="hll"><span class="sd">    --cfg_class CartpoleTheiaJobCfg --mlflow_uri &lt;MLFLOW_URI_FROM_GROK_OR_MANUAL&gt;</span>
</span><span class="hll">
</span><span class="hll"><span class="sd">&quot;&quot;&quot;</span>
</span>
<span class="n">DOCKER_PREFIX</span> <span class="o">=</span> <span class="s2">&quot;/workspace/isaaclab/&quot;</span>
<span class="n">BASE_DIR</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">expanduser</span><span class="p">(</span><span class="s2">&quot;~&quot;</span><span class="p">)</span>
<span class="n">PYTHON_EXEC</span> <span class="o">=</span> <span class="s2">&quot;./isaaclab.sh -p&quot;</span>
<span class="n">WORKFLOW</span> <span class="o">=</span> <span class="s2">&quot;scripts/reinforcement_learning/rl_games/train.py&quot;</span>
<span class="n">NUM_WORKERS_PER_NODE</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># needed for local parallelism</span>


<span class="k">class</span><span class="w"> </span><span class="nc">IsaacLabTuneTrainable</span><span class="p">(</span><span class="n">tune</span><span class="o">.</span><span class="n">Trainable</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;The Isaac Lab Ray Tune Trainable.</span>
<span class="sd">    This class uses the standalone workflows to start jobs, along with the hydra integration.</span>
<span class="sd">    This class achieves Ray-based logging through reading the tensorboard logs from</span>
<span class="sd">    the standalone workflows. This depends on a config generated in the format of</span>
<span class="sd">    :class:`JobCfg`</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">setup</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get the invocation command, return quick for easy scheduling.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">invoke_cmd</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">get_invocation_command_from_cfg</span><span class="p">(</span><span class="n">cfg</span><span class="o">=</span><span class="n">config</span><span class="p">,</span> <span class="n">python_cmd</span><span class="o">=</span><span class="n">PYTHON_EXEC</span><span class="p">,</span> <span class="n">workflow</span><span class="o">=</span><span class="n">WORKFLOW</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[INFO]: Recovered invocation with </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">invoke_cmd</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">experiment</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">reset_config</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_config</span><span class="p">:</span> <span class="nb">dict</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Allow environments to be re-used by fetching a new invocation command&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">setup</span><span class="p">(</span><span class="n">new_config</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">True</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">experiment</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># start experiment</span>
            <span class="c1"># When including this as first step instead of setup, experiments get scheduled faster</span>
            <span class="c1"># Don&#39;t want to block the scheduler while the experiment spins up</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[INFO]: Invoking experiment as first step with </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">invoke_cmd</span><span class="si">}</span><span class="s2">...&quot;</span><span class="p">)</span>
            <span class="n">experiment</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">execute_job</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">invoke_cmd</span><span class="p">,</span>
                <span class="n">identifier_string</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span>
                <span class="n">extract_experiment</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">persistent_dir</span><span class="o">=</span><span class="n">BASE_DIR</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">experiment</span> <span class="o">=</span> <span class="n">experiment</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[INFO]: Tuner recovered experiment info </span><span class="si">{</span><span class="n">experiment</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">proc</span> <span class="o">=</span> <span class="n">experiment</span><span class="p">[</span><span class="s2">&quot;proc&quot;</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">experiment_name</span> <span class="o">=</span> <span class="n">experiment</span><span class="p">[</span><span class="s2">&quot;experiment_name&quot;</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">isaac_logdir</span> <span class="o">=</span> <span class="n">experiment</span><span class="p">[</span><span class="s2">&quot;logdir&quot;</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">tensorboard_logdir</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">isaac_logdir</span> <span class="o">+</span> <span class="s2">&quot;/&quot;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">experiment_name</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">done</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">proc</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Could not start trial.&quot;</span><span class="p">)</span>
        <span class="n">proc_status</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">proc</span><span class="o">.</span><span class="n">poll</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">proc_status</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># process finished, signal finish</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;done&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[INFO]: Process finished with </span><span class="si">{</span><span class="n">proc_status</span><span class="si">}</span><span class="s2">, returning...&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>  <span class="c1"># wait until the logs are ready or fresh</span>
            <span class="n">data</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">load_tensorboard_logs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tensorboard_logdir</span><span class="p">)</span>

            <span class="k">while</span> <span class="n">data</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">data</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">load_tensorboard_logs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tensorboard_logdir</span><span class="p">)</span>
                <span class="n">sleep</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># Lazy report metrics to avoid performance overhead</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">while</span> <span class="n">util</span><span class="o">.</span><span class="n">_dicts_equal</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">):</span>
                    <span class="n">data</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">load_tensorboard_logs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tensorboard_logdir</span><span class="p">)</span>
                    <span class="n">sleep</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># Lazy report metrics to avoid performance overhead</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;done&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">default_resource_request</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;How many resources each trainable uses. Assumes homogeneous resources across gpu nodes,</span>
<span class="sd">        and that each trainable is meant for one node, where it uses all available resources.&quot;&quot;&quot;</span>
        <span class="n">resources</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">get_gpu_node_resources</span><span class="p">(</span><span class="n">one_node_only</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">NUM_WORKERS_PER_NODE</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;[WARNING]: Splitting node into more than one worker&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tune</span><span class="o">.</span><span class="n">PlacementGroupFactory</span><span class="p">(</span>
            <span class="p">[{</span><span class="s2">&quot;CPU&quot;</span><span class="p">:</span> <span class="n">resources</span><span class="p">[</span><span class="s2">&quot;CPU&quot;</span><span class="p">]</span> <span class="o">/</span> <span class="n">NUM_WORKERS_PER_NODE</span><span class="p">,</span> <span class="s2">&quot;GPU&quot;</span><span class="p">:</span> <span class="n">resources</span><span class="p">[</span><span class="s2">&quot;GPU&quot;</span><span class="p">]</span> <span class="o">/</span> <span class="n">NUM_WORKERS_PER_NODE</span><span class="p">}],</span>
            <span class="n">strategy</span><span class="o">=</span><span class="s2">&quot;STRICT_PACK&quot;</span><span class="p">,</span>
        <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">invoke_tuning_run</span><span class="p">(</span><span class="n">cfg</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">args</span><span class="p">:</span> <span class="n">argparse</span><span class="o">.</span><span class="n">Namespace</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Invoke an Isaac-Ray tuning run.</span>

<span class="sd">    Log either to a local directory or to MLFlow.</span>
<span class="sd">    Args:</span>
<span class="sd">        cfg: Configuration dictionary extracted from job setup</span>
<span class="sd">        args: Command-line arguments related to tuning.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Allow for early exit</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;TUNE_DISABLE_STRICT_METRIC_CHECKING&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;1&quot;</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;[WARNING]: Not saving checkpoints, just running experiment...&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;[INFO]: Model parameters and metrics will be preserved.&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;[WARNING]: For homogeneous cluster resources only...&quot;</span><span class="p">)</span>
    <span class="c1"># Get available resources</span>
    <span class="n">resources</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">get_gpu_node_resources</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[INFO]: Available resources </span><span class="si">{</span><span class="n">resources</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">ray</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">():</span>
        <span class="n">ray</span><span class="o">.</span><span class="n">init</span><span class="p">(</span>
            <span class="n">address</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">ray_address</span><span class="p">,</span>
            <span class="n">log_to_driver</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">num_gpus</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">resources</span><span class="p">),</span>
        <span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[INFO]: Using config </span><span class="si">{</span><span class="n">cfg</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Configure the search algorithm and the repeater</span>
    <span class="n">searcher</span> <span class="o">=</span> <span class="n">OptunaSearch</span><span class="p">(</span>
        <span class="n">metric</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">metric</span><span class="p">,</span>
        <span class="n">mode</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">mode</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">repeat_search</span> <span class="o">=</span> <span class="n">Repeater</span><span class="p">(</span><span class="n">searcher</span><span class="p">,</span> <span class="n">repeat</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">repeat_run_count</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">run_mode</span> <span class="o">==</span> <span class="s2">&quot;local&quot;</span><span class="p">:</span>  <span class="c1"># Standard config, to file</span>
        <span class="n">run_config</span> <span class="o">=</span> <span class="n">air</span><span class="o">.</span><span class="n">RunConfig</span><span class="p">(</span>
            <span class="n">storage_path</span><span class="o">=</span><span class="s2">&quot;/tmp/ray&quot;</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;IsaacRay-</span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">cfg_class</span><span class="si">}</span><span class="s2">-tune&quot;</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">checkpoint_config</span><span class="o">=</span><span class="n">air</span><span class="o">.</span><span class="n">CheckpointConfig</span><span class="p">(</span>
                <span class="n">checkpoint_frequency</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>  <span class="c1"># Disable periodic checkpointing</span>
                <span class="n">checkpoint_at_end</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>  <span class="c1"># Disable final checkpoint</span>
            <span class="p">),</span>
        <span class="p">)</span>

    <span class="k">elif</span> <span class="n">args</span><span class="o">.</span><span class="n">run_mode</span> <span class="o">==</span> <span class="s2">&quot;remote&quot;</span><span class="p">:</span>  <span class="c1"># MLFlow, to MLFlow server</span>
        <span class="n">mlflow_callback</span> <span class="o">=</span> <span class="n">MLflowLoggerCallback</span><span class="p">(</span>
            <span class="n">tracking_uri</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">mlflow_uri</span><span class="p">,</span>
            <span class="n">experiment_name</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;IsaacRay-</span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">cfg_class</span><span class="si">}</span><span class="s2">-tune&quot;</span><span class="p">,</span>
            <span class="n">save_artifact</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">tags</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;run_mode&quot;</span><span class="p">:</span> <span class="s2">&quot;remote&quot;</span><span class="p">,</span> <span class="s2">&quot;cfg_class&quot;</span><span class="p">:</span> <span class="n">args</span><span class="o">.</span><span class="n">cfg_class</span><span class="p">},</span>
        <span class="p">)</span>

        <span class="n">run_config</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">RunConfig</span><span class="p">(</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">&quot;mlflow&quot;</span><span class="p">,</span>
            <span class="n">storage_path</span><span class="o">=</span><span class="s2">&quot;/tmp/ray&quot;</span><span class="p">,</span>
            <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">mlflow_callback</span><span class="p">],</span>
            <span class="n">checkpoint_config</span><span class="o">=</span><span class="n">ray</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">CheckpointConfig</span><span class="p">(</span><span class="n">checkpoint_frequency</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">checkpoint_at_end</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Unrecognized run mode.&quot;</span><span class="p">)</span>

    <span class="c1"># Configure the tuning job</span>
    <span class="n">tuner</span> <span class="o">=</span> <span class="n">tune</span><span class="o">.</span><span class="n">Tuner</span><span class="p">(</span>
        <span class="n">IsaacLabTuneTrainable</span><span class="p">,</span>
        <span class="n">param_space</span><span class="o">=</span><span class="n">cfg</span><span class="p">,</span>
        <span class="n">tune_config</span><span class="o">=</span><span class="n">tune</span><span class="o">.</span><span class="n">TuneConfig</span><span class="p">(</span>
            <span class="n">search_alg</span><span class="o">=</span><span class="n">repeat_search</span><span class="p">,</span>
            <span class="n">num_samples</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">num_samples</span><span class="p">,</span>
            <span class="n">reuse_actors</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">),</span>
        <span class="n">run_config</span><span class="o">=</span><span class="n">run_config</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Execute the tuning</span>
    <span class="n">tuner</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

    <span class="c1"># Save results to mounted volume</span>
    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">run_mode</span> <span class="o">==</span> <span class="s2">&quot;local&quot;</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;[DONE!]: Check results with tensorboard dashboard&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;[DONE!]: Check results with MLFlow dashboard&quot;</span><span class="p">)</span>


<span class="k">class</span><span class="w"> </span><span class="nc">JobCfg</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;To be compatible with :meth: invoke_tuning_run and :class:IsaacLabTuneTrainable,</span>
<span class="sd">    at a minimum, the tune job should inherit from this class.&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">:</span> <span class="nb">dict</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Runner args include command line arguments passed to the task.</span>
<span class="sd">        For example:</span>
<span class="sd">        cfg[&quot;runner_args&quot;][&quot;headless_singleton&quot;] = &quot;--headless&quot;</span>
<span class="sd">        cfg[&quot;runner_args&quot;][&quot;enable_cameras_singleton&quot;] = &quot;--enable_cameras&quot;</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="s2">&quot;runner_args&quot;</span> <span class="ow">in</span> <span class="n">cfg</span><span class="p">,</span> <span class="s2">&quot;No runner arguments specified.&quot;</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Task is the desired task to train on. For example:</span>
<span class="sd">        cfg[&quot;runner_args&quot;][&quot;--task&quot;] = tune.choice([&quot;Isaac-Cartpole-RGB-TheiaTiny-v0&quot;])</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="s2">&quot;--task&quot;</span> <span class="ow">in</span> <span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;runner_args&quot;</span><span class="p">],</span> <span class="s2">&quot;No task specified.&quot;</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Hydra args define the hyperparameters varied within the sweep. For example:</span>
<span class="sd">        cfg[&quot;hydra_args&quot;][&quot;agent.params.network.cnn.activation&quot;] = tune.choice([&quot;relu&quot;, &quot;elu&quot;])</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="s2">&quot;hydra_args&quot;</span> <span class="ow">in</span> <span class="n">cfg</span><span class="p">,</span> <span class="s2">&quot;No hyperparameters specified.&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cfg</span> <span class="o">=</span> <span class="n">cfg</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">&quot;Tune Isaac Lab hyperparameters.&quot;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--ray_address&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;the Ray address.&quot;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--cfg_file&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="s2">&quot;hyperparameter_tuning/vision_cartpole_cfg.py&quot;</span><span class="p">,</span>
        <span class="n">required</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The relative filepath where a hyperparameter sweep is defined&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--cfg_class&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="s2">&quot;CartpoleRGBNoTuneJobCfg&quot;</span><span class="p">,</span>
        <span class="n">required</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Name of the hyperparameter sweep class to use&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--run_mode&quot;</span><span class="p">,</span>
        <span class="n">choices</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;local&quot;</span><span class="p">,</span> <span class="s2">&quot;remote&quot;</span><span class="p">],</span>
        <span class="n">default</span><span class="o">=</span><span class="s2">&quot;remote&quot;</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="p">(</span>
            <span class="s2">&quot;Set to local to use ./isaaclab.sh -p python, set to &quot;</span>
            <span class="s2">&quot;remote to use /workspace/isaaclab/isaaclab.sh -p python&quot;</span>
        <span class="p">),</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--workflow&quot;</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>  <span class="c1"># populated with RL Games</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The absolute path of the workflow to use for the experiment. By default, RL Games is used.&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--mlflow_uri&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">required</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The MLFlow Uri.&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--num_workers_per_node&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Number of workers to run on each GPU node. Only supply for parallelism on multi-gpu nodes&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--metric&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&quot;rewards/time&quot;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;What metric to tune for.&quot;</span><span class="p">)</span>

    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--mode&quot;</span><span class="p">,</span>
        <span class="n">choices</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;max&quot;</span><span class="p">,</span> <span class="s2">&quot;min&quot;</span><span class="p">],</span>
        <span class="n">default</span><span class="o">=</span><span class="s2">&quot;max&quot;</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;What to optimize the metric to while tuning&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--num_samples&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;How many hyperparameter runs to try total.&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--repeat_run_count&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;How many times to repeat each hyperparameter config.&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>
    <span class="n">NUM_WORKERS_PER_NODE</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">num_workers_per_node</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[INFO]: Using </span><span class="si">{</span><span class="n">NUM_WORKERS_PER_NODE</span><span class="si">}</span><span class="s2"> workers per node.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">run_mode</span> <span class="o">==</span> <span class="s2">&quot;remote&quot;</span><span class="p">:</span>
        <span class="n">BASE_DIR</span> <span class="o">=</span> <span class="n">DOCKER_PREFIX</span>  <span class="c1"># ensure logs are dumped to persistent location</span>
        <span class="n">PYTHON_EXEC</span> <span class="o">=</span> <span class="n">DOCKER_PREFIX</span> <span class="o">+</span> <span class="n">PYTHON_EXEC</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span>
        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">workflow</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">WORKFLOW</span> <span class="o">=</span> <span class="n">DOCKER_PREFIX</span> <span class="o">+</span> <span class="n">WORKFLOW</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">WORKFLOW</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">workflow</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[INFO]: Using remote mode </span><span class="si">{</span><span class="n">PYTHON_EXEC</span><span class="si">=}</span><span class="s2"> </span><span class="si">{</span><span class="n">WORKFLOW</span><span class="si">=}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">mlflow_uri</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="kn">import</span><span class="w"> </span><span class="nn">mlflow</span>

            <span class="n">mlflow</span><span class="o">.</span><span class="n">set_tracking_uri</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">mlflow_uri</span><span class="p">)</span>
            <span class="kn">from</span><span class="w"> </span><span class="nn">ray.air.integrations.mlflow</span><span class="w"> </span><span class="kn">import</span> <span class="n">MLflowLoggerCallback</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Please provide a result MLFLow URI server.&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>  <span class="c1"># local</span>
        <span class="n">PYTHON_EXEC</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">()</span> <span class="o">+</span> <span class="s2">&quot;/&quot;</span> <span class="o">+</span> <span class="n">PYTHON_EXEC</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span>
        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">workflow</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">WORKFLOW</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">()</span> <span class="o">+</span> <span class="s2">&quot;/&quot;</span> <span class="o">+</span> <span class="n">WORKFLOW</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">WORKFLOW</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">workflow</span>
        <span class="n">BASE_DIR</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">()</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[INFO]: Using local mode </span><span class="si">{</span><span class="n">PYTHON_EXEC</span><span class="si">=}</span><span class="s2"> </span><span class="si">{</span><span class="n">WORKFLOW</span><span class="si">=}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">file_path</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">cfg_file</span>
    <span class="n">class_name</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">cfg_class</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[INFO]: Attempting to use sweep config from </span><span class="si">{</span><span class="n">file_path</span><span class="si">=}</span><span class="s2"> </span><span class="si">{</span><span class="n">class_name</span><span class="si">=}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">module_name</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">splitext</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">basename</span><span class="p">(</span><span class="n">file_path</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span>

    <span class="n">spec</span> <span class="o">=</span> <span class="n">importlib</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="n">spec_from_file_location</span><span class="p">(</span><span class="n">module_name</span><span class="p">,</span> <span class="n">file_path</span><span class="p">)</span>
    <span class="n">module</span> <span class="o">=</span> <span class="n">importlib</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="n">module_from_spec</span><span class="p">(</span><span class="n">spec</span><span class="p">)</span>
    <span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="p">[</span><span class="n">module_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">module</span>
    <span class="n">spec</span><span class="o">.</span><span class="n">loader</span><span class="o">.</span><span class="n">exec_module</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[INFO]: Successfully imported </span><span class="si">{</span><span class="n">module_name</span><span class="si">}</span><span class="s2"> from </span><span class="si">{</span><span class="n">file_path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">class_name</span><span class="p">):</span>
        <span class="n">ClassToInstantiate</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">class_name</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[INFO]: Found correct class </span><span class="si">{</span><span class="n">ClassToInstantiate</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">instance</span> <span class="o">=</span> <span class="n">ClassToInstantiate</span><span class="p">()</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[INFO]: Successfully instantiated class &#39;</span><span class="si">{</span><span class="n">class_name</span><span class="si">}</span><span class="s2">&#39; from </span><span class="si">{</span><span class="n">file_path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">cfg</span> <span class="o">=</span> <span class="n">instance</span><span class="o">.</span><span class="n">cfg</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[INFO]: Grabbed the following hyperparameter sweep config: </span><span class="se">\n</span><span class="s2"> </span><span class="si">{</span><span class="n">cfg</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">invoke_tuning_run</span><span class="p">(</span><span class="n">cfg</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[ERROR]:Class &#39;</span><span class="si">{</span><span class="n">class_name</span><span class="si">}</span><span class="s2">&#39; not found in </span><span class="si">{</span><span class="n">file_path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details><p>The following script can be used to submit aggregate
jobs to one or more Ray cluster(s), which can be used for
running jobs on a remote cluster or simultaneous jobs with heterogeneous
resource requirements.</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-icon"><svg version="1.1" width="1.0em" height="1.0em" class="sd-octicon sd-octicon-code" viewBox="0 0 16 16" aria-hidden="true"><path d="m11.28 3.22 4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734L13.94 8l-3.72-3.72a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215Zm-6.56 0a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042L2.06 8l3.72 3.72a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L.47 8.53a.75.75 0 0 1 0-1.06Z"></path></svg></span><span class="sd-summary-text">scripts/reinforcement_learning/ray/submit_job.py</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Copyright (c) 2022-2025, The Isaac Lab Project Developers.</span>
<span class="c1"># All rights reserved.</span>
<span class="c1">#</span>
<span class="c1"># SPDX-License-Identifier: BSD-3-Clause</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">argparse</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">concurrent.futures</span><span class="w"> </span><span class="kn">import</span> <span class="n">ThreadPoolExecutor</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">ray</span><span class="w"> </span><span class="kn">import</span> <span class="n">job_submission</span>
<span class="hll">
</span><span class="hll"><span class="sd">&quot;&quot;&quot;</span>
</span><span class="hll"><span class="sd">This script submits aggregate job(s) to cluster(s) described in a</span>
</span><span class="hll"><span class="sd">config file containing ``name: &lt;NAME&gt; address: http://&lt;IP&gt;:&lt;PORT&gt;`` on</span>
</span><span class="hll"><span class="sd">a new line for each cluster. For KubeRay clusters, this file</span>
</span><span class="hll"><span class="sd">can be automatically created with :file:`grok_cluster_with_kubectl.py`</span>
</span><span class="hll">
</span><span class="hll"><span class="sd">Aggregate job(s) are matched with cluster(s) via the following relation:</span>
</span><span class="hll"><span class="sd">cluster_line_index_submitted_to = job_index % total_cluster_count</span>
</span><span class="hll">
</span><span class="hll"><span class="sd">Aggregate jobs are separated by the * delimiter. The ``--aggregate_jobs`` argument must be</span>
</span><span class="hll"><span class="sd">the last argument supplied to the script.</span>
</span><span class="hll">
</span><span class="hll"><span class="sd">An aggregate job could be a :file:`../tuner.py` tuning job, which automatically</span>
</span><span class="hll"><span class="sd">creates several individual jobs when started on a cluster. Alternatively, an aggregate job</span>
</span><span class="hll"><span class="sd">could be a :file:&#39;../wrap_resources.py` resource-wrapped job,</span>
</span><span class="hll"><span class="sd">which may contain several individual sub-jobs separated by</span>
</span><span class="hll"><span class="sd">the + delimiter.</span>
</span><span class="hll">
</span><span class="hll"><span class="sd">If there are more aggregate jobs than cluster(s), aggregate jobs will be submitted</span>
</span><span class="hll"><span class="sd">as clusters become available via the defined relation above. If there are less aggregate job(s)</span>
</span><span class="hll"><span class="sd">than clusters, some clusters will not receive aggregate job(s). The maximum number of</span>
</span><span class="hll"><span class="sd">aggregate jobs that can be run simultaneously is equal to the number of workers created by</span>
</span><span class="hll"><span class="sd">default by a ThreadPoolExecutor on the machine submitting jobs due to fetching the log output after</span>
</span><span class="hll"><span class="sd">jobs finish, which is unlikely to constrain overall-job submission.</span>
</span><span class="hll">
</span><span class="hll"><span class="sd">Usage:</span>
</span><span class="hll">
</span><span class="hll"><span class="sd">.. code-block:: bash</span>
</span><span class="hll">
</span><span class="hll"><span class="sd">    # Example; submitting a tuning job</span>
</span><span class="hll"><span class="sd">    python3 scripts/reinforcement_learning/ray/submit_job.py \</span>
</span><span class="hll"><span class="sd">    --aggregate_jobs /workspace/isaaclab/scripts/reinforcement_learning/ray/tuner.py \</span>
</span><span class="hll"><span class="sd">        --cfg_file hyperparameter_tuning/vision_cartpole_cfg.py \</span>
</span><span class="hll"><span class="sd">        --cfg_class CartpoleTheiaJobCfg --mlflow_uri &lt;ML_FLOW_URI&gt;</span>
</span><span class="hll">
</span><span class="hll"><span class="sd">    # Example: Submitting resource wrapped job</span>
</span><span class="hll"><span class="sd">    python3 scripts/reinforcement_learning/ray/submit_job.py --aggregate_jobs wrap_resources.py --test</span>
</span><span class="hll">
</span><span class="hll"><span class="sd">    # For all command line arguments</span>
</span><span class="hll"><span class="sd">    python3 scripts/reinforcement_learning/ray/submit_job.py -h</span>
</span><span class="hll"><span class="sd">&quot;&quot;&quot;</span>
</span><span class="n">script_directory</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="vm">__file__</span><span class="p">))</span>
<span class="n">CONFIG</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;working_dir&quot;</span><span class="p">:</span> <span class="n">script_directory</span><span class="p">,</span> <span class="s2">&quot;executable&quot;</span><span class="p">:</span> <span class="s2">&quot;/workspace/isaaclab/isaaclab.sh -p&quot;</span><span class="p">}</span>


<span class="k">def</span><span class="w"> </span><span class="nf">read_cluster_spec</span><span class="p">(</span><span class="n">fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">dict</span><span class="p">]:</span>
    <span class="k">if</span> <span class="n">fn</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">cluster_spec_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">expanduser</span><span class="p">(</span><span class="s2">&quot;~/.cluster_config&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">cluster_spec_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">expanduser</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">cluster_spec_path</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">FileNotFoundError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Cluster spec file not found at </span><span class="si">{</span><span class="n">cluster_spec_path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="n">clusters</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">cluster_spec_path</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">parts</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">)</span>
            <span class="n">http_address</span> <span class="o">=</span> <span class="n">parts</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
            <span class="n">cluster_info</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="n">parts</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;address&quot;</span><span class="p">:</span> <span class="n">http_address</span><span class="p">}</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[INFO] Setting </span><span class="si">{</span><span class="n">cluster_info</span><span class="p">[</span><span class="s1">&#39;name&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># with {cluster_info[&#39;num_gpu&#39;]} GPUs.&quot;)</span>
            <span class="n">clusters</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cluster_info</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">clusters</span>


<span class="k">def</span><span class="w"> </span><span class="nf">submit_job</span><span class="p">(</span><span class="n">cluster</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">job_command</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Submits a job to a single cluster, prints the final result and Ray dashboard URL at the end.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">address</span> <span class="o">=</span> <span class="n">cluster</span><span class="p">[</span><span class="s2">&quot;address&quot;</span><span class="p">]</span>
    <span class="n">cluster_name</span> <span class="o">=</span> <span class="n">cluster</span><span class="p">[</span><span class="s2">&quot;name&quot;</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[INFO]: Submitting job to cluster &#39;</span><span class="si">{</span><span class="n">cluster_name</span><span class="si">}</span><span class="s2">&#39; at </span><span class="si">{</span><span class="n">address</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># with {num_gpus} GPUs.&quot;)</span>
    <span class="n">client</span> <span class="o">=</span> <span class="n">job_submission</span><span class="o">.</span><span class="n">JobSubmissionClient</span><span class="p">(</span><span class="n">address</span><span class="p">)</span>
    <span class="n">runtime_env</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;working_dir&quot;</span><span class="p">:</span> <span class="n">CONFIG</span><span class="p">[</span><span class="s2">&quot;working_dir&quot;</span><span class="p">],</span> <span class="s2">&quot;executable&quot;</span><span class="p">:</span> <span class="n">CONFIG</span><span class="p">[</span><span class="s2">&quot;executable&quot;</span><span class="p">]}</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[INFO]: Checking contents of the directory: </span><span class="si">{</span><span class="n">CONFIG</span><span class="p">[</span><span class="s1">&#39;working_dir&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">dir_contents</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">CONFIG</span><span class="p">[</span><span class="s2">&quot;working_dir&quot;</span><span class="p">])</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[INFO]: Directory contents: </span><span class="si">{</span><span class="n">dir_contents</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[INFO]: Failed to list directory contents: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">entrypoint</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">CONFIG</span><span class="p">[</span><span class="s1">&#39;executable&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">job_command</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[INFO]: Attempting entrypoint </span><span class="si">{</span><span class="n">entrypoint</span><span class="si">=}</span><span class="s2"> in cluster </span><span class="si">{</span><span class="n">cluster</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">job_id</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">submit_job</span><span class="p">(</span><span class="n">entrypoint</span><span class="o">=</span><span class="n">entrypoint</span><span class="p">,</span> <span class="n">runtime_env</span><span class="o">=</span><span class="n">runtime_env</span><span class="p">)</span>
    <span class="n">status</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">get_job_status</span><span class="p">(</span><span class="n">job_id</span><span class="p">)</span>
    <span class="k">while</span> <span class="n">status</span> <span class="ow">in</span> <span class="p">[</span><span class="n">job_submission</span><span class="o">.</span><span class="n">JobStatus</span><span class="o">.</span><span class="n">PENDING</span><span class="p">,</span> <span class="n">job_submission</span><span class="o">.</span><span class="n">JobStatus</span><span class="o">.</span><span class="n">RUNNING</span><span class="p">]:</span>
        <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
        <span class="n">status</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">get_job_status</span><span class="p">(</span><span class="n">job_id</span><span class="p">)</span>

    <span class="n">final_logs</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">get_job_logs</span><span class="p">(</span><span class="n">job_id</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;----------------------------------------------------&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[INFO]: Cluster </span><span class="si">{</span><span class="n">cluster_name</span><span class="si">}</span><span class="s2"> Logs: </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">final_logs</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;----------------------------------------------------&quot;</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">submit_jobs_to_clusters</span><span class="p">(</span><span class="n">jobs</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">clusters</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">dict</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Submit all jobs to their respective clusters, cycling through clusters if there are more jobs than clusters.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">clusters</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;No clusters available for job submission.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">jobs</span><span class="p">)</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">clusters</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;[INFO]: Less jobs than clusters, some clusters will not receive jobs&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">jobs</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">clusters</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;[INFO]: Exactly one job per cluster&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;[INFO]: More jobs than clusters, jobs submitted as clusters become available.&quot;</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">ThreadPoolExecutor</span><span class="p">()</span> <span class="k">as</span> <span class="n">executor</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">job_command</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">jobs</span><span class="p">):</span>
            <span class="c1"># Cycle through clusters using modulus to wrap around if there are more jobs than clusters</span>
            <span class="n">cluster</span> <span class="o">=</span> <span class="n">clusters</span><span class="p">[</span><span class="n">idx</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">clusters</span><span class="p">)]</span>
            <span class="n">executor</span><span class="o">.</span><span class="n">submit</span><span class="p">(</span><span class="n">submit_job</span><span class="p">,</span> <span class="n">cluster</span><span class="p">,</span> <span class="n">job_command</span><span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">&quot;Submit multiple GPU jobs to multiple Ray clusters.&quot;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--config_file&quot;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&quot;~/.cluster_config&quot;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The cluster config path.&quot;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--aggregate_jobs&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
        <span class="n">nargs</span><span class="o">=</span><span class="n">argparse</span><span class="o">.</span><span class="n">REMAINDER</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;This should be last argument. The aggregate jobs to submit separated by the * delimiter.&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">aggregate_jobs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">jobs</span> <span class="o">=</span> <span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">aggregate_jobs</span><span class="p">)</span>
        <span class="n">formatted_jobs</span> <span class="o">=</span> <span class="n">jobs</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;*&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">formatted_jobs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Warning; Split jobs by cluster with the * delimiter&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">formatted_jobs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[INFO]: Isaac Ray Wrapper received jobs </span><span class="si">{</span><span class="n">formatted_jobs</span><span class="si">=}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">clusters</span> <span class="o">=</span> <span class="n">read_cluster_spec</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">config_file</span><span class="p">)</span>
    <span class="n">submit_jobs_to_clusters</span><span class="p">(</span><span class="n">formatted_jobs</span><span class="p">,</span> <span class="n">clusters</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details><p>The following script can be used to extract KubeRay cluster information for aggregate job submission.</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-icon"><svg version="1.1" width="1.0em" height="1.0em" class="sd-octicon sd-octicon-code" viewBox="0 0 16 16" aria-hidden="true"><path d="m11.28 3.22 4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734L13.94 8l-3.72-3.72a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215Zm-6.56 0a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042L2.06 8l3.72 3.72a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L.47 8.53a.75.75 0 0 1 0-1.06Z"></path></svg></span><span class="sd-summary-text">scripts/reinforcement_learning/ray/grok_cluster_with_kubectl.py</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Copyright (c) 2022-2025, The Isaac Lab Project Developers.</span>
<span class="c1"># All rights reserved.</span>
<span class="c1">#</span>
<span class="c1"># SPDX-License-Identifier: BSD-3-Clause</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">argparse</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">re</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">subprocess</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">threading</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">concurrent.futures</span><span class="w"> </span><span class="kn">import</span> <span class="n">ThreadPoolExecutor</span><span class="p">,</span> <span class="n">as_completed</span>

<span class="hll"><span class="sd">&quot;&quot;&quot;</span>
</span><span class="hll"><span class="sd">This script requires that kubectl is installed and KubeRay was used to create the cluster.</span>
</span><span class="hll">
</span><span class="hll"><span class="sd">Creates a config file containing ``name: &lt;NAME&gt; address: http://&lt;IP&gt;:&lt;PORT&gt;`` on</span>
</span><span class="hll"><span class="sd">a new line for each cluster, and also fetches the MLFlow URI.</span>
</span><span class="hll">
</span><span class="hll"><span class="sd">Usage:</span>
</span><span class="hll">
</span><span class="hll"><span class="sd">.. code-block:: bash</span>
</span><span class="hll">
</span><span class="hll"><span class="sd">    python3 scripts/reinforcement_learning/ray/grok_cluster_with_kubectl.py</span>
</span><span class="hll"><span class="sd">    # For options, supply -h arg</span>
</span><span class="hll"><span class="sd">&quot;&quot;&quot;</span>
</span>

<span class="k">def</span><span class="w"> </span><span class="nf">get_namespace</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Get the current Kubernetes namespace from the context, fallback to default if not set&quot;&quot;&quot;</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">namespace</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">subprocess</span><span class="o">.</span><span class="n">check_output</span><span class="p">([</span><span class="s2">&quot;kubectl&quot;</span><span class="p">,</span> <span class="s2">&quot;config&quot;</span><span class="p">,</span> <span class="s2">&quot;view&quot;</span><span class="p">,</span> <span class="s2">&quot;--minify&quot;</span><span class="p">,</span> <span class="s2">&quot;--output&quot;</span><span class="p">,</span> <span class="s2">&quot;jsonpath={..namespace}&quot;</span><span class="p">])</span>
            <span class="o">.</span><span class="n">decode</span><span class="p">()</span>
            <span class="o">.</span><span class="n">strip</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">namespace</span><span class="p">:</span>
            <span class="n">namespace</span> <span class="o">=</span> <span class="s2">&quot;default&quot;</span>
    <span class="k">except</span> <span class="n">subprocess</span><span class="o">.</span><span class="n">CalledProcessError</span><span class="p">:</span>
        <span class="n">namespace</span> <span class="o">=</span> <span class="s2">&quot;default&quot;</span>
    <span class="k">return</span> <span class="n">namespace</span>


<span class="k">def</span><span class="w"> </span><span class="nf">get_pods</span><span class="p">(</span><span class="n">namespace</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;default&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">tuple</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Get a list of all of the pods in the namespace&quot;&quot;&quot;</span>
    <span class="n">cmd</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;kubectl&quot;</span><span class="p">,</span> <span class="s2">&quot;get&quot;</span><span class="p">,</span> <span class="s2">&quot;pods&quot;</span><span class="p">,</span> <span class="s2">&quot;-n&quot;</span><span class="p">,</span> <span class="n">namespace</span><span class="p">,</span> <span class="s2">&quot;--no-headers&quot;</span><span class="p">]</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">subprocess</span><span class="o">.</span><span class="n">check_output</span><span class="p">(</span><span class="n">cmd</span><span class="p">)</span><span class="o">.</span><span class="n">decode</span><span class="p">()</span>
    <span class="n">pods</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">output</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">):</span>
        <span class="n">fields</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
        <span class="n">pod_name</span> <span class="o">=</span> <span class="n">fields</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">status</span> <span class="o">=</span> <span class="n">fields</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">pods</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">pod_name</span><span class="p">,</span> <span class="n">status</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">pods</span>


<span class="k">def</span><span class="w"> </span><span class="nf">get_clusters</span><span class="p">(</span><span class="n">pods</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span> <span class="n">cluster_name_prefix</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">set</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get unique cluster name(s). Works for one or more clusters, based off of the number of head nodes.</span>
<span class="sd">    Excludes MLflow deployments.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">clusters</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">pod_name</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">pods</span><span class="p">:</span>
        <span class="c1"># Skip MLflow pods</span>
        <span class="k">if</span> <span class="s2">&quot;-mlflow&quot;</span> <span class="ow">in</span> <span class="n">pod_name</span><span class="p">:</span>
            <span class="k">continue</span>

        <span class="n">match</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;(&quot;</span> <span class="o">+</span> <span class="n">re</span><span class="o">.</span><span class="n">escape</span><span class="p">(</span><span class="n">cluster_name_prefix</span><span class="p">)</span> <span class="o">+</span> <span class="sa">r</span><span class="s2">&quot;[-\w]+)&quot;</span><span class="p">,</span> <span class="n">pod_name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">match</span><span class="p">:</span>
            <span class="c1"># Get base name without head/worker suffix (skip workers)</span>
            <span class="k">if</span> <span class="s2">&quot;head&quot;</span> <span class="ow">in</span> <span class="n">pod_name</span><span class="p">:</span>
                <span class="n">base_name</span> <span class="o">=</span> <span class="n">match</span><span class="o">.</span><span class="n">group</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;-head&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">clusters</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">base_name</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">clusters</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">get_mlflow_info</span><span class="p">(</span><span class="n">namespace</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">cluster_prefix</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;isaacray&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get MLflow service information if it exists in the namespace with the given prefix.</span>
<span class="sd">    Only works for a single cluster instance.</span>
<span class="sd">    Args:</span>
<span class="sd">        namespace: Kubernetes namespace</span>
<span class="sd">        cluster_prefix: Base cluster name (without -head/-worker suffixes)</span>
<span class="sd">    Returns:</span>
<span class="sd">        MLflow service URL</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Strip any -head or -worker suffixes to get base name</span>
    <span class="k">if</span> <span class="n">namespace</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">namespace</span> <span class="o">=</span> <span class="n">get_namespace</span><span class="p">()</span>
    <span class="n">pods</span> <span class="o">=</span> <span class="n">get_pods</span><span class="p">(</span><span class="n">namespace</span><span class="o">=</span><span class="n">namespace</span><span class="p">)</span>
    <span class="n">clusters</span> <span class="o">=</span> <span class="n">get_clusters</span><span class="p">(</span><span class="n">pods</span><span class="o">=</span><span class="n">pods</span><span class="p">,</span> <span class="n">cluster_name_prefix</span><span class="o">=</span><span class="n">cluster_prefix</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">clusters</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;More than one cluster matches prefix, could not automatically determine mlflow info.&quot;</span><span class="p">)</span>
    <span class="n">mlflow_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">cluster_prefix</span><span class="si">}</span><span class="s2">-mlflow&quot;</span>

    <span class="n">cmd</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;kubectl&quot;</span><span class="p">,</span> <span class="s2">&quot;get&quot;</span><span class="p">,</span> <span class="s2">&quot;svc&quot;</span><span class="p">,</span> <span class="n">mlflow_name</span><span class="p">,</span> <span class="s2">&quot;-n&quot;</span><span class="p">,</span> <span class="n">namespace</span><span class="p">,</span> <span class="s2">&quot;--no-headers&quot;</span><span class="p">]</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">subprocess</span><span class="o">.</span><span class="n">check_output</span><span class="p">(</span><span class="n">cmd</span><span class="p">)</span><span class="o">.</span><span class="n">decode</span><span class="p">()</span>
        <span class="n">fields</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>

        <span class="c1"># Get cluster IP</span>
        <span class="n">cluster_ip</span> <span class="o">=</span> <span class="n">fields</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">port</span> <span class="o">=</span> <span class="s2">&quot;5000&quot;</span>  <span class="c1"># Default MLflow port</span>
        <span class="c1"># This needs to be http to be resolved. HTTPS can&#39;t be resolved</span>
        <span class="c1"># This should be fine as it is on a subnet on the cluster regardless</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;http://</span><span class="si">{</span><span class="n">cluster_ip</span><span class="si">}</span><span class="s2">:</span><span class="si">{</span><span class="n">port</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="k">except</span> <span class="n">subprocess</span><span class="o">.</span><span class="n">CalledProcessError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Could not grok MLflow: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># Fixed f-string</span>


<span class="k">def</span><span class="w"> </span><span class="nf">check_clusters_running</span><span class="p">(</span><span class="n">pods</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span> <span class="n">clusters</span><span class="p">:</span> <span class="nb">set</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Check that all of the pods in all provided clusters are running.</span>

<span class="sd">    Args:</span>
<span class="sd">        pods (list): A list of tuples where each tuple contains the pod name and its status.</span>
<span class="sd">        clusters (set): A set of cluster names to check.</span>

<span class="sd">    Returns:</span>
<span class="sd">        bool: True if all pods in any of the clusters are running, False otherwise.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">clusters_running</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">for</span> <span class="n">cluster</span> <span class="ow">in</span> <span class="n">clusters</span><span class="p">:</span>
        <span class="n">cluster_pods</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">pods</span> <span class="k">if</span> <span class="n">p</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="n">cluster</span><span class="p">)]</span>
        <span class="n">total_pods</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">cluster_pods</span><span class="p">)</span>
        <span class="n">running_pods</span> <span class="o">=</span> <span class="nb">len</span><span class="p">([</span><span class="n">p</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">cluster_pods</span> <span class="k">if</span> <span class="n">p</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;Running&quot;</span><span class="p">])</span>
        <span class="k">if</span> <span class="n">running_pods</span> <span class="o">==</span> <span class="n">total_pods</span> <span class="ow">and</span> <span class="n">running_pods</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">clusters_running</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">break</span>
    <span class="k">return</span> <span class="n">clusters_running</span>


<span class="k">def</span><span class="w"> </span><span class="nf">get_ray_address</span><span class="p">(</span><span class="n">head_pod</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">namespace</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;default&quot;</span><span class="p">,</span> <span class="n">ray_head_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;head&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Given a cluster head pod, check its logs, which should include the ray address which can accept job requests.</span>

<span class="sd">    Args:</span>
<span class="sd">        head_pod (str): The name of the head pod.</span>
<span class="sd">        namespace (str, optional): The Kubernetes namespace. Defaults to &quot;default&quot;.</span>
<span class="sd">        ray_head_name (str, optional): The name of the ray head container. Defaults to &quot;head&quot;.</span>

<span class="sd">    Returns:</span>
<span class="sd">        str: The ray address if found, None otherwise.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the logs cannot be retrieved or the ray address is not found.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">cmd</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;kubectl&quot;</span><span class="p">,</span> <span class="s2">&quot;logs&quot;</span><span class="p">,</span> <span class="n">head_pod</span><span class="p">,</span> <span class="s2">&quot;-c&quot;</span><span class="p">,</span> <span class="n">ray_head_name</span><span class="p">,</span> <span class="s2">&quot;-n&quot;</span><span class="p">,</span> <span class="n">namespace</span><span class="p">]</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">subprocess</span><span class="o">.</span><span class="n">check_output</span><span class="p">(</span><span class="n">cmd</span><span class="p">)</span><span class="o">.</span><span class="n">decode</span><span class="p">()</span>
    <span class="k">except</span> <span class="n">subprocess</span><span class="o">.</span><span class="n">CalledProcessError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Could not enter head container with cmd </span><span class="si">{</span><span class="n">cmd</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">Perhaps try a different namespace or ray head name.&quot;</span>
        <span class="p">)</span>
    <span class="n">match</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;RAY_ADDRESS=&#39;([^&#39;]+)&#39;&quot;</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">match</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">match</span><span class="o">.</span><span class="n">group</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">None</span>


<span class="k">def</span><span class="w"> </span><span class="nf">process_cluster</span><span class="p">(</span><span class="n">cluster_info</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">ray_head_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;head&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    For each cluster, check that it is running, and get the Ray head address that will accept jobs.</span>

<span class="sd">    Args:</span>
<span class="sd">        cluster_info (dict): A dictionary containing cluster information with keys &#39;cluster&#39;, &#39;pods&#39;, and &#39;namespace&#39;.</span>
<span class="sd">        ray_head_name (str, optional): The name of the ray head container. Defaults to &quot;head&quot;.</span>

<span class="sd">    Returns:</span>
<span class="sd">        str: A string containing the cluster name and its Ray head address, or an error message if the head pod or Ray address is not found.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">cluster</span><span class="p">,</span> <span class="n">pods</span><span class="p">,</span> <span class="n">namespace</span> <span class="o">=</span> <span class="n">cluster_info</span>
    <span class="n">head_pod</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">for</span> <span class="n">pod_name</span><span class="p">,</span> <span class="n">status</span> <span class="ow">in</span> <span class="n">pods</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">pod_name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="n">cluster</span> <span class="o">+</span> <span class="s2">&quot;-head&quot;</span><span class="p">):</span>
            <span class="n">head_pod</span> <span class="o">=</span> <span class="n">pod_name</span>
            <span class="k">break</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">head_pod</span><span class="p">:</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;Error: Could not find head pod for cluster </span><span class="si">{</span><span class="n">cluster</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span>

    <span class="c1"># Get RAY_ADDRESS and status</span>
    <span class="n">ray_address</span> <span class="o">=</span> <span class="n">get_ray_address</span><span class="p">(</span><span class="n">head_pod</span><span class="p">,</span> <span class="n">namespace</span><span class="o">=</span><span class="n">namespace</span><span class="p">,</span> <span class="n">ray_head_name</span><span class="o">=</span><span class="n">ray_head_name</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">ray_address</span><span class="p">:</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;Error: Could not find RAY_ADDRESS for cluster </span><span class="si">{</span><span class="n">cluster</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span>

    <span class="c1"># Return only cluster and ray address</span>
    <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;name: </span><span class="si">{</span><span class="n">cluster</span><span class="si">}</span><span class="s2"> address: </span><span class="si">{</span><span class="n">ray_address</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span>


<span class="k">def</span><span class="w"> </span><span class="nf">main</span><span class="p">():</span>
    <span class="c1"># Parse command-line arguments</span>
    <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">&quot;Process Ray clusters and save their specifications.&quot;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--prefix&quot;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&quot;isaacray&quot;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The prefix for the cluster names.&quot;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--output&quot;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&quot;~/.cluster_config&quot;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The file to save cluster specifications.&quot;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--ray_head_name&quot;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&quot;head&quot;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The metadata name for the ray head container&quot;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--namespace&quot;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Kubernetes namespace to use. If not provided, will detect from current context.&quot;</span>
    <span class="p">)</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

    <span class="c1"># Get namespace from args or detect it</span>
    <span class="n">current_namespace</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">namespace</span> <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">namespace</span> <span class="k">else</span> <span class="n">get_namespace</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Using namespace: </span><span class="si">{</span><span class="n">current_namespace</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="n">cluster_name_prefix</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">prefix</span>
    <span class="n">cluster_spec_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">expanduser</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">output</span><span class="p">)</span>

    <span class="c1"># Get all pods</span>
    <span class="n">pods</span> <span class="o">=</span> <span class="n">get_pods</span><span class="p">(</span><span class="n">namespace</span><span class="o">=</span><span class="n">current_namespace</span><span class="p">)</span>

    <span class="c1"># Get clusters</span>
    <span class="n">clusters</span> <span class="o">=</span> <span class="n">get_clusters</span><span class="p">(</span><span class="n">pods</span><span class="p">,</span> <span class="n">cluster_name_prefix</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">clusters</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;No clusters found with prefix </span><span class="si">{</span><span class="n">cluster_name_prefix</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">return</span>

    <span class="c1"># Wait for clusters to be running</span>
    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="n">pods</span> <span class="o">=</span> <span class="n">get_pods</span><span class="p">(</span><span class="n">namespace</span><span class="o">=</span><span class="n">current_namespace</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">check_clusters_running</span><span class="p">(</span><span class="n">pods</span><span class="p">,</span> <span class="n">clusters</span><span class="p">):</span>
            <span class="k">break</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Waiting for all clusters to spin up...&quot;</span><span class="p">)</span>
        <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Checking for MLflow:&quot;</span><span class="p">)</span>
    <span class="c1"># Check MLflow status for each cluster</span>
    <span class="k">for</span> <span class="n">cluster</span> <span class="ow">in</span> <span class="n">clusters</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">mlflow_address</span> <span class="o">=</span> <span class="n">get_mlflow_info</span><span class="p">(</span><span class="n">current_namespace</span><span class="p">,</span> <span class="n">cluster</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;MLflow address for </span><span class="si">{</span><span class="n">cluster</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">mlflow_address</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">ValueError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ML Flow not located: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>

    <span class="c1"># Prepare cluster info for parallel processing</span>
    <span class="n">cluster_infos</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">cluster</span> <span class="ow">in</span> <span class="n">clusters</span><span class="p">:</span>
        <span class="n">cluster_pods</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">pods</span> <span class="k">if</span> <span class="n">p</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="n">cluster</span><span class="p">)]</span>
        <span class="n">cluster_infos</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">cluster</span><span class="p">,</span> <span class="n">cluster_pods</span><span class="p">,</span> <span class="n">current_namespace</span><span class="p">))</span>

    <span class="c1"># Use ThreadPoolExecutor to process clusters in parallel</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">results_lock</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">Lock</span><span class="p">()</span>

    <span class="k">with</span> <span class="n">ThreadPoolExecutor</span><span class="p">()</span> <span class="k">as</span> <span class="n">executor</span><span class="p">:</span>
        <span class="n">future_to_cluster</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">executor</span><span class="o">.</span><span class="n">submit</span><span class="p">(</span><span class="n">process_cluster</span><span class="p">,</span> <span class="n">info</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">ray_head_name</span><span class="p">):</span> <span class="n">info</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">info</span> <span class="ow">in</span> <span class="n">cluster_infos</span>
        <span class="p">}</span>
        <span class="k">for</span> <span class="n">future</span> <span class="ow">in</span> <span class="n">as_completed</span><span class="p">(</span><span class="n">future_to_cluster</span><span class="p">):</span>
            <span class="n">cluster_name</span> <span class="o">=</span> <span class="n">future_to_cluster</span><span class="p">[</span><span class="n">future</span><span class="p">]</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">result</span> <span class="o">=</span> <span class="n">future</span><span class="o">.</span><span class="n">result</span><span class="p">()</span>
                <span class="k">with</span> <span class="n">results_lock</span><span class="p">:</span>
                    <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">exc</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">cluster_name</span><span class="si">}</span><span class="s2"> generated an exception: </span><span class="si">{</span><span class="n">exc</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Sort results alphabetically by cluster name</span>
    <span class="n">results</span><span class="o">.</span><span class="n">sort</span><span class="p">()</span>

    <span class="c1"># Write sorted results to the output file (Ray info only)</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">cluster_spec_file</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">results</span><span class="p">:</span>
            <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Cluster spec information saved to </span><span class="si">{</span><span class="n">cluster_spec_file</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="c1"># Display the contents of the config file</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">cluster_spec_file</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">())</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details><p>The following script can be used to easily create clusters on Google GKE.</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-icon"><svg version="1.1" width="1.0em" height="1.0em" class="sd-octicon sd-octicon-code" viewBox="0 0 16 16" aria-hidden="true"><path d="m11.28 3.22 4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734L13.94 8l-3.72-3.72a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215Zm-6.56 0a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042L2.06 8l3.72 3.72a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L.47 8.53a.75.75 0 0 1 0-1.06Z"></path></svg></span><span class="sd-summary-text">scripts/reinforcement_learning/ray/launch.py</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Copyright (c) 2022-2025, The Isaac Lab Project Developers.</span>
<span class="c1"># All rights reserved.</span>
<span class="c1">#</span>
<span class="c1"># SPDX-License-Identifier: BSD-3-Clause</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">argparse</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pathlib</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">subprocess</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">yaml</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">util</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">jinja2</span><span class="w"> </span><span class="kn">import</span> <span class="n">Environment</span><span class="p">,</span> <span class="n">FileSystemLoader</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">kubernetes</span><span class="w"> </span><span class="kn">import</span> <span class="n">config</span>

<span class="sd">&quot;&quot;&quot;This script helps create one or more KubeRay clusters.</span>
<span class="hll">
</span><span class="hll"><span class="sd">Usage:</span>
</span><span class="hll">
</span><span class="hll"><span class="sd">.. code-block:: bash</span>
</span><span class="hll"><span class="sd">    # If the head node is stuck on container creating, make sure to create a secret</span>
</span><span class="hll"><span class="sd">    python3 scripts/reinforcement_learning/ray/launch.py -h</span>
</span><span class="hll">
</span><span class="hll"><span class="sd">    # Examples</span>
</span><span class="hll">
</span><span class="hll"><span class="sd">    # The following creates 8 GPUx1 nvidia l4 workers</span>
</span><span class="hll"><span class="sd">    python3 scripts/reinforcement_learning/ray/launch.py --cluster_host google_cloud \</span>
</span><span class="hll"><span class="sd">    --namespace &lt;NAMESPACE&gt; --image &lt;YOUR_ISAAC_RAY_IMAGE&gt; \</span>
</span><span class="hll"><span class="sd">    --num_workers 8 --num_clusters 1 --worker_accelerator nvidia-l4 --gpu_per_worker 1</span>
</span><span class="hll">
</span><span class="hll"><span class="sd">    # The following creates 1 GPUx1 nvidia l4 worker, 2 GPUx2 nvidia-tesla-t4 workers,</span>
</span><span class="hll"><span class="sd">    # and 2 GPUx4 nvidia-tesla-t4 GPU workers</span>
</span><span class="hll"><span class="sd">    python3 scripts/reinforcement_learning/ray/launch.py --cluster_host google_cloud \</span>
</span><span class="hll"><span class="sd">    --namespace &lt;NAMESPACE&gt; --image &lt;YOUR_ISAAC_RAY_IMAGE&gt; \</span>
</span><span class="hll"><span class="sd">    --num_workers 1 2 --num_clusters 1 \</span>
</span><span class="hll"><span class="sd">    --worker_accelerator nvidia-l4 nvidia-tesla-t4 --gpu_per_worker 1 2 4</span>
</span><span class="hll"><span class="sd">&quot;&quot;&quot;</span>
</span><span class="hll"><span class="n">RAY_DIR</span> <span class="o">=</span> <span class="n">pathlib</span><span class="o">.</span><span class="n">Path</span><span class="p">(</span><span class="vm">__file__</span><span class="p">)</span><span class="o">.</span><span class="n">parent</span>
</span>

<span class="k">def</span><span class="w"> </span><span class="nf">apply_manifest</span><span class="p">(</span><span class="n">args</span><span class="p">:</span> <span class="n">argparse</span><span class="o">.</span><span class="n">Namespace</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Provided a Jinja templated ray.io/v1alpha1 file,</span>
<span class="sd">    populate the arguments and create the cluster. Additionally, create</span>
<span class="sd">    kubernetes containers for resources separated by &#39;---&#39; from the rest</span>
<span class="sd">    of the file.</span>

<span class="sd">    Args:</span>
<span class="sd">        args: Possible arguments concerning cluster parameters.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Load Kubernetes configuration</span>
    <span class="n">config</span><span class="o">.</span><span class="n">load_kube_config</span><span class="p">()</span>

    <span class="c1"># Set up Jinja2 environment for loading templates</span>
    <span class="n">templates_dir</span> <span class="o">=</span> <span class="n">RAY_DIR</span> <span class="o">/</span> <span class="s2">&quot;cluster_configs&quot;</span> <span class="o">/</span> <span class="n">args</span><span class="o">.</span><span class="n">cluster_host</span>
    <span class="n">file_loader</span> <span class="o">=</span> <span class="n">FileSystemLoader</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">templates_dir</span><span class="p">))</span>
    <span class="n">jinja_env</span> <span class="o">=</span> <span class="n">Environment</span><span class="p">(</span><span class="n">loader</span><span class="o">=</span><span class="n">file_loader</span><span class="p">,</span> <span class="n">keep_trailing_newline</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">autoescape</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># Define template filename</span>
    <span class="n">template_file</span> <span class="o">=</span> <span class="s2">&quot;kuberay.yaml.jinja&quot;</span>

    <span class="c1"># Convert args namespace to a dictionary</span>
    <span class="n">template_params</span> <span class="o">=</span> <span class="nb">vars</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>

    <span class="c1"># Load and render the template</span>
    <span class="n">template</span> <span class="o">=</span> <span class="n">jinja_env</span><span class="o">.</span><span class="n">get_template</span><span class="p">(</span><span class="n">template_file</span><span class="p">)</span>
    <span class="n">file_contents</span> <span class="o">=</span> <span class="n">template</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="n">template_params</span><span class="p">)</span>

    <span class="c1"># Parse all YAML documents in the rendered template</span>
    <span class="n">all_yamls</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">yaml</span><span class="o">.</span><span class="n">safe_load_all</span><span class="p">(</span><span class="n">file_contents</span><span class="p">):</span>
        <span class="n">all_yamls</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span>

    <span class="c1"># Convert back to YAML string, preserving multiple documents</span>
    <span class="n">cleaned_yaml_string</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">doc</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">all_yamls</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">cleaned_yaml_string</span> <span class="o">+=</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">---</span><span class="se">\n</span><span class="s2">&quot;</span>
        <span class="n">cleaned_yaml_string</span> <span class="o">+=</span> <span class="n">yaml</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span>

    <span class="c1"># Apply the Kubernetes manifest using kubectl</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">cleaned_yaml_string</span><span class="p">)</span>
        <span class="n">subprocess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="s2">&quot;kubectl&quot;</span><span class="p">,</span> <span class="s2">&quot;apply&quot;</span><span class="p">,</span> <span class="s2">&quot;-f&quot;</span><span class="p">,</span> <span class="s2">&quot;-&quot;</span><span class="p">],</span> <span class="nb">input</span><span class="o">=</span><span class="n">cleaned_yaml_string</span><span class="p">,</span> <span class="n">text</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">check</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">except</span> <span class="n">subprocess</span><span class="o">.</span><span class="n">CalledProcessError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="n">exit</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;An error occurred while running `kubectl`: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">parse_args</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">argparse</span><span class="o">.</span><span class="n">Namespace</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Parse command-line arguments for Kubernetes deployment script.</span>

<span class="sd">    Returns:</span>
<span class="sd">        argparse.Namespace: Parsed command-line arguments.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">arg_parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span>
        <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Script to apply manifests to create Kubernetes objects for Ray clusters.&quot;</span><span class="p">,</span>
        <span class="n">formatter_class</span><span class="o">=</span><span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentDefaultsHelpFormatter</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">arg_parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--cluster_host&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="s2">&quot;google_cloud&quot;</span><span class="p">,</span>
        <span class="n">choices</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;google_cloud&quot;</span><span class="p">],</span>
        <span class="n">help</span><span class="o">=</span><span class="p">(</span>
            <span class="s2">&quot;In the cluster_configs directory, the name of the folder where a tune.yaml.jinja&quot;</span>
            <span class="s2">&quot;file exists defining the KubeRay config. Currently only google_cloud is supported.&quot;</span>
        <span class="p">),</span>
    <span class="p">)</span>

    <span class="n">arg_parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--name&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
        <span class="n">required</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="s2">&quot;isaacray&quot;</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Name of the Kubernetes deployment.&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">arg_parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--namespace&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
        <span class="n">required</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="s2">&quot;default&quot;</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Kubernetes namespace to deploy the Ray cluster.&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">arg_parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--service_acount_name&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">required</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&quot;default&quot;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The service account name to use.&quot;</span>
    <span class="p">)</span>

    <span class="n">arg_parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--image&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
        <span class="n">required</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Docker image for the Ray cluster pods.&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">arg_parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--worker_accelerator&quot;</span><span class="p">,</span>
        <span class="n">nargs</span><span class="o">=</span><span class="s2">&quot;+&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;nvidia-l4&quot;</span><span class="p">],</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;GPU accelerator name. Supply more than one for heterogeneous resources.&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">arg_parser</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">add_resource_arguments</span><span class="p">(</span><span class="n">arg_parser</span><span class="p">,</span> <span class="n">cluster_create_defaults</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="n">arg_parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--num_clusters&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;How many Ray Clusters to create.&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">arg_parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--num_head_cpu&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>  <span class="c1"># to be able to schedule partial CPU heads</span>
        <span class="n">default</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The number of CPUs to give the Ray head.&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">arg_parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--head_ram_gb&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;How many gigs of ram to give the Ray head&quot;</span><span class="p">)</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">arg_parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">util</span><span class="o">.</span><span class="n">fill_in_missing_resources</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">cluster_creation_flag</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">main</span><span class="p">():</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">parse_args</span><span class="p">()</span>

    <span class="k">if</span> <span class="s2">&quot;head&quot;</span> <span class="ow">in</span> <span class="n">args</span><span class="o">.</span><span class="n">name</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For compatibility with other scripts, do not include head in the name&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">num_clusters</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">apply_manifest</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">default_name</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">name</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">num_clusters</span><span class="p">):</span>
            <span class="n">args</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">default_name</span> <span class="o">+</span> <span class="s2">&quot;-&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
            <span class="n">apply_manifest</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details></section>
<section id="docker-based-local-quickstart">
<h2><a class="toc-backref" href="#id3" role="doc-backlink">Docker-based Local Quickstart</a><a class="headerlink" href="#docker-based-local-quickstart" title="Permalink to this heading">#</a></h2>
<p>First, follow the <a class="reference external" href="https://isaac-sim.github.io/IsaacLab/main/source/deployment/docker.html">Docker Guide</a>
to set up the NVIDIA Container Toolkit and Docker Compose.</p>
<p>Then, run the following steps to start a tuning run.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Build the base image, but we don&#39;t need to run it</span>
python3<span class="w"> </span>docker/container.py<span class="w"> </span>start<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>python3<span class="w"> </span>docker/container.py<span class="w"> </span>stop
<span class="c1"># Build the tuning image with extra deps</span>
docker<span class="w"> </span>build<span class="w"> </span>-t<span class="w"> </span>isaacray<span class="w"> </span>-f<span class="w"> </span>scripts/reinforcement_learning/ray/cluster_configs/Dockerfile<span class="w"> </span>.
<span class="c1"># Start the tuning image - symlink so that changes in the source folder show up in the container</span>
docker<span class="w"> </span>run<span class="w"> </span>-v<span class="w"> </span><span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span>/source:/workspace/isaaclab/source<span class="w"> </span>-it<span class="w"> </span>--gpus<span class="w"> </span>all<span class="w"> </span>--net<span class="o">=</span>host<span class="w"> </span>--entrypoint<span class="w"> </span>/bin/bash<span class="w"> </span>isaacray
<span class="c1"># Start the Ray server within the tuning image</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;import ray; ray.init(); import time; [time.sleep(10) for _ in iter(int, 1)]&quot;</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>./isaaclab.sh<span class="w"> </span>-p
</pre></div>
</div>
<p>In a different terminal, run the following.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># In a new terminal (don&#39;t close the above) , enter the image with a new shell.</span>
docker<span class="w"> </span>container<span class="w"> </span>ps
docker<span class="w"> </span><span class="nb">exec</span><span class="w"> </span>-it<span class="w"> </span>&lt;ISAAC_RAY_IMAGE_ID_FROM_CONTAINER_PS&gt;<span class="w"> </span>/bin/bash
<span class="c1"># Start a tuning run, with one parallel worker per GPU</span>
./isaaclab.sh<span class="w"> </span>-p<span class="w"> </span>scripts/reinforcement_learning/ray/tuner.py<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--cfg_file<span class="w"> </span>scripts/reinforcement_learning/ray/hyperparameter_tuning/vision_cartpole_cfg.py<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--cfg_class<span class="w"> </span>CartpoleTheiaJobCfg<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--run_mode<span class="w"> </span><span class="nb">local</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--workflow<span class="w"> </span>scripts/reinforcement_learning/rl_games/train.py<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--num_workers_per_node<span class="w"> </span>&lt;NUMBER_OF_GPUS_IN_COMPUTER&gt;
</pre></div>
</div>
<p>To view the training logs, in a different terminal, run the following and visit <code class="docutils literal notranslate"><span class="pre">localhost:6006</span></code> in a browser afterwards.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># In a new terminal (don&#39;t close the above) , enter the image with a new shell.</span>
docker<span class="w"> </span>container<span class="w"> </span>ps
docker<span class="w"> </span><span class="nb">exec</span><span class="w"> </span>-it<span class="w"> </span>&lt;ISAAC_RAY_IMAGE_ID_FROM_CONTAINER_PS&gt;<span class="w"> </span>/bin/bash
<span class="c1"># Start a tuning run, with one parallel worker per GPU</span>
tensorboard<span class="w"> </span>--logdir<span class="o">=</span>.
</pre></div>
</div>
<p>Submitting resource-wrapped individual jobs instead of automatic tuning runs is described in the following file.</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-icon"><svg version="1.1" width="1.0em" height="1.0em" class="sd-octicon sd-octicon-code" viewBox="0 0 16 16" aria-hidden="true"><path d="m11.28 3.22 4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734L13.94 8l-3.72-3.72a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215Zm-6.56 0a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042L2.06 8l3.72 3.72a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L.47 8.53a.75.75 0 0 1 0-1.06Z"></path></svg></span><span class="sd-summary-text">scripts/reinforcement_learning/ray/wrap_resources.py</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Copyright (c) 2022-2025, The Isaac Lab Project Developers.</span>
<span class="c1"># All rights reserved.</span>
<span class="c1">#</span>
<span class="c1"># SPDX-License-Identifier: BSD-3-Clause</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">argparse</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">ray</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">util</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray.util.scheduling_strategies</span><span class="w"> </span><span class="kn">import</span> <span class="n">NodeAffinitySchedulingStrategy</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">This script dispatches sub-job(s) (individual jobs, use :file:`tuner.py` for tuning jobs)</span>
<span class="hll"><span class="sd">to worker(s) on GPU-enabled node(s) of a specific cluster as part of an resource-wrapped aggregate</span>
</span><span class="hll"><span class="sd">job. If no desired compute resources for each sub-job are specified,</span>
</span><span class="hll"><span class="sd">this script creates one worker per available node for each node with GPU(s) in the cluster.</span>
</span><span class="hll"><span class="sd">If the desired resources for each sub-job is specified,</span>
</span><span class="hll"><span class="sd">the maximum number of workers possible with the desired resources are created for each node</span>
</span><span class="hll"><span class="sd">with GPU(s) in the cluster. It is also possible to split available node resources for each node</span>
</span><span class="hll"><span class="sd">into the desired number of workers with the ``--num_workers`` flag, to be able to easily</span>
</span><span class="hll"><span class="sd">parallelize sub-jobs on multi-GPU nodes. Due to Isaac Lab requiring a GPU,</span>
</span><span class="hll"><span class="sd">this ignores all CPU only nodes such as loggers.</span>
</span><span class="hll">
</span><span class="hll"><span class="sd">Sub-jobs are matched with node(s) in a cluster via the following relation:</span>
</span><span class="hll"><span class="sd">sorted_nodes = Node sorted by descending GPUs, then descending CPUs, then descending RAM, then node ID</span>
</span><span class="hll"><span class="sd">node_submitted_to = sorted_nodes[job_index % total_node_count]</span>
</span><span class="hll">
</span><span class="hll"><span class="sd">To check the ordering of sorted nodes, supply the ``--test`` argument and run the script.</span>
</span><span class="hll">
</span><span class="hll"><span class="sd">Sub-jobs are separated by the + delimiter. The ``--sub_jobs`` argument must be the last</span>
</span><span class="hll"><span class="sd">argument supplied to the script.</span>
</span><span class="hll">
</span><span class="hll"><span class="sd">If there is more than one available worker, and more than one sub-job,</span>
</span><span class="hll"><span class="sd">sub-jobs will be executed in parallel. If there are more sub-jobs than workers, sub-jobs will</span>
</span><span class="hll"><span class="sd">be dispatched to workers as they become available. There is no limit on the number</span>
</span><span class="hll"><span class="sd">of sub-jobs that can be near-simultaneously submitted.</span>
</span><span class="hll">
</span><span class="hll"><span class="sd">This script is meant to be executed on a Ray cluster head node as an aggregate cluster job.</span>
</span><span class="hll"><span class="sd">To submit aggregate cluster jobs such as this script to one or more remote clusters,</span>
</span><span class="hll"><span class="sd">see :file:`../submit_isaac_ray_job.py`.</span>
</span><span class="hll">
</span><span class="hll"><span class="sd">KubeRay clusters on Google GKE can be created with :file:`../launch.py`</span>
</span><span class="hll">
</span><span class="hll"><span class="sd">Usage:</span>
</span><span class="hll">
</span><span class="hll"><span class="sd">.. code-block:: bash</span>
</span><span class="hll"><span class="sd">    # **Ensure that sub-jobs are separated by the ``+`` delimiter.**</span>
</span><span class="hll"><span class="sd">    # Generic Templates-----------------------------------</span>
</span><span class="hll"><span class="sd">    ./isaaclab.sh -p scripts/reinforcement_learning/ray/wrap_resources.py -h</span>
</span><span class="hll"><span class="sd">    # No resource isolation; no parallelization:</span>
</span><span class="hll"><span class="sd">    ./isaaclab.sh -p scripts/reinforcement_learning/ray/wrap_resources.py</span>
</span><span class="hll"><span class="sd">    --sub_jobs &lt;JOB0&gt;+&lt;JOB1&gt;+&lt;JOB2&gt;</span>
</span><span class="hll"><span class="sd">    # Automatic Resource Isolation; Example A: needed for parallelization</span>
</span><span class="hll"><span class="sd">    ./isaaclab.sh -p scripts/reinforcement_learning/ray/wrap_resources.py \</span>
</span><span class="hll"><span class="sd">    --num_workers &lt;NUM_TO_DIVIDE_TOTAL_RESOURCES_BY&gt; \</span>
</span><span class="hll"><span class="sd">    --sub_jobs &lt;JOB0&gt;+&lt;JOB1&gt;</span>
</span><span class="hll"><span class="sd">    # Manual Resource Isolation; Example B:  needed for parallelization</span>
</span><span class="hll"><span class="sd">    ./isaaclab.sh -p scripts/reinforcement_learning/ray/wrap_resources.py --num_cpu_per_worker &lt;CPU&gt; \</span>
</span><span class="hll"><span class="sd">    --gpu_per_worker &lt;GPU&gt; --ram_gb_per_worker &lt;RAM&gt; --sub_jobs &lt;JOB0&gt;+&lt;JOB1&gt;</span>
</span><span class="hll"><span class="sd">    # Manual Resource Isolation; Example C: Needed for parallelization, for heterogeneous workloads</span>
</span><span class="hll"><span class="sd">    ./isaaclab.sh -p scripts/reinforcement_learning/ray/wrap_resources.py --num_cpu_per_worker &lt;CPU&gt; \</span>
</span><span class="hll"><span class="sd">    --gpu_per_worker &lt;GPU1&gt; &lt;GPU2&gt; --ram_gb_per_worker &lt;RAM&gt; --sub_jobs &lt;JOB0&gt;+&lt;JOB1&gt;</span>
</span><span class="hll"><span class="sd">    # to see all arguments</span>
</span><span class="hll"><span class="sd">    ./isaaclab.sh -p scripts/reinforcement_learning/ray/wrap_resources.py -h</span>
</span><span class="hll"><span class="sd">&quot;&quot;&quot;</span>
</span><span class="hll">
</span>
<span class="k">def</span><span class="w"> </span><span class="nf">wrap_resources_to_jobs</span><span class="p">(</span><span class="n">jobs</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">args</span><span class="p">:</span> <span class="n">argparse</span><span class="o">.</span><span class="n">Namespace</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Provided a list of jobs, dispatch jobs to one worker per available node,</span>
<span class="sd">    unless otherwise specified by resource constraints.</span>

<span class="sd">    Args:</span>
<span class="sd">        jobs: bash commands to execute on a Ray cluster</span>
<span class="sd">        args: The arguments for resource allocation</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">ray</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">():</span>
        <span class="n">ray</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">address</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">ray_address</span><span class="p">,</span> <span class="n">log_to_driver</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">job_results</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">gpu_node_resources</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">get_gpu_node_resources</span><span class="p">(</span><span class="n">include_id</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">include_gb_ram</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">any</span><span class="p">([</span><span class="n">args</span><span class="o">.</span><span class="n">gpu_per_worker</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">cpu_per_worker</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">ram_gb_per_worker</span><span class="p">])</span> <span class="ow">and</span> <span class="n">args</span><span class="o">.</span><span class="n">num_workers</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Either specify only num_workers or only granular resources(GPU,CPU,RAM_GB).&quot;</span><span class="p">)</span>

    <span class="n">num_nodes</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">gpu_node_resources</span><span class="p">)</span>
    <span class="c1"># Populate arguments</span>
    <span class="n">formatted_node_resources</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;gpu_per_worker&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">gpu_node_resources</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s2">&quot;GPU&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_nodes</span><span class="p">)],</span>
        <span class="s2">&quot;cpu_per_worker&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">gpu_node_resources</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s2">&quot;CPU&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_nodes</span><span class="p">)],</span>
        <span class="s2">&quot;ram_gb_per_worker&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">gpu_node_resources</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s2">&quot;ram_gb&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_nodes</span><span class="p">)],</span>
        <span class="s2">&quot;num_workers&quot;</span><span class="p">:</span> <span class="n">args</span><span class="o">.</span><span class="n">num_workers</span><span class="p">,</span>  <span class="c1"># By default, 1 worker por node</span>
    <span class="p">}</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">fill_in_missing_resources</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">resources</span><span class="o">=</span><span class="n">formatted_node_resources</span><span class="p">,</span> <span class="n">policy</span><span class="o">=</span><span class="nb">min</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[INFO]: Number of GPU nodes found: </span><span class="si">{</span><span class="n">num_nodes</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">test</span><span class="p">:</span>
        <span class="n">jobs</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;nvidia-smi&quot;</span><span class="p">]</span> <span class="o">*</span> <span class="n">num_nodes</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">job</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">jobs</span><span class="p">):</span>
        <span class="n">gpu_node</span> <span class="o">=</span> <span class="n">gpu_node_resources</span><span class="p">[</span><span class="n">i</span> <span class="o">%</span> <span class="n">num_nodes</span><span class="p">]</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[INFO]: Submitting job </span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2"> of </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">jobs</span><span class="p">)</span><span class="si">}</span><span class="s2"> with job &#39;</span><span class="si">{</span><span class="n">job</span><span class="si">}</span><span class="s2">&#39; to node </span><span class="si">{</span><span class="n">gpu_node</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;[INFO]: Resource parameters: GPU: </span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">gpu_per_worker</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="sa">f</span><span class="s2">&quot; CPU: </span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">cpu_per_worker</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2"> RAM </span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">ram_gb_per_worker</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[INFO] For the node parameters, creating </span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">num_workers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2"> workers&quot;</span><span class="p">)</span>
        <span class="n">num_gpus</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">gpu_per_worker</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">/</span> <span class="n">args</span><span class="o">.</span><span class="n">num_workers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">num_cpus</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">cpu_per_worker</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">/</span> <span class="n">args</span><span class="o">.</span><span class="n">num_workers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">memory</span> <span class="o">=</span> <span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">ram_gb_per_worker</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span> <span class="o">/</span> <span class="n">args</span><span class="o">.</span><span class="n">num_workers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[INFO]: Requesting </span><span class="si">{</span><span class="n">num_gpus</span><span class="si">=}</span><span class="s2"> </span><span class="si">{</span><span class="n">num_cpus</span><span class="si">=}</span><span class="s2"> </span><span class="si">{</span><span class="n">memory</span><span class="si">=}</span><span class="s2"> id=</span><span class="si">{</span><span class="n">gpu_node</span><span class="p">[</span><span class="s1">&#39;id&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">job</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">remote_execute_job</span><span class="o">.</span><span class="n">options</span><span class="p">(</span>
            <span class="n">num_gpus</span><span class="o">=</span><span class="n">num_gpus</span><span class="p">,</span>
            <span class="n">num_cpus</span><span class="o">=</span><span class="n">num_cpus</span><span class="p">,</span>
            <span class="n">memory</span><span class="o">=</span><span class="n">memory</span><span class="p">,</span>
            <span class="n">scheduling_strategy</span><span class="o">=</span><span class="n">NodeAffinitySchedulingStrategy</span><span class="p">(</span><span class="n">gpu_node</span><span class="p">[</span><span class="s2">&quot;id&quot;</span><span class="p">],</span> <span class="n">soft</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
        <span class="p">)</span><span class="o">.</span><span class="n">remote</span><span class="p">(</span><span class="n">job</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Job </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">test</span><span class="p">)</span>
        <span class="n">job_results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">job</span><span class="p">)</span>

    <span class="n">results</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">job_results</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">result</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">results</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[INFO]: Job </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2"> result: </span><span class="si">{</span><span class="n">result</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;[INFO]: All jobs completed.&quot;</span><span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">&quot;Submit multiple jobs with optional GPU testing.&quot;</span><span class="p">)</span>
    <span class="n">parser</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">add_resource_arguments</span><span class="p">(</span><span class="n">arg_parser</span><span class="o">=</span><span class="n">parser</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--ray_address&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;the Ray address.&quot;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--test&quot;</span><span class="p">,</span>
        <span class="n">action</span><span class="o">=</span><span class="s2">&quot;store_true&quot;</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="p">(</span>
            <span class="s2">&quot;Run nvidia-smi test instead of the arbitrary job,&quot;</span>
            <span class="s2">&quot;can use as a sanity check prior to any jobs to check &quot;</span>
            <span class="s2">&quot;that GPU resources are correctly isolated.&quot;</span>
        <span class="p">),</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--sub_jobs&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
        <span class="n">nargs</span><span class="o">=</span><span class="n">argparse</span><span class="o">.</span><span class="n">REMAINDER</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;This should be last wrapper argument. Jobs separated by the + delimiter to run on a cluster.&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">sub_jobs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">jobs</span> <span class="o">=</span> <span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">sub_jobs</span><span class="p">)</span>
        <span class="n">formatted_jobs</span> <span class="o">=</span> <span class="n">jobs</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;+&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">formatted_jobs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[INFO]: Isaac Ray Wrapper received jobs </span><span class="si">{</span><span class="n">formatted_jobs</span><span class="si">=}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">wrap_resources_to_jobs</span><span class="p">(</span><span class="n">jobs</span><span class="o">=</span><span class="n">formatted_jobs</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details><p>Transferring files from the running container can be done as follows.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>container<span class="w"> </span>ps
docker<span class="w"> </span>cp<span class="w"> </span>&lt;ISAAC_RAY_IMAGE_ID_FROM_CONTAINER_PS&gt;:&lt;/path/in/container/file&gt;<span class="w">  </span>&lt;/path/on/host/&gt;
</pre></div>
</div>
<p>For tuning jobs, specify the tuning job / hyperparameter sweep as child class of <code class="xref py py-class docutils literal notranslate"><span class="pre">JobCfg</span></code> .
The included <code class="xref py py-class docutils literal notranslate"><span class="pre">JobCfg</span></code> only supports the <code class="docutils literal notranslate"><span class="pre">rl_games</span></code> workflow due to differences in
environment entrypoints and hydra arguments, although other workflows will work if provided a compatible
<code class="xref py py-class docutils literal notranslate"><span class="pre">JobCfg</span></code>.</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-icon"><svg version="1.1" width="1.0em" height="1.0em" class="sd-octicon sd-octicon-code" viewBox="0 0 16 16" aria-hidden="true"><path d="m11.28 3.22 4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734L13.94 8l-3.72-3.72a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215Zm-6.56 0a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042L2.06 8l3.72 3.72a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L.47 8.53a.75.75 0 0 1 0-1.06Z"></path></svg></span><span class="sd-summary-text">scripts/reinforcement_learning/ray/tuner.py (JobCfg definition)</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">JobCfg</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;To be compatible with :meth: invoke_tuning_run and :class:IsaacLabTuneTrainable,</span>
<span class="sd">    at a minimum, the tune job should inherit from this class.&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">:</span> <span class="nb">dict</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Runner args include command line arguments passed to the task.</span>
<span class="sd">        For example:</span>
<span class="sd">        cfg[&quot;runner_args&quot;][&quot;headless_singleton&quot;] = &quot;--headless&quot;</span>
<span class="sd">        cfg[&quot;runner_args&quot;][&quot;enable_cameras_singleton&quot;] = &quot;--enable_cameras&quot;</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="s2">&quot;runner_args&quot;</span> <span class="ow">in</span> <span class="n">cfg</span><span class="p">,</span> <span class="s2">&quot;No runner arguments specified.&quot;</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Task is the desired task to train on. For example:</span>
<span class="sd">        cfg[&quot;runner_args&quot;][&quot;--task&quot;] = tune.choice([&quot;Isaac-Cartpole-RGB-TheiaTiny-v0&quot;])</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="s2">&quot;--task&quot;</span> <span class="ow">in</span> <span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;runner_args&quot;</span><span class="p">],</span> <span class="s2">&quot;No task specified.&quot;</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Hydra args define the hyperparameters varied within the sweep. For example:</span>
<span class="sd">        cfg[&quot;hydra_args&quot;][&quot;agent.params.network.cnn.activation&quot;] = tune.choice([&quot;relu&quot;, &quot;elu&quot;])</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="s2">&quot;hydra_args&quot;</span> <span class="ow">in</span> <span class="n">cfg</span><span class="p">,</span> <span class="s2">&quot;No hyperparameters specified.&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cfg</span> <span class="o">=</span> <span class="n">cfg</span>
</pre></div>
</div>
</div>
</details><p>For example, see the following Cartpole Example configurations.</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-icon"><svg version="1.1" width="1.0em" height="1.0em" class="sd-octicon sd-octicon-code" viewBox="0 0 16 16" aria-hidden="true"><path d="m11.28 3.22 4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734L13.94 8l-3.72-3.72a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215Zm-6.56 0a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042L2.06 8l3.72 3.72a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L.47 8.53a.75.75 0 0 1 0-1.06Z"></path></svg></span><span class="sd-summary-text">scripts/reinforcement_learning/ray/hyperparameter_tuning/vision_cartpole_cfg.py</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Copyright (c) 2022-2025, The Isaac Lab Project Developers.</span>
<span class="c1"># All rights reserved.</span>
<span class="c1">#</span>
<span class="c1"># SPDX-License-Identifier: BSD-3-Clause</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pathlib</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sys</span>

<span class="c1"># Allow for import of items from the ray workflow.</span>
<span class="n">CUR_DIR</span> <span class="o">=</span> <span class="n">pathlib</span><span class="o">.</span><span class="n">Path</span><span class="p">(</span><span class="vm">__file__</span><span class="p">)</span><span class="o">.</span><span class="n">parent</span>
<span class="n">UTIL_DIR</span> <span class="o">=</span> <span class="n">CUR_DIR</span><span class="o">.</span><span class="n">parent</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="nb">str</span><span class="p">(</span><span class="n">UTIL_DIR</span><span class="p">),</span> <span class="nb">str</span><span class="p">(</span><span class="n">CUR_DIR</span><span class="p">)])</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">util</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">vision_cfg</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray</span><span class="w"> </span><span class="kn">import</span> <span class="n">tune</span>


<span class="k">class</span><span class="w"> </span><span class="nc">CartpoleRGBNoTuneJobCfg</span><span class="p">(</span><span class="n">vision_cfg</span><span class="o">.</span><span class="n">CameraJobCfg</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="p">{}):</span>
        <span class="n">cfg</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">populate_isaac_ray_cfg_args</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>
        <span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;runner_args&quot;</span><span class="p">][</span><span class="s2">&quot;--task&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">tune</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="s2">&quot;Isaac-Cartpole-RGB-v0&quot;</span><span class="p">])</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">cfg</span><span class="p">,</span> <span class="n">vary_env_count</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">vary_cnn</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">vary_mlp</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>


<span class="k">class</span><span class="w"> </span><span class="nc">CartpoleRGBCNNOnlyJobCfg</span><span class="p">(</span><span class="n">vision_cfg</span><span class="o">.</span><span class="n">CameraJobCfg</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="p">{}):</span>
        <span class="n">cfg</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">populate_isaac_ray_cfg_args</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>
        <span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;runner_args&quot;</span><span class="p">][</span><span class="s2">&quot;--task&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">tune</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="s2">&quot;Isaac-Cartpole-RGB-v0&quot;</span><span class="p">])</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">cfg</span><span class="p">,</span> <span class="n">vary_env_count</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">vary_cnn</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">vary_mlp</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>


<span class="k">class</span><span class="w"> </span><span class="nc">CartpoleRGBJobCfg</span><span class="p">(</span><span class="n">vision_cfg</span><span class="o">.</span><span class="n">CameraJobCfg</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="p">{}):</span>
        <span class="n">cfg</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">populate_isaac_ray_cfg_args</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>
        <span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;runner_args&quot;</span><span class="p">][</span><span class="s2">&quot;--task&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">tune</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="s2">&quot;Isaac-Cartpole-RGB-v0&quot;</span><span class="p">])</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">cfg</span><span class="p">,</span> <span class="n">vary_env_count</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">vary_cnn</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">vary_mlp</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>


<span class="k">class</span><span class="w"> </span><span class="nc">CartpoleResNetJobCfg</span><span class="p">(</span><span class="n">vision_cfg</span><span class="o">.</span><span class="n">ResNetCameraJob</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="p">{}):</span>
        <span class="n">cfg</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">populate_isaac_ray_cfg_args</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>
        <span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;runner_args&quot;</span><span class="p">][</span><span class="s2">&quot;--task&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">tune</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="s2">&quot;Isaac-Cartpole-RGB-ResNet18-v0&quot;</span><span class="p">])</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>


<span class="k">class</span><span class="w"> </span><span class="nc">CartpoleTheiaJobCfg</span><span class="p">(</span><span class="n">vision_cfg</span><span class="o">.</span><span class="n">TheiaCameraJob</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="p">{}):</span>
        <span class="n">cfg</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">populate_isaac_ray_cfg_args</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>
        <span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;runner_args&quot;</span><span class="p">][</span><span class="s2">&quot;--task&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">tune</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="s2">&quot;Isaac-Cartpole-RGB-TheiaTiny-v0&quot;</span><span class="p">])</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details></section>
<section id="remote-clusters">
<h2><a class="toc-backref" href="#id4" role="doc-backlink">Remote Clusters</a><a class="headerlink" href="#remote-clusters" title="Permalink to this heading">#</a></h2>
<p>Select one of the following methods to create a Ray cluster to accept and execute dispatched jobs.</p>
<section id="kuberay-setup">
<h3><a class="toc-backref" href="#id5" role="doc-backlink">KubeRay Setup</a><a class="headerlink" href="#kuberay-setup" title="Permalink to this heading">#</a></h3>
<p>If using KubeRay clusters on Google GKE with the batteries-included cluster launch file,
the following dependencies are also needed.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>-p<span class="w"> </span>-m<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>kubernetes<span class="w"> </span>Jinja2
</pre></div>
</div>
<p>For use on Kubernetes clusters with KubeRay,
such as Google Kubernetes Engine or Amazon Elastic Kubernetes Service, <code class="docutils literal notranslate"><span class="pre">kubectl</span></code> is required, and can
be installed via the <a class="reference external" href="https://kubernetes.io/docs/tasks/tools/">Kubernetes website</a> .</p>
<p>Google Cloud is currently the only platform tested, although
any cloud provider should work if one configures the following.</p>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>The <code class="docutils literal notranslate"><span class="pre">ray</span></code> command should be modified to use Isaac python, which could be achieved in a fashion similar to
<code class="docutils literal notranslate"><span class="pre">sed</span> <span class="pre">-i</span> <span class="pre">&quot;1i</span> <span class="pre">$(echo</span> <span class="pre">&quot;#!/workspace/isaaclab/_isaac_sim/python.sh&quot;)&quot;</span> <span class="pre">\</span>
<span class="pre">/isaac-sim/kit/python/bin/ray</span> <span class="pre">&amp;&amp;</span> <span class="pre">ln</span> <span class="pre">-s</span> <span class="pre">/isaac-sim/kit/python/bin/ray</span> <span class="pre">/usr/local/bin/ray</span></code>.</p>
</div>
<ul class="simple">
<li><p>An container registry (NGC, GCS artifact registry, AWS ECR, etc) with
an Isaac Lab image configured to support Ray. See <code class="docutils literal notranslate"><span class="pre">cluster_configs/Dockerfile</span></code> to see how to modify the <code class="docutils literal notranslate"><span class="pre">isaac-lab-base</span></code>
container for Ray compatibility. Ray should use the isaac sim python shebang, and <code class="docutils literal notranslate"><span class="pre">nvidia-smi</span></code>
should work within the container. Be careful with the setup here as
paths need to be configured correctly for everything to work. It’s likely that
the example dockerfile will work out of the box and can be pushed to the registry, as
long as the base image has already been built as in the container guide.</p></li>
<li><p>A Kubernetes setup with available NVIDIA RTX (likely <code class="docutils literal notranslate"><span class="pre">l4</span></code> or <code class="docutils literal notranslate"><span class="pre">l40</span></code> or <code class="docutils literal notranslate"><span class="pre">tesla-t4</span></code> or <code class="docutils literal notranslate"><span class="pre">a10</span></code>) GPU-passthrough node-pool resources,
that has access to your container registry/storage bucket and has the Ray operator enabled with correct IAM
permissions. This can be easily achieved with services such as Google GKE or AWS EKS,
provided that your account or organization has been granted a GPU-budget. It is recommended
to use manual kubernetes services as opposed to “autopilot” services for cost-effective
experimentation as this way clusters can be completely shut down when not in use, although
this may require installing the <a class="reference external" href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/google-gke.html">Nvidia GPU Operator</a> .</p></li>
<li><p>An <a class="reference external" href="https://mlflow.org/docs/latest/getting-started/logging-first-model/step1-tracking-server.html">MLFlow server</a> that your cluster has access to
(already included for Google Cloud, which can be referenced for the format and MLFlow integration).</p></li>
<li><p>A <code class="docutils literal notranslate"><span class="pre">kuberay.yaml.ninja</span></code> file that describes how to allocate resources (already included for
Google Cloud, which can be referenced for the format and MLFlow integration).</p></li>
</ul>
</section>
<section id="ray-clusters-without-kubernetes-setup">
<h3><a class="toc-backref" href="#id6" role="doc-backlink">Ray Clusters (Without Kubernetes) Setup</a><a class="headerlink" href="#ray-clusters-without-kubernetes-setup" title="Permalink to this heading">#</a></h3>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Modify the Ray command to use Isaac Python like in KubeRay clusters, and follow the same
steps for creating an image/cluster permissions.</p>
</div>
<p>See the <a class="reference external" href="https://docs.ray.io/en/latest/cluster/getting-started.html">Ray Clusters Overview</a> or
<a class="reference external" href="https://www.anyscale.com/product">Anyscale</a> for more information.</p>
<p>Also, create an <a class="reference external" href="https://mlflow.org/docs/latest/getting-started/logging-first-model/step1-tracking-server.html">MLFlow server</a> that your local
host and cluster have access to.</p>
</section>
<section id="shared-steps-between-kuberay-and-pure-ray-part-i">
<h3><a class="toc-backref" href="#id7" role="doc-backlink">Shared Steps Between KubeRay and Pure Ray Part I</a><a class="headerlink" href="#shared-steps-between-kuberay-and-pure-ray-part-i" title="Permalink to this heading">#</a></h3>
<p>1.) Install Ray on your local machine.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>-p<span class="w"> </span>-m<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>ray<span class="o">[</span>default<span class="o">]==</span><span class="m">2</span>.31.0
</pre></div>
</div>
<p>2.) Build the Isaac Ray image, and upload it to your container registry of choice.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Login with NGC (nvcr.io) registry first, see docker steps in repo.</span>
python3<span class="w"> </span>docker/container.py<span class="w"> </span>start
<span class="c1"># Build the special Isaac Lab Ray Image</span>
docker<span class="w"> </span>build<span class="w"> </span>-t<span class="w"> </span>&lt;REGISTRY/IMAGE_NAME&gt;<span class="w"> </span>-f<span class="w"> </span>scripts/reinforcement_learning/ray/cluster_configs/Dockerfile<span class="w"> </span>.
<span class="c1"># Push the image to your registry of choice.</span>
docker<span class="w"> </span>push<span class="w"> </span>&lt;REGISTRY/IMAGE_NAME&gt;
</pre></div>
</div>
</section>
<section id="kuberay-clusters-only">
<h3><a class="toc-backref" href="#id8" role="doc-backlink">KubeRay Clusters Only</a><a class="headerlink" href="#kuberay-clusters-only" title="Permalink to this heading">#</a></h3>
<p><a class="reference external" href="https://github.com/derailed/k9s">k9s</a> is a great tool for monitoring your clusters that can
easily be installed with <code class="docutils literal notranslate"><span class="pre">snap</span> <span class="pre">install</span> <span class="pre">k9s</span> <span class="pre">--devmode</span></code>.</p>
<p>1.) Verify cluster access, and that the correct operators are installed.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Verify cluster access</span>
kubectl<span class="w"> </span>cluster-info
<span class="c1"># If using a manually managed cluster (not Autopilot or the like)</span>
<span class="c1"># verify that there are node pools</span>
kubectl<span class="w"> </span>get<span class="w"> </span>nodes
<span class="c1"># Check that the ray operator is installed on the cluster</span>
<span class="c1"># should list rayclusters.ray.io , rayjobs.ray.io , and rayservices.ray.io</span>
kubectl<span class="w"> </span>get<span class="w"> </span>crds<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span>ray
<span class="c1"># Check that the NVIDIA Driver Operator is installed on the cluster</span>
<span class="c1"># should list clusterpolicies.nvidia.com</span>
kubectl<span class="w"> </span>get<span class="w"> </span>crds<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span>nvidia
</pre></div>
</div>
<p>2.) Create the KubeRay cluster and an MLFlow server for receiving logs
that your cluster has access to.
This can be done automatically for Google GKE,
where instructions are included in the following creation file.</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-icon"><svg version="1.1" width="1.0em" height="1.0em" class="sd-octicon sd-octicon-code" viewBox="0 0 16 16" aria-hidden="true"><path d="m11.28 3.22 4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734L13.94 8l-3.72-3.72a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215Zm-6.56 0a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042L2.06 8l3.72 3.72a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L.47 8.53a.75.75 0 0 1 0-1.06Z"></path></svg></span><span class="sd-summary-text">scripts/reinforcement_learning/ray/launch.py</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Copyright (c) 2022-2025, The Isaac Lab Project Developers.</span>
<span class="c1"># All rights reserved.</span>
<span class="c1">#</span>
<span class="c1"># SPDX-License-Identifier: BSD-3-Clause</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">argparse</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pathlib</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">subprocess</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">yaml</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">util</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">jinja2</span><span class="w"> </span><span class="kn">import</span> <span class="n">Environment</span><span class="p">,</span> <span class="n">FileSystemLoader</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">kubernetes</span><span class="w"> </span><span class="kn">import</span> <span class="n">config</span>

<span class="hll"><span class="sd">&quot;&quot;&quot;This script helps create one or more KubeRay clusters.</span>
</span><span class="hll">
</span><span class="hll"><span class="sd">Usage:</span>
</span><span class="hll">
</span><span class="hll"><span class="sd">.. code-block:: bash</span>
</span><span class="hll"><span class="sd">    # If the head node is stuck on container creating, make sure to create a secret</span>
</span><span class="hll"><span class="sd">    python3 scripts/reinforcement_learning/ray/launch.py -h</span>
</span><span class="hll">
</span><span class="hll"><span class="sd">    # Examples</span>
</span><span class="hll">
</span><span class="hll"><span class="sd">    # The following creates 8 GPUx1 nvidia l4 workers</span>
</span><span class="hll"><span class="sd">    python3 scripts/reinforcement_learning/ray/launch.py --cluster_host google_cloud \</span>
</span><span class="hll"><span class="sd">    --namespace &lt;NAMESPACE&gt; --image &lt;YOUR_ISAAC_RAY_IMAGE&gt; \</span>
</span><span class="hll"><span class="sd">    --num_workers 8 --num_clusters 1 --worker_accelerator nvidia-l4 --gpu_per_worker 1</span>
</span><span class="hll">
</span><span class="hll"><span class="sd">    # The following creates 1 GPUx1 nvidia l4 worker, 2 GPUx2 nvidia-tesla-t4 workers,</span>
</span><span class="hll"><span class="sd">    # and 2 GPUx4 nvidia-tesla-t4 GPU workers</span>
</span><span class="hll"><span class="sd">    python3 scripts/reinforcement_learning/ray/launch.py --cluster_host google_cloud \</span>
</span><span class="hll"><span class="sd">    --namespace &lt;NAMESPACE&gt; --image &lt;YOUR_ISAAC_RAY_IMAGE&gt; \</span>
</span><span class="hll"><span class="sd">    --num_workers 1 2 --num_clusters 1 \</span>
</span><span class="hll"><span class="sd">    --worker_accelerator nvidia-l4 nvidia-tesla-t4 --gpu_per_worker 1 2 4</span>
</span><span class="hll"><span class="sd">&quot;&quot;&quot;</span>
</span><span class="hll"><span class="n">RAY_DIR</span> <span class="o">=</span> <span class="n">pathlib</span><span class="o">.</span><span class="n">Path</span><span class="p">(</span><span class="vm">__file__</span><span class="p">)</span><span class="o">.</span><span class="n">parent</span>
</span>

<span class="k">def</span><span class="w"> </span><span class="nf">apply_manifest</span><span class="p">(</span><span class="n">args</span><span class="p">:</span> <span class="n">argparse</span><span class="o">.</span><span class="n">Namespace</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Provided a Jinja templated ray.io/v1alpha1 file,</span>
<span class="sd">    populate the arguments and create the cluster. Additionally, create</span>
<span class="sd">    kubernetes containers for resources separated by &#39;---&#39; from the rest</span>
<span class="sd">    of the file.</span>

<span class="sd">    Args:</span>
<span class="sd">        args: Possible arguments concerning cluster parameters.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Load Kubernetes configuration</span>
    <span class="n">config</span><span class="o">.</span><span class="n">load_kube_config</span><span class="p">()</span>

    <span class="c1"># Set up Jinja2 environment for loading templates</span>
    <span class="n">templates_dir</span> <span class="o">=</span> <span class="n">RAY_DIR</span> <span class="o">/</span> <span class="s2">&quot;cluster_configs&quot;</span> <span class="o">/</span> <span class="n">args</span><span class="o">.</span><span class="n">cluster_host</span>
    <span class="n">file_loader</span> <span class="o">=</span> <span class="n">FileSystemLoader</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">templates_dir</span><span class="p">))</span>
    <span class="n">jinja_env</span> <span class="o">=</span> <span class="n">Environment</span><span class="p">(</span><span class="n">loader</span><span class="o">=</span><span class="n">file_loader</span><span class="p">,</span> <span class="n">keep_trailing_newline</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">autoescape</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># Define template filename</span>
    <span class="n">template_file</span> <span class="o">=</span> <span class="s2">&quot;kuberay.yaml.jinja&quot;</span>

    <span class="c1"># Convert args namespace to a dictionary</span>
    <span class="n">template_params</span> <span class="o">=</span> <span class="nb">vars</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>

    <span class="c1"># Load and render the template</span>
    <span class="n">template</span> <span class="o">=</span> <span class="n">jinja_env</span><span class="o">.</span><span class="n">get_template</span><span class="p">(</span><span class="n">template_file</span><span class="p">)</span>
    <span class="n">file_contents</span> <span class="o">=</span> <span class="n">template</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="n">template_params</span><span class="p">)</span>

    <span class="c1"># Parse all YAML documents in the rendered template</span>
    <span class="n">all_yamls</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">yaml</span><span class="o">.</span><span class="n">safe_load_all</span><span class="p">(</span><span class="n">file_contents</span><span class="p">):</span>
        <span class="n">all_yamls</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span>

    <span class="c1"># Convert back to YAML string, preserving multiple documents</span>
    <span class="n">cleaned_yaml_string</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">doc</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">all_yamls</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">cleaned_yaml_string</span> <span class="o">+=</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">---</span><span class="se">\n</span><span class="s2">&quot;</span>
        <span class="n">cleaned_yaml_string</span> <span class="o">+=</span> <span class="n">yaml</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span>

    <span class="c1"># Apply the Kubernetes manifest using kubectl</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">cleaned_yaml_string</span><span class="p">)</span>
        <span class="n">subprocess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="s2">&quot;kubectl&quot;</span><span class="p">,</span> <span class="s2">&quot;apply&quot;</span><span class="p">,</span> <span class="s2">&quot;-f&quot;</span><span class="p">,</span> <span class="s2">&quot;-&quot;</span><span class="p">],</span> <span class="nb">input</span><span class="o">=</span><span class="n">cleaned_yaml_string</span><span class="p">,</span> <span class="n">text</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">check</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">except</span> <span class="n">subprocess</span><span class="o">.</span><span class="n">CalledProcessError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="n">exit</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;An error occurred while running `kubectl`: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">parse_args</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">argparse</span><span class="o">.</span><span class="n">Namespace</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Parse command-line arguments for Kubernetes deployment script.</span>

<span class="sd">    Returns:</span>
<span class="sd">        argparse.Namespace: Parsed command-line arguments.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">arg_parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span>
        <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Script to apply manifests to create Kubernetes objects for Ray clusters.&quot;</span><span class="p">,</span>
        <span class="n">formatter_class</span><span class="o">=</span><span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentDefaultsHelpFormatter</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">arg_parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--cluster_host&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="s2">&quot;google_cloud&quot;</span><span class="p">,</span>
        <span class="n">choices</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;google_cloud&quot;</span><span class="p">],</span>
        <span class="n">help</span><span class="o">=</span><span class="p">(</span>
            <span class="s2">&quot;In the cluster_configs directory, the name of the folder where a tune.yaml.jinja&quot;</span>
            <span class="s2">&quot;file exists defining the KubeRay config. Currently only google_cloud is supported.&quot;</span>
        <span class="p">),</span>
    <span class="p">)</span>

    <span class="n">arg_parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--name&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
        <span class="n">required</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="s2">&quot;isaacray&quot;</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Name of the Kubernetes deployment.&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">arg_parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--namespace&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
        <span class="n">required</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="s2">&quot;default&quot;</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Kubernetes namespace to deploy the Ray cluster.&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">arg_parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--service_acount_name&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">required</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&quot;default&quot;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The service account name to use.&quot;</span>
    <span class="p">)</span>

    <span class="n">arg_parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--image&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
        <span class="n">required</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Docker image for the Ray cluster pods.&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">arg_parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--worker_accelerator&quot;</span><span class="p">,</span>
        <span class="n">nargs</span><span class="o">=</span><span class="s2">&quot;+&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;nvidia-l4&quot;</span><span class="p">],</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;GPU accelerator name. Supply more than one for heterogeneous resources.&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">arg_parser</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">add_resource_arguments</span><span class="p">(</span><span class="n">arg_parser</span><span class="p">,</span> <span class="n">cluster_create_defaults</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="n">arg_parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--num_clusters&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;How many Ray Clusters to create.&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">arg_parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--num_head_cpu&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>  <span class="c1"># to be able to schedule partial CPU heads</span>
        <span class="n">default</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The number of CPUs to give the Ray head.&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">arg_parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--head_ram_gb&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;How many gigs of ram to give the Ray head&quot;</span><span class="p">)</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">arg_parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">util</span><span class="o">.</span><span class="n">fill_in_missing_resources</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">cluster_creation_flag</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">main</span><span class="p">():</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">parse_args</span><span class="p">()</span>

    <span class="k">if</span> <span class="s2">&quot;head&quot;</span> <span class="ow">in</span> <span class="n">args</span><span class="o">.</span><span class="n">name</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For compatibility with other scripts, do not include head in the name&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">num_clusters</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">apply_manifest</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">default_name</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">name</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">num_clusters</span><span class="p">):</span>
            <span class="n">args</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">default_name</span> <span class="o">+</span> <span class="s2">&quot;-&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
            <span class="n">apply_manifest</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details><p>For other cloud services, the <code class="docutils literal notranslate"><span class="pre">kuberay.yaml.ninja</span></code> will be similar to that of
Google’s.</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-icon"><svg version="1.1" width="1.0em" height="1.0em" class="sd-octicon sd-octicon-code" viewBox="0 0 16 16" aria-hidden="true"><path d="m11.28 3.22 4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734L13.94 8l-3.72-3.72a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215Zm-6.56 0a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042L2.06 8l3.72 3.72a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L.47 8.53a.75.75 0 0 1 0-1.06Z"></path></svg></span><span class="sd-summary-text">scripts/reinforcement_learning/ray/cluster_configs/google_cloud/kuberay.yaml.ninja</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Jinja is used for templating here as full helm setup is excessive for application</span>
<span class="n">apiVersion</span><span class="p">:</span> <span class="n">ray</span><span class="o">.</span><span class="n">io</span><span class="o">/</span><span class="n">v1alpha1</span>
<span class="n">kind</span><span class="p">:</span> <span class="n">RayCluster</span>
<span class="n">metadata</span><span class="p">:</span>
  <span class="n">name</span><span class="p">:</span> <span class="p">{{</span> <span class="n">name</span> <span class="p">}}</span>
  <span class="n">namespace</span><span class="p">:</span> <span class="p">{{</span> <span class="n">namespace</span> <span class="p">}}</span>
<span class="n">spec</span><span class="p">:</span>
  <span class="n">rayVersion</span><span class="p">:</span> <span class="s2">&quot;2.8.0&quot;</span>
  <span class="n">enableInTreeAutoscaling</span><span class="p">:</span> <span class="n">true</span>
  <span class="n">autoscalerOptions</span><span class="p">:</span>
    <span class="n">upscalingMode</span><span class="p">:</span> <span class="n">Default</span>
    <span class="n">idleTimeoutSeconds</span><span class="p">:</span> <span class="mi">120</span>
    <span class="n">imagePullPolicy</span><span class="p">:</span> <span class="n">Always</span>
    <span class="n">securityContext</span><span class="p">:</span> <span class="p">{}</span>
    <span class="n">envFrom</span><span class="p">:</span> <span class="p">[]</span>

  <span class="n">headGroupSpec</span><span class="p">:</span>
    <span class="n">rayStartParams</span><span class="p">:</span>
      <span class="n">block</span><span class="p">:</span> <span class="s2">&quot;true&quot;</span>
      <span class="n">dashboard</span><span class="o">-</span><span class="n">host</span><span class="p">:</span> <span class="mf">0.0.0.0</span>
      <span class="n">dashboard</span><span class="o">-</span><span class="n">port</span><span class="p">:</span> <span class="s2">&quot;8265&quot;</span>
      <span class="n">port</span><span class="p">:</span> <span class="s2">&quot;6379&quot;</span>
      <span class="n">include</span><span class="o">-</span><span class="n">dashboard</span><span class="p">:</span> <span class="s2">&quot;true&quot;</span>
      <span class="n">ray</span><span class="o">-</span><span class="n">debugger</span><span class="o">-</span><span class="n">external</span><span class="p">:</span> <span class="s2">&quot;true&quot;</span>
      <span class="nb">object</span><span class="o">-</span><span class="n">manager</span><span class="o">-</span><span class="n">port</span><span class="p">:</span> <span class="s2">&quot;8076&quot;</span>
      <span class="n">num</span><span class="o">-</span><span class="n">gpus</span><span class="p">:</span> <span class="s2">&quot;0&quot;</span>
      <span class="n">num</span><span class="o">-</span><span class="n">cpus</span><span class="p">:</span> <span class="s2">&quot;0&quot;</span> <span class="c1"># prevent scheduling jobs to the head node - workers only</span>
    <span class="n">headService</span><span class="p">:</span>
      <span class="n">apiVersion</span><span class="p">:</span> <span class="n">v1</span>
      <span class="n">kind</span><span class="p">:</span> <span class="n">Service</span>
      <span class="n">metadata</span><span class="p">:</span>
        <span class="n">name</span><span class="p">:</span> <span class="p">{{</span> <span class="n">name</span> <span class="p">}}</span><span class="o">-</span><span class="n">head</span>
      <span class="n">spec</span><span class="p">:</span>
        <span class="nb">type</span><span class="p">:</span> <span class="n">LoadBalancer</span>
    <span class="n">template</span><span class="p">:</span>
      <span class="n">metadata</span><span class="p">:</span>
        <span class="n">labels</span><span class="p">:</span>
          <span class="n">app</span><span class="o">.</span><span class="n">kubernetes</span><span class="o">.</span><span class="n">io</span><span class="o">/</span><span class="n">instance</span><span class="p">:</span> <span class="n">tuner</span>
          <span class="n">app</span><span class="o">.</span><span class="n">kubernetes</span><span class="o">.</span><span class="n">io</span><span class="o">/</span><span class="n">name</span><span class="p">:</span> <span class="n">kuberay</span>
          <span class="n">cloud</span><span class="o">.</span><span class="n">google</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">gke</span><span class="o">-</span><span class="n">ray</span><span class="o">-</span><span class="n">node</span><span class="o">-</span><span class="nb">type</span><span class="p">:</span> <span class="n">head</span>
      <span class="n">spec</span><span class="p">:</span>
        <span class="n">serviceAccountName</span><span class="p">:</span> <span class="p">{{</span> <span class="n">service_account_name</span> <span class="p">}}</span>
        <span class="n">affinity</span><span class="p">:</span> <span class="p">{}</span>
        <span class="n">securityContext</span><span class="p">:</span>
          <span class="n">fsGroup</span><span class="p">:</span> <span class="mi">100</span>
        <span class="n">containers</span><span class="p">:</span>
          <span class="o">-</span> <span class="n">env</span><span class="p">:</span>
            <span class="n">image</span><span class="p">:</span> <span class="p">{{</span> <span class="n">image</span> <span class="p">}}</span>
            <span class="n">imagePullPolicy</span><span class="p">:</span> <span class="n">Always</span>
            <span class="n">name</span><span class="p">:</span> <span class="n">head</span>
            <span class="n">resources</span><span class="p">:</span>
              <span class="n">limits</span><span class="p">:</span>
                <span class="n">cpu</span><span class="p">:</span> <span class="s2">&quot;{{ num_head_cpu }}&quot;</span>
                <span class="n">memory</span><span class="p">:</span> <span class="p">{{</span> <span class="n">head_ram_gb</span> <span class="p">}}</span><span class="n">G</span>
                <span class="n">nvidia</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">gpu</span><span class="p">:</span> <span class="s2">&quot;0&quot;</span>
              <span class="n">requests</span><span class="p">:</span>
                <span class="n">cpu</span><span class="p">:</span> <span class="s2">&quot;{{ num_head_cpu }}&quot;</span>
                <span class="n">memory</span><span class="p">:</span> <span class="p">{{</span> <span class="n">head_ram_gb</span> <span class="p">}}</span><span class="n">G</span>
                <span class="n">nvidia</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">gpu</span><span class="p">:</span> <span class="s2">&quot;0&quot;</span>
            <span class="n">securityContext</span><span class="p">:</span> <span class="p">{}</span>
            <span class="n">volumeMounts</span><span class="p">:</span>
              <span class="o">-</span> <span class="n">mountPath</span><span class="p">:</span> <span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">ray</span>
                <span class="n">name</span><span class="p">:</span> <span class="n">ray</span><span class="o">-</span><span class="n">logs</span>
            <span class="n">command</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;/bin/bash&quot;</span><span class="p">,</span> <span class="s2">&quot;-c&quot;</span><span class="p">,</span> <span class="s2">&quot;ray start --head --port=6379 --object-manager-port=8076 --dashboard-host=0.0.0.0 --dashboard-port=8265 --include-dashboard=true &amp;&amp; tail -f /dev/null&quot;</span><span class="p">]</span>
          <span class="o">-</span> <span class="n">image</span><span class="p">:</span> <span class="n">fluent</span><span class="o">/</span><span class="n">fluent</span><span class="o">-</span><span class="n">bit</span><span class="p">:</span><span class="mf">1.9.6</span>
            <span class="n">name</span><span class="p">:</span> <span class="n">fluentbit</span>
            <span class="n">resources</span><span class="p">:</span>
              <span class="n">limits</span><span class="p">:</span>
                <span class="n">cpu</span><span class="p">:</span> <span class="mi">100</span><span class="n">m</span>
                <span class="n">memory</span><span class="p">:</span> <span class="mi">128</span><span class="n">Mi</span>
              <span class="n">requests</span><span class="p">:</span>
                <span class="n">cpu</span><span class="p">:</span> <span class="mi">100</span><span class="n">m</span>
                <span class="n">memory</span><span class="p">:</span> <span class="mi">128</span><span class="n">Mi</span>
            <span class="n">volumeMounts</span><span class="p">:</span>
              <span class="o">-</span> <span class="n">mountPath</span><span class="p">:</span> <span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">ray</span>
                <span class="n">name</span><span class="p">:</span> <span class="n">ray</span><span class="o">-</span><span class="n">logs</span>
        <span class="n">imagePullSecrets</span><span class="p">:</span> <span class="p">[]</span>
        <span class="n">nodeSelector</span><span class="p">:</span>
          <span class="n">iam</span><span class="o">.</span><span class="n">gke</span><span class="o">.</span><span class="n">io</span><span class="o">/</span><span class="n">gke</span><span class="o">-</span><span class="n">metadata</span><span class="o">-</span><span class="n">server</span><span class="o">-</span><span class="n">enabled</span><span class="p">:</span> <span class="s2">&quot;true&quot;</span>
        <span class="n">volumes</span><span class="p">:</span>
          <span class="o">-</span> <span class="n">configMap</span><span class="p">:</span>
              <span class="n">name</span><span class="p">:</span> <span class="n">fluentbit</span><span class="o">-</span><span class="n">config</span>
            <span class="n">name</span><span class="p">:</span> <span class="n">fluentbit</span><span class="o">-</span><span class="n">config</span>
          <span class="o">-</span> <span class="n">name</span><span class="p">:</span> <span class="n">ray</span><span class="o">-</span><span class="n">logs</span>
            <span class="n">emptyDir</span><span class="p">:</span> <span class="p">{}</span>

  <span class="n">workerGroupSpecs</span><span class="p">:</span>
    <span class="p">{</span><span class="o">%</span> <span class="k">for</span> <span class="n">it</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">gpu_per_worker</span><span class="o">|</span><span class="n">length</span><span class="p">)</span> <span class="o">%</span><span class="p">}</span>
    <span class="o">-</span> <span class="n">groupName</span><span class="p">:</span> <span class="s2">&quot;{{ worker_accelerator[it] }}x{{ gpu_per_worker[it] }}-cpu-{{ cpu_per_worker[it] }}-ram-gb-{{ ram_gb_per_worker[it] }}&quot;</span>
      <span class="n">replicas</span><span class="p">:</span> <span class="p">{{</span> <span class="n">num_workers</span><span class="p">[</span><span class="n">it</span><span class="p">]</span> <span class="p">}}</span>
      <span class="n">maxReplicas</span><span class="p">:</span> <span class="p">{{</span> <span class="n">num_workers</span><span class="p">[</span><span class="n">it</span><span class="p">]</span> <span class="p">}}</span>
      <span class="n">minReplicas</span><span class="p">:</span> <span class="p">{{</span> <span class="n">num_workers</span><span class="p">[</span><span class="n">it</span><span class="p">]</span> <span class="p">}}</span>
      <span class="n">rayStartParams</span><span class="p">:</span>
        <span class="n">block</span><span class="p">:</span> <span class="s2">&quot;true&quot;</span>
        <span class="n">ray</span><span class="o">-</span><span class="n">debugger</span><span class="o">-</span><span class="n">external</span><span class="p">:</span> <span class="s2">&quot;true&quot;</span>
        <span class="n">replicas</span><span class="p">:</span> <span class="s2">&quot;{{num_workers[it]}}&quot;</span>
      <span class="n">template</span><span class="p">:</span>
        <span class="n">metadata</span><span class="p">:</span>
          <span class="n">annotations</span><span class="p">:</span> <span class="p">{}</span>
          <span class="n">labels</span><span class="p">:</span>
            <span class="n">app</span><span class="o">.</span><span class="n">kubernetes</span><span class="o">.</span><span class="n">io</span><span class="o">/</span><span class="n">instance</span><span class="p">:</span> <span class="n">tuner</span>
            <span class="n">app</span><span class="o">.</span><span class="n">kubernetes</span><span class="o">.</span><span class="n">io</span><span class="o">/</span><span class="n">name</span><span class="p">:</span> <span class="n">kuberay</span>
            <span class="n">cloud</span><span class="o">.</span><span class="n">google</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">gke</span><span class="o">-</span><span class="n">ray</span><span class="o">-</span><span class="n">node</span><span class="o">-</span><span class="nb">type</span><span class="p">:</span> <span class="n">worker</span>
        <span class="n">spec</span><span class="p">:</span>
          <span class="n">serviceAccountName</span><span class="p">:</span> <span class="p">{{</span> <span class="n">service_account_name</span> <span class="p">}}</span>
          <span class="n">affinity</span><span class="p">:</span> <span class="p">{}</span>
          <span class="n">securityContext</span><span class="p">:</span>
            <span class="n">fsGroup</span><span class="p">:</span> <span class="mi">100</span>
          <span class="n">containers</span><span class="p">:</span>
            <span class="o">-</span> <span class="n">env</span><span class="p">:</span>
              <span class="o">-</span> <span class="n">name</span><span class="p">:</span> <span class="n">NVIDIA_VISIBLE_DEVICES</span>
                <span class="n">value</span><span class="p">:</span> <span class="s2">&quot;all&quot;</span>
              <span class="o">-</span> <span class="n">name</span><span class="p">:</span> <span class="n">NVIDIA_DRIVER_CAPABILITIES</span>
                <span class="n">value</span><span class="p">:</span> <span class="s2">&quot;compute,utility&quot;</span>

              <span class="n">image</span><span class="p">:</span> <span class="p">{{</span> <span class="n">image</span> <span class="p">}}</span>
              <span class="n">imagePullPolicy</span><span class="p">:</span> <span class="n">Always</span>
              <span class="n">name</span><span class="p">:</span> <span class="n">ray</span><span class="o">-</span><span class="n">worker</span>
              <span class="n">resources</span><span class="p">:</span>
                <span class="n">limits</span><span class="p">:</span>
                  <span class="n">cpu</span><span class="p">:</span> <span class="s2">&quot;{{ cpu_per_worker[it] }}&quot;</span>
                  <span class="n">memory</span><span class="p">:</span> <span class="p">{{</span> <span class="n">ram_gb_per_worker</span><span class="p">[</span><span class="n">it</span><span class="p">]</span> <span class="p">}}</span><span class="n">G</span>
                  <span class="n">nvidia</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">gpu</span><span class="p">:</span> <span class="s2">&quot;{{ gpu_per_worker[it] }}&quot;</span>
                <span class="n">requests</span><span class="p">:</span>
                  <span class="n">cpu</span><span class="p">:</span> <span class="s2">&quot;{{ cpu_per_worker[it] }}&quot;</span>
                  <span class="n">memory</span><span class="p">:</span> <span class="p">{{</span> <span class="n">ram_gb_per_worker</span><span class="p">[</span><span class="n">it</span><span class="p">]</span> <span class="p">}}</span><span class="n">G</span>
                  <span class="n">nvidia</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">gpu</span><span class="p">:</span> <span class="s2">&quot;{{ gpu_per_worker[it] }}&quot;</span>
              <span class="n">securityContext</span><span class="p">:</span> <span class="p">{}</span>
              <span class="n">volumeMounts</span><span class="p">:</span>
                <span class="o">-</span> <span class="n">mountPath</span><span class="p">:</span> <span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">ray</span>
                  <span class="n">name</span><span class="p">:</span> <span class="n">ray</span><span class="o">-</span><span class="n">logs</span>
              <span class="n">command</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;/bin/bash&quot;</span><span class="p">,</span> <span class="s2">&quot;-c&quot;</span><span class="p">,</span> <span class="s2">&quot;ray start --address={{name}}-head.{{ namespace }}.svc.cluster.local:6379 &amp;&amp; tail -f /dev/null&quot;</span><span class="p">]</span>
            <span class="o">-</span> <span class="n">image</span><span class="p">:</span> <span class="n">fluent</span><span class="o">/</span><span class="n">fluent</span><span class="o">-</span><span class="n">bit</span><span class="p">:</span><span class="mf">1.9.6</span>
              <span class="n">name</span><span class="p">:</span> <span class="n">fluentbit</span>
              <span class="n">resources</span><span class="p">:</span>
                <span class="n">limits</span><span class="p">:</span>
                  <span class="n">cpu</span><span class="p">:</span> <span class="mi">100</span><span class="n">m</span>
                  <span class="n">memory</span><span class="p">:</span> <span class="mi">128</span><span class="n">Mi</span>
                <span class="n">requests</span><span class="p">:</span>
                  <span class="n">cpu</span><span class="p">:</span> <span class="mi">100</span><span class="n">m</span>
                  <span class="n">memory</span><span class="p">:</span> <span class="mi">128</span><span class="n">Mi</span>
              <span class="n">volumeMounts</span><span class="p">:</span>
                <span class="o">-</span> <span class="n">mountPath</span><span class="p">:</span> <span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">ray</span>
                  <span class="n">name</span><span class="p">:</span> <span class="n">ray</span><span class="o">-</span><span class="n">logs</span>

          <span class="n">imagePullSecrets</span><span class="p">:</span> <span class="p">[]</span>
          <span class="n">nodeSelector</span><span class="p">:</span>
            <span class="n">cloud</span><span class="o">.</span><span class="n">google</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">gke</span><span class="o">-</span><span class="n">accelerator</span><span class="p">:</span> <span class="p">{{</span> <span class="n">worker_accelerator</span><span class="p">[</span><span class="n">it</span><span class="p">]</span> <span class="p">}}</span>
            <span class="n">iam</span><span class="o">.</span><span class="n">gke</span><span class="o">.</span><span class="n">io</span><span class="o">/</span><span class="n">gke</span><span class="o">-</span><span class="n">metadata</span><span class="o">-</span><span class="n">server</span><span class="o">-</span><span class="n">enabled</span><span class="p">:</span> <span class="s2">&quot;true&quot;</span>
          <span class="n">tolerations</span><span class="p">:</span>
            <span class="o">-</span> <span class="n">key</span><span class="p">:</span> <span class="s2">&quot;nvidia.com/gpu&quot;</span>
              <span class="n">operator</span><span class="p">:</span> <span class="s2">&quot;Exists&quot;</span>
              <span class="n">effect</span><span class="p">:</span> <span class="s2">&quot;NoSchedule&quot;</span>
          <span class="n">volumes</span><span class="p">:</span>
            <span class="o">-</span> <span class="n">configMap</span><span class="p">:</span>
                <span class="n">name</span><span class="p">:</span> <span class="n">fluentbit</span><span class="o">-</span><span class="n">config</span>
              <span class="n">name</span><span class="p">:</span> <span class="n">fluentbit</span><span class="o">-</span><span class="n">config</span>
            <span class="o">-</span> <span class="n">name</span><span class="p">:</span> <span class="n">ray</span><span class="o">-</span><span class="n">logs</span>
              <span class="n">emptyDir</span><span class="p">:</span> <span class="p">{}</span>
    <span class="p">{</span><span class="o">%</span> <span class="n">endfor</span> <span class="o">%</span><span class="p">}</span>

<span class="o">---</span>
<span class="c1"># ML Flow Server - for fetching logs</span>
<span class="n">apiVersion</span><span class="p">:</span> <span class="n">apps</span><span class="o">/</span><span class="n">v1</span>
<span class="n">kind</span><span class="p">:</span> <span class="n">Deployment</span>
<span class="n">metadata</span><span class="p">:</span>
  <span class="n">name</span><span class="p">:</span> <span class="p">{{</span><span class="n">name</span><span class="p">}}</span><span class="o">-</span><span class="n">mlflow</span>
  <span class="n">namespace</span><span class="p">:</span> <span class="p">{{</span> <span class="n">namespace</span> <span class="p">}}</span>
<span class="n">spec</span><span class="p">:</span>
  <span class="n">replicas</span><span class="p">:</span> <span class="mi">1</span>
  <span class="n">selector</span><span class="p">:</span>
    <span class="n">matchLabels</span><span class="p">:</span>
      <span class="n">app</span><span class="p">:</span> <span class="n">mlflow</span>
  <span class="n">template</span><span class="p">:</span>
    <span class="n">metadata</span><span class="p">:</span>
      <span class="n">labels</span><span class="p">:</span>
        <span class="n">app</span><span class="p">:</span> <span class="n">mlflow</span>
    <span class="n">spec</span><span class="p">:</span>
      <span class="n">containers</span><span class="p">:</span>
      <span class="o">-</span> <span class="n">name</span><span class="p">:</span> <span class="n">mlflow</span>
        <span class="n">image</span><span class="p">:</span> <span class="n">ghcr</span><span class="o">.</span><span class="n">io</span><span class="o">/</span><span class="n">mlflow</span><span class="o">/</span><span class="n">mlflow</span><span class="p">:</span><span class="n">v2</span><span class="mf">.9.2</span>
        <span class="n">ports</span><span class="p">:</span>
        <span class="o">-</span> <span class="n">containerPort</span><span class="p">:</span> <span class="mi">5000</span>
        <span class="n">command</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;mlflow&quot;</span><span class="p">]</span>
        <span class="n">args</span><span class="p">:</span>
        <span class="o">-</span> <span class="n">server</span>
        <span class="o">-</span> <span class="o">--</span><span class="n">host</span><span class="o">=</span><span class="mf">0.0.0.0</span>
        <span class="o">-</span> <span class="o">--</span><span class="n">port</span><span class="o">=</span><span class="mi">5000</span>
        <span class="o">-</span> <span class="o">--</span><span class="n">backend</span><span class="o">-</span><span class="n">store</span><span class="o">-</span><span class="n">uri</span><span class="o">=</span><span class="n">sqlite</span><span class="p">:</span><span class="o">///</span><span class="n">mlflow</span><span class="o">.</span><span class="n">db</span>
<span class="o">---</span>
<span class="c1"># ML Flow Service (for port forwarding, kubectl port-forward service/{name}-mlflow 5000:5000)</span>
<span class="n">apiVersion</span><span class="p">:</span> <span class="n">v1</span>
<span class="n">kind</span><span class="p">:</span> <span class="n">Service</span>
<span class="n">metadata</span><span class="p">:</span>
  <span class="n">name</span><span class="p">:</span> <span class="p">{{</span><span class="n">name</span><span class="p">}}</span><span class="o">-</span><span class="n">mlflow</span>
  <span class="n">namespace</span><span class="p">:</span> <span class="p">{{</span> <span class="n">namespace</span> <span class="p">}}</span>
<span class="n">spec</span><span class="p">:</span>
  <span class="n">selector</span><span class="p">:</span>
    <span class="n">app</span><span class="p">:</span> <span class="n">mlflow</span>
  <span class="n">ports</span><span class="p">:</span>
  <span class="o">-</span> <span class="n">port</span><span class="p">:</span> <span class="mi">5000</span>
    <span class="n">targetPort</span><span class="p">:</span> <span class="mi">5000</span>
  <span class="nb">type</span><span class="p">:</span> <span class="n">ClusterIP</span>
</pre></div>
</div>
</div>
</details><p>3.) Fetch the KubeRay cluster IP addresses, and the MLFLow Server IP.
This can be done automatically for KubeRay clusters,
where instructions are included in the following fetching file.
The KubeRay clusters are saved to a file, but the MLFLow Server IP is
printed.</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-icon"><svg version="1.1" width="1.0em" height="1.0em" class="sd-octicon sd-octicon-code" viewBox="0 0 16 16" aria-hidden="true"><path d="m11.28 3.22 4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734L13.94 8l-3.72-3.72a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215Zm-6.56 0a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042L2.06 8l3.72 3.72a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L.47 8.53a.75.75 0 0 1 0-1.06Z"></path></svg></span><span class="sd-summary-text">scripts/reinforcement_learning/ray/grok_cluster_with_kubectl.py</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Copyright (c) 2022-2025, The Isaac Lab Project Developers.</span>
<span class="c1"># All rights reserved.</span>
<span class="c1">#</span>
<span class="c1"># SPDX-License-Identifier: BSD-3-Clause</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">argparse</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">re</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">subprocess</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">threading</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">concurrent.futures</span><span class="w"> </span><span class="kn">import</span> <span class="n">ThreadPoolExecutor</span><span class="p">,</span> <span class="n">as_completed</span>

<span class="hll"><span class="sd">&quot;&quot;&quot;</span>
</span><span class="hll"><span class="sd">This script requires that kubectl is installed and KubeRay was used to create the cluster.</span>
</span><span class="hll">
</span><span class="hll"><span class="sd">Creates a config file containing ``name: &lt;NAME&gt; address: http://&lt;IP&gt;:&lt;PORT&gt;`` on</span>
</span><span class="hll"><span class="sd">a new line for each cluster, and also fetches the MLFlow URI.</span>
</span><span class="hll">
</span><span class="hll"><span class="sd">Usage:</span>
</span><span class="hll">
</span><span class="hll"><span class="sd">.. code-block:: bash</span>
</span><span class="hll">
</span><span class="hll"><span class="sd">    python3 scripts/reinforcement_learning/ray/grok_cluster_with_kubectl.py</span>
</span><span class="hll"><span class="sd">    # For options, supply -h arg</span>
</span><span class="hll"><span class="sd">&quot;&quot;&quot;</span>
</span>

<span class="k">def</span><span class="w"> </span><span class="nf">get_namespace</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Get the current Kubernetes namespace from the context, fallback to default if not set&quot;&quot;&quot;</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">namespace</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">subprocess</span><span class="o">.</span><span class="n">check_output</span><span class="p">([</span><span class="s2">&quot;kubectl&quot;</span><span class="p">,</span> <span class="s2">&quot;config&quot;</span><span class="p">,</span> <span class="s2">&quot;view&quot;</span><span class="p">,</span> <span class="s2">&quot;--minify&quot;</span><span class="p">,</span> <span class="s2">&quot;--output&quot;</span><span class="p">,</span> <span class="s2">&quot;jsonpath={..namespace}&quot;</span><span class="p">])</span>
            <span class="o">.</span><span class="n">decode</span><span class="p">()</span>
            <span class="o">.</span><span class="n">strip</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">namespace</span><span class="p">:</span>
            <span class="n">namespace</span> <span class="o">=</span> <span class="s2">&quot;default&quot;</span>
    <span class="k">except</span> <span class="n">subprocess</span><span class="o">.</span><span class="n">CalledProcessError</span><span class="p">:</span>
        <span class="n">namespace</span> <span class="o">=</span> <span class="s2">&quot;default&quot;</span>
    <span class="k">return</span> <span class="n">namespace</span>


<span class="k">def</span><span class="w"> </span><span class="nf">get_pods</span><span class="p">(</span><span class="n">namespace</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;default&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">tuple</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Get a list of all of the pods in the namespace&quot;&quot;&quot;</span>
    <span class="n">cmd</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;kubectl&quot;</span><span class="p">,</span> <span class="s2">&quot;get&quot;</span><span class="p">,</span> <span class="s2">&quot;pods&quot;</span><span class="p">,</span> <span class="s2">&quot;-n&quot;</span><span class="p">,</span> <span class="n">namespace</span><span class="p">,</span> <span class="s2">&quot;--no-headers&quot;</span><span class="p">]</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">subprocess</span><span class="o">.</span><span class="n">check_output</span><span class="p">(</span><span class="n">cmd</span><span class="p">)</span><span class="o">.</span><span class="n">decode</span><span class="p">()</span>
    <span class="n">pods</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">output</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">):</span>
        <span class="n">fields</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
        <span class="n">pod_name</span> <span class="o">=</span> <span class="n">fields</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">status</span> <span class="o">=</span> <span class="n">fields</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">pods</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">pod_name</span><span class="p">,</span> <span class="n">status</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">pods</span>


<span class="k">def</span><span class="w"> </span><span class="nf">get_clusters</span><span class="p">(</span><span class="n">pods</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span> <span class="n">cluster_name_prefix</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">set</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get unique cluster name(s). Works for one or more clusters, based off of the number of head nodes.</span>
<span class="sd">    Excludes MLflow deployments.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">clusters</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">pod_name</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">pods</span><span class="p">:</span>
        <span class="c1"># Skip MLflow pods</span>
        <span class="k">if</span> <span class="s2">&quot;-mlflow&quot;</span> <span class="ow">in</span> <span class="n">pod_name</span><span class="p">:</span>
            <span class="k">continue</span>

        <span class="n">match</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;(&quot;</span> <span class="o">+</span> <span class="n">re</span><span class="o">.</span><span class="n">escape</span><span class="p">(</span><span class="n">cluster_name_prefix</span><span class="p">)</span> <span class="o">+</span> <span class="sa">r</span><span class="s2">&quot;[-\w]+)&quot;</span><span class="p">,</span> <span class="n">pod_name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">match</span><span class="p">:</span>
            <span class="c1"># Get base name without head/worker suffix (skip workers)</span>
            <span class="k">if</span> <span class="s2">&quot;head&quot;</span> <span class="ow">in</span> <span class="n">pod_name</span><span class="p">:</span>
                <span class="n">base_name</span> <span class="o">=</span> <span class="n">match</span><span class="o">.</span><span class="n">group</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;-head&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">clusters</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">base_name</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">clusters</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">get_mlflow_info</span><span class="p">(</span><span class="n">namespace</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">cluster_prefix</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;isaacray&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get MLflow service information if it exists in the namespace with the given prefix.</span>
<span class="sd">    Only works for a single cluster instance.</span>
<span class="sd">    Args:</span>
<span class="sd">        namespace: Kubernetes namespace</span>
<span class="sd">        cluster_prefix: Base cluster name (without -head/-worker suffixes)</span>
<span class="sd">    Returns:</span>
<span class="sd">        MLflow service URL</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Strip any -head or -worker suffixes to get base name</span>
    <span class="k">if</span> <span class="n">namespace</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">namespace</span> <span class="o">=</span> <span class="n">get_namespace</span><span class="p">()</span>
    <span class="n">pods</span> <span class="o">=</span> <span class="n">get_pods</span><span class="p">(</span><span class="n">namespace</span><span class="o">=</span><span class="n">namespace</span><span class="p">)</span>
    <span class="n">clusters</span> <span class="o">=</span> <span class="n">get_clusters</span><span class="p">(</span><span class="n">pods</span><span class="o">=</span><span class="n">pods</span><span class="p">,</span> <span class="n">cluster_name_prefix</span><span class="o">=</span><span class="n">cluster_prefix</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">clusters</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;More than one cluster matches prefix, could not automatically determine mlflow info.&quot;</span><span class="p">)</span>
    <span class="n">mlflow_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">cluster_prefix</span><span class="si">}</span><span class="s2">-mlflow&quot;</span>

    <span class="n">cmd</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;kubectl&quot;</span><span class="p">,</span> <span class="s2">&quot;get&quot;</span><span class="p">,</span> <span class="s2">&quot;svc&quot;</span><span class="p">,</span> <span class="n">mlflow_name</span><span class="p">,</span> <span class="s2">&quot;-n&quot;</span><span class="p">,</span> <span class="n">namespace</span><span class="p">,</span> <span class="s2">&quot;--no-headers&quot;</span><span class="p">]</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">subprocess</span><span class="o">.</span><span class="n">check_output</span><span class="p">(</span><span class="n">cmd</span><span class="p">)</span><span class="o">.</span><span class="n">decode</span><span class="p">()</span>
        <span class="n">fields</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>

        <span class="c1"># Get cluster IP</span>
        <span class="n">cluster_ip</span> <span class="o">=</span> <span class="n">fields</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">port</span> <span class="o">=</span> <span class="s2">&quot;5000&quot;</span>  <span class="c1"># Default MLflow port</span>
        <span class="c1"># This needs to be http to be resolved. HTTPS can&#39;t be resolved</span>
        <span class="c1"># This should be fine as it is on a subnet on the cluster regardless</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;http://</span><span class="si">{</span><span class="n">cluster_ip</span><span class="si">}</span><span class="s2">:</span><span class="si">{</span><span class="n">port</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="k">except</span> <span class="n">subprocess</span><span class="o">.</span><span class="n">CalledProcessError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Could not grok MLflow: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># Fixed f-string</span>


<span class="k">def</span><span class="w"> </span><span class="nf">check_clusters_running</span><span class="p">(</span><span class="n">pods</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span> <span class="n">clusters</span><span class="p">:</span> <span class="nb">set</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Check that all of the pods in all provided clusters are running.</span>

<span class="sd">    Args:</span>
<span class="sd">        pods (list): A list of tuples where each tuple contains the pod name and its status.</span>
<span class="sd">        clusters (set): A set of cluster names to check.</span>

<span class="sd">    Returns:</span>
<span class="sd">        bool: True if all pods in any of the clusters are running, False otherwise.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">clusters_running</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">for</span> <span class="n">cluster</span> <span class="ow">in</span> <span class="n">clusters</span><span class="p">:</span>
        <span class="n">cluster_pods</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">pods</span> <span class="k">if</span> <span class="n">p</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="n">cluster</span><span class="p">)]</span>
        <span class="n">total_pods</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">cluster_pods</span><span class="p">)</span>
        <span class="n">running_pods</span> <span class="o">=</span> <span class="nb">len</span><span class="p">([</span><span class="n">p</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">cluster_pods</span> <span class="k">if</span> <span class="n">p</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;Running&quot;</span><span class="p">])</span>
        <span class="k">if</span> <span class="n">running_pods</span> <span class="o">==</span> <span class="n">total_pods</span> <span class="ow">and</span> <span class="n">running_pods</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">clusters_running</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">break</span>
    <span class="k">return</span> <span class="n">clusters_running</span>


<span class="k">def</span><span class="w"> </span><span class="nf">get_ray_address</span><span class="p">(</span><span class="n">head_pod</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">namespace</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;default&quot;</span><span class="p">,</span> <span class="n">ray_head_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;head&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Given a cluster head pod, check its logs, which should include the ray address which can accept job requests.</span>

<span class="sd">    Args:</span>
<span class="sd">        head_pod (str): The name of the head pod.</span>
<span class="sd">        namespace (str, optional): The Kubernetes namespace. Defaults to &quot;default&quot;.</span>
<span class="sd">        ray_head_name (str, optional): The name of the ray head container. Defaults to &quot;head&quot;.</span>

<span class="sd">    Returns:</span>
<span class="sd">        str: The ray address if found, None otherwise.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the logs cannot be retrieved or the ray address is not found.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">cmd</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;kubectl&quot;</span><span class="p">,</span> <span class="s2">&quot;logs&quot;</span><span class="p">,</span> <span class="n">head_pod</span><span class="p">,</span> <span class="s2">&quot;-c&quot;</span><span class="p">,</span> <span class="n">ray_head_name</span><span class="p">,</span> <span class="s2">&quot;-n&quot;</span><span class="p">,</span> <span class="n">namespace</span><span class="p">]</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">subprocess</span><span class="o">.</span><span class="n">check_output</span><span class="p">(</span><span class="n">cmd</span><span class="p">)</span><span class="o">.</span><span class="n">decode</span><span class="p">()</span>
    <span class="k">except</span> <span class="n">subprocess</span><span class="o">.</span><span class="n">CalledProcessError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Could not enter head container with cmd </span><span class="si">{</span><span class="n">cmd</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">Perhaps try a different namespace or ray head name.&quot;</span>
        <span class="p">)</span>
    <span class="n">match</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;RAY_ADDRESS=&#39;([^&#39;]+)&#39;&quot;</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">match</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">match</span><span class="o">.</span><span class="n">group</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">None</span>


<span class="k">def</span><span class="w"> </span><span class="nf">process_cluster</span><span class="p">(</span><span class="n">cluster_info</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">ray_head_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;head&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    For each cluster, check that it is running, and get the Ray head address that will accept jobs.</span>

<span class="sd">    Args:</span>
<span class="sd">        cluster_info (dict): A dictionary containing cluster information with keys &#39;cluster&#39;, &#39;pods&#39;, and &#39;namespace&#39;.</span>
<span class="sd">        ray_head_name (str, optional): The name of the ray head container. Defaults to &quot;head&quot;.</span>

<span class="sd">    Returns:</span>
<span class="sd">        str: A string containing the cluster name and its Ray head address, or an error message if the head pod or Ray address is not found.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">cluster</span><span class="p">,</span> <span class="n">pods</span><span class="p">,</span> <span class="n">namespace</span> <span class="o">=</span> <span class="n">cluster_info</span>
    <span class="n">head_pod</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">for</span> <span class="n">pod_name</span><span class="p">,</span> <span class="n">status</span> <span class="ow">in</span> <span class="n">pods</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">pod_name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="n">cluster</span> <span class="o">+</span> <span class="s2">&quot;-head&quot;</span><span class="p">):</span>
            <span class="n">head_pod</span> <span class="o">=</span> <span class="n">pod_name</span>
            <span class="k">break</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">head_pod</span><span class="p">:</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;Error: Could not find head pod for cluster </span><span class="si">{</span><span class="n">cluster</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span>

    <span class="c1"># Get RAY_ADDRESS and status</span>
    <span class="n">ray_address</span> <span class="o">=</span> <span class="n">get_ray_address</span><span class="p">(</span><span class="n">head_pod</span><span class="p">,</span> <span class="n">namespace</span><span class="o">=</span><span class="n">namespace</span><span class="p">,</span> <span class="n">ray_head_name</span><span class="o">=</span><span class="n">ray_head_name</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">ray_address</span><span class="p">:</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;Error: Could not find RAY_ADDRESS for cluster </span><span class="si">{</span><span class="n">cluster</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span>

    <span class="c1"># Return only cluster and ray address</span>
    <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;name: </span><span class="si">{</span><span class="n">cluster</span><span class="si">}</span><span class="s2"> address: </span><span class="si">{</span><span class="n">ray_address</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span>


<span class="k">def</span><span class="w"> </span><span class="nf">main</span><span class="p">():</span>
    <span class="c1"># Parse command-line arguments</span>
    <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">&quot;Process Ray clusters and save their specifications.&quot;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--prefix&quot;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&quot;isaacray&quot;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The prefix for the cluster names.&quot;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--output&quot;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&quot;~/.cluster_config&quot;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The file to save cluster specifications.&quot;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--ray_head_name&quot;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&quot;head&quot;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The metadata name for the ray head container&quot;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--namespace&quot;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Kubernetes namespace to use. If not provided, will detect from current context.&quot;</span>
    <span class="p">)</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

    <span class="c1"># Get namespace from args or detect it</span>
    <span class="n">current_namespace</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">namespace</span> <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">namespace</span> <span class="k">else</span> <span class="n">get_namespace</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Using namespace: </span><span class="si">{</span><span class="n">current_namespace</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="n">cluster_name_prefix</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">prefix</span>
    <span class="n">cluster_spec_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">expanduser</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">output</span><span class="p">)</span>

    <span class="c1"># Get all pods</span>
    <span class="n">pods</span> <span class="o">=</span> <span class="n">get_pods</span><span class="p">(</span><span class="n">namespace</span><span class="o">=</span><span class="n">current_namespace</span><span class="p">)</span>

    <span class="c1"># Get clusters</span>
    <span class="n">clusters</span> <span class="o">=</span> <span class="n">get_clusters</span><span class="p">(</span><span class="n">pods</span><span class="p">,</span> <span class="n">cluster_name_prefix</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">clusters</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;No clusters found with prefix </span><span class="si">{</span><span class="n">cluster_name_prefix</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">return</span>

    <span class="c1"># Wait for clusters to be running</span>
    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="n">pods</span> <span class="o">=</span> <span class="n">get_pods</span><span class="p">(</span><span class="n">namespace</span><span class="o">=</span><span class="n">current_namespace</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">check_clusters_running</span><span class="p">(</span><span class="n">pods</span><span class="p">,</span> <span class="n">clusters</span><span class="p">):</span>
            <span class="k">break</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Waiting for all clusters to spin up...&quot;</span><span class="p">)</span>
        <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Checking for MLflow:&quot;</span><span class="p">)</span>
    <span class="c1"># Check MLflow status for each cluster</span>
    <span class="k">for</span> <span class="n">cluster</span> <span class="ow">in</span> <span class="n">clusters</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">mlflow_address</span> <span class="o">=</span> <span class="n">get_mlflow_info</span><span class="p">(</span><span class="n">current_namespace</span><span class="p">,</span> <span class="n">cluster</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;MLflow address for </span><span class="si">{</span><span class="n">cluster</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">mlflow_address</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">ValueError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ML Flow not located: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>

    <span class="c1"># Prepare cluster info for parallel processing</span>
    <span class="n">cluster_infos</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">cluster</span> <span class="ow">in</span> <span class="n">clusters</span><span class="p">:</span>
        <span class="n">cluster_pods</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">pods</span> <span class="k">if</span> <span class="n">p</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="n">cluster</span><span class="p">)]</span>
        <span class="n">cluster_infos</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">cluster</span><span class="p">,</span> <span class="n">cluster_pods</span><span class="p">,</span> <span class="n">current_namespace</span><span class="p">))</span>

    <span class="c1"># Use ThreadPoolExecutor to process clusters in parallel</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">results_lock</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">Lock</span><span class="p">()</span>

    <span class="k">with</span> <span class="n">ThreadPoolExecutor</span><span class="p">()</span> <span class="k">as</span> <span class="n">executor</span><span class="p">:</span>
        <span class="n">future_to_cluster</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">executor</span><span class="o">.</span><span class="n">submit</span><span class="p">(</span><span class="n">process_cluster</span><span class="p">,</span> <span class="n">info</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">ray_head_name</span><span class="p">):</span> <span class="n">info</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">info</span> <span class="ow">in</span> <span class="n">cluster_infos</span>
        <span class="p">}</span>
        <span class="k">for</span> <span class="n">future</span> <span class="ow">in</span> <span class="n">as_completed</span><span class="p">(</span><span class="n">future_to_cluster</span><span class="p">):</span>
            <span class="n">cluster_name</span> <span class="o">=</span> <span class="n">future_to_cluster</span><span class="p">[</span><span class="n">future</span><span class="p">]</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">result</span> <span class="o">=</span> <span class="n">future</span><span class="o">.</span><span class="n">result</span><span class="p">()</span>
                <span class="k">with</span> <span class="n">results_lock</span><span class="p">:</span>
                    <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">exc</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">cluster_name</span><span class="si">}</span><span class="s2"> generated an exception: </span><span class="si">{</span><span class="n">exc</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Sort results alphabetically by cluster name</span>
    <span class="n">results</span><span class="o">.</span><span class="n">sort</span><span class="p">()</span>

    <span class="c1"># Write sorted results to the output file (Ray info only)</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">cluster_spec_file</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">results</span><span class="p">:</span>
            <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Cluster spec information saved to </span><span class="si">{</span><span class="n">cluster_spec_file</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="c1"># Display the contents of the config file</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">cluster_spec_file</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">())</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details></section>
<section id="ray-clusters-only-without-kubernetes">
<h3><a class="toc-backref" href="#id9" role="doc-backlink">Ray Clusters Only (Without Kubernetes)</a><a class="headerlink" href="#ray-clusters-only-without-kubernetes" title="Permalink to this heading">#</a></h3>
<p>1.) Verify cluster access.</p>
<p>2.) Create a <code class="docutils literal notranslate"><span class="pre">~/.cluster_config</span></code> file, where <code class="docutils literal notranslate"><span class="pre">name:</span> <span class="pre">&lt;NAME&gt;</span> <span class="pre">address:</span> <span class="pre">http://&lt;IP&gt;:&lt;PORT&gt;</span></code> is on
a new line for each unique cluster. For one cluster, there should only be one line in this file.</p>
<p>3.) Start an MLFLow Server to receive the logs that the ray cluster has access to,
and determine the server URI.</p>
</section>
<section id="dispatching-steps-shared-between-kuberay-and-pure-ray-part-ii">
<h3><a class="toc-backref" href="#id10" role="doc-backlink">Dispatching Steps Shared Between KubeRay and Pure Ray Part II</a><a class="headerlink" href="#dispatching-steps-shared-between-kuberay-and-pure-ray-part-ii" title="Permalink to this heading">#</a></h3>
<p>1.) Test that your cluster is operational with the following.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Test that NVIDIA GPUs are visible and that Ray is operation with the following command:</span>
python3<span class="w"> </span>scripts/reinforcement_learning/ray/submit_job.py<span class="w"> </span>--aggregate_jobs<span class="w"> </span>wrap_resources.py<span class="w"> </span>--test
</pre></div>
</div>
<p>2.) Submitting tuning and/or resource-wrapped jobs is described in the <code class="file docutils literal notranslate"><span class="pre">submit_job.py</span></code> file.</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-icon"><svg version="1.1" width="1.0em" height="1.0em" class="sd-octicon sd-octicon-code" viewBox="0 0 16 16" aria-hidden="true"><path d="m11.28 3.22 4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734L13.94 8l-3.72-3.72a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215Zm-6.56 0a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042L2.06 8l3.72 3.72a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L.47 8.53a.75.75 0 0 1 0-1.06Z"></path></svg></span><span class="sd-summary-text">scripts/reinforcement_learning/ray/submit_job.py</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Copyright (c) 2022-2025, The Isaac Lab Project Developers.</span>
<span class="c1"># All rights reserved.</span>
<span class="c1">#</span>
<span class="c1"># SPDX-License-Identifier: BSD-3-Clause</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">argparse</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">concurrent.futures</span><span class="w"> </span><span class="kn">import</span> <span class="n">ThreadPoolExecutor</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">ray</span><span class="w"> </span><span class="kn">import</span> <span class="n">job_submission</span>
<span class="hll">
</span><span class="hll"><span class="sd">&quot;&quot;&quot;</span>
</span><span class="hll"><span class="sd">This script submits aggregate job(s) to cluster(s) described in a</span>
</span><span class="hll"><span class="sd">config file containing ``name: &lt;NAME&gt; address: http://&lt;IP&gt;:&lt;PORT&gt;`` on</span>
</span><span class="hll"><span class="sd">a new line for each cluster. For KubeRay clusters, this file</span>
</span><span class="hll"><span class="sd">can be automatically created with :file:`grok_cluster_with_kubectl.py`</span>
</span><span class="hll">
</span><span class="hll"><span class="sd">Aggregate job(s) are matched with cluster(s) via the following relation:</span>
</span><span class="hll"><span class="sd">cluster_line_index_submitted_to = job_index % total_cluster_count</span>
</span><span class="hll">
</span><span class="hll"><span class="sd">Aggregate jobs are separated by the * delimiter. The ``--aggregate_jobs`` argument must be</span>
</span><span class="hll"><span class="sd">the last argument supplied to the script.</span>
</span><span class="hll">
</span><span class="hll"><span class="sd">An aggregate job could be a :file:`../tuner.py` tuning job, which automatically</span>
</span><span class="hll"><span class="sd">creates several individual jobs when started on a cluster. Alternatively, an aggregate job</span>
</span><span class="hll"><span class="sd">could be a :file:&#39;../wrap_resources.py` resource-wrapped job,</span>
</span><span class="hll"><span class="sd">which may contain several individual sub-jobs separated by</span>
</span><span class="hll"><span class="sd">the + delimiter.</span>
</span><span class="hll">
</span><span class="hll"><span class="sd">If there are more aggregate jobs than cluster(s), aggregate jobs will be submitted</span>
</span><span class="hll"><span class="sd">as clusters become available via the defined relation above. If there are less aggregate job(s)</span>
</span><span class="hll"><span class="sd">than clusters, some clusters will not receive aggregate job(s). The maximum number of</span>
</span><span class="hll"><span class="sd">aggregate jobs that can be run simultaneously is equal to the number of workers created by</span>
</span><span class="hll"><span class="sd">default by a ThreadPoolExecutor on the machine submitting jobs due to fetching the log output after</span>
</span><span class="hll"><span class="sd">jobs finish, which is unlikely to constrain overall-job submission.</span>
</span><span class="hll">
</span><span class="hll"><span class="sd">Usage:</span>
</span><span class="hll">
</span><span class="hll"><span class="sd">.. code-block:: bash</span>
</span><span class="hll">
</span><span class="hll"><span class="sd">    # Example; submitting a tuning job</span>
</span><span class="hll"><span class="sd">    python3 scripts/reinforcement_learning/ray/submit_job.py \</span>
</span><span class="hll"><span class="sd">    --aggregate_jobs /workspace/isaaclab/scripts/reinforcement_learning/ray/tuner.py \</span>
</span><span class="hll"><span class="sd">        --cfg_file hyperparameter_tuning/vision_cartpole_cfg.py \</span>
</span><span class="hll"><span class="sd">        --cfg_class CartpoleTheiaJobCfg --mlflow_uri &lt;ML_FLOW_URI&gt;</span>
</span><span class="hll">
</span><span class="hll"><span class="sd">    # Example: Submitting resource wrapped job</span>
</span><span class="hll"><span class="sd">    python3 scripts/reinforcement_learning/ray/submit_job.py --aggregate_jobs wrap_resources.py --test</span>
</span><span class="hll">
</span><span class="hll"><span class="sd">    # For all command line arguments</span>
</span><span class="hll"><span class="sd">    python3 scripts/reinforcement_learning/ray/submit_job.py -h</span>
</span><span class="hll"><span class="sd">&quot;&quot;&quot;</span>
</span><span class="n">script_directory</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="vm">__file__</span><span class="p">))</span>
<span class="n">CONFIG</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;working_dir&quot;</span><span class="p">:</span> <span class="n">script_directory</span><span class="p">,</span> <span class="s2">&quot;executable&quot;</span><span class="p">:</span> <span class="s2">&quot;/workspace/isaaclab/isaaclab.sh -p&quot;</span><span class="p">}</span>


<span class="k">def</span><span class="w"> </span><span class="nf">read_cluster_spec</span><span class="p">(</span><span class="n">fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">dict</span><span class="p">]:</span>
    <span class="k">if</span> <span class="n">fn</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">cluster_spec_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">expanduser</span><span class="p">(</span><span class="s2">&quot;~/.cluster_config&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">cluster_spec_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">expanduser</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">cluster_spec_path</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">FileNotFoundError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Cluster spec file not found at </span><span class="si">{</span><span class="n">cluster_spec_path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="n">clusters</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">cluster_spec_path</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">parts</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">)</span>
            <span class="n">http_address</span> <span class="o">=</span> <span class="n">parts</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
            <span class="n">cluster_info</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="n">parts</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;address&quot;</span><span class="p">:</span> <span class="n">http_address</span><span class="p">}</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[INFO] Setting </span><span class="si">{</span><span class="n">cluster_info</span><span class="p">[</span><span class="s1">&#39;name&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># with {cluster_info[&#39;num_gpu&#39;]} GPUs.&quot;)</span>
            <span class="n">clusters</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cluster_info</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">clusters</span>


<span class="k">def</span><span class="w"> </span><span class="nf">submit_job</span><span class="p">(</span><span class="n">cluster</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">job_command</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Submits a job to a single cluster, prints the final result and Ray dashboard URL at the end.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">address</span> <span class="o">=</span> <span class="n">cluster</span><span class="p">[</span><span class="s2">&quot;address&quot;</span><span class="p">]</span>
    <span class="n">cluster_name</span> <span class="o">=</span> <span class="n">cluster</span><span class="p">[</span><span class="s2">&quot;name&quot;</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[INFO]: Submitting job to cluster &#39;</span><span class="si">{</span><span class="n">cluster_name</span><span class="si">}</span><span class="s2">&#39; at </span><span class="si">{</span><span class="n">address</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># with {num_gpus} GPUs.&quot;)</span>
    <span class="n">client</span> <span class="o">=</span> <span class="n">job_submission</span><span class="o">.</span><span class="n">JobSubmissionClient</span><span class="p">(</span><span class="n">address</span><span class="p">)</span>
    <span class="n">runtime_env</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;working_dir&quot;</span><span class="p">:</span> <span class="n">CONFIG</span><span class="p">[</span><span class="s2">&quot;working_dir&quot;</span><span class="p">],</span> <span class="s2">&quot;executable&quot;</span><span class="p">:</span> <span class="n">CONFIG</span><span class="p">[</span><span class="s2">&quot;executable&quot;</span><span class="p">]}</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[INFO]: Checking contents of the directory: </span><span class="si">{</span><span class="n">CONFIG</span><span class="p">[</span><span class="s1">&#39;working_dir&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">dir_contents</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">CONFIG</span><span class="p">[</span><span class="s2">&quot;working_dir&quot;</span><span class="p">])</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[INFO]: Directory contents: </span><span class="si">{</span><span class="n">dir_contents</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[INFO]: Failed to list directory contents: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">entrypoint</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">CONFIG</span><span class="p">[</span><span class="s1">&#39;executable&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">job_command</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[INFO]: Attempting entrypoint </span><span class="si">{</span><span class="n">entrypoint</span><span class="si">=}</span><span class="s2"> in cluster </span><span class="si">{</span><span class="n">cluster</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">job_id</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">submit_job</span><span class="p">(</span><span class="n">entrypoint</span><span class="o">=</span><span class="n">entrypoint</span><span class="p">,</span> <span class="n">runtime_env</span><span class="o">=</span><span class="n">runtime_env</span><span class="p">)</span>
    <span class="n">status</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">get_job_status</span><span class="p">(</span><span class="n">job_id</span><span class="p">)</span>
    <span class="k">while</span> <span class="n">status</span> <span class="ow">in</span> <span class="p">[</span><span class="n">job_submission</span><span class="o">.</span><span class="n">JobStatus</span><span class="o">.</span><span class="n">PENDING</span><span class="p">,</span> <span class="n">job_submission</span><span class="o">.</span><span class="n">JobStatus</span><span class="o">.</span><span class="n">RUNNING</span><span class="p">]:</span>
        <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
        <span class="n">status</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">get_job_status</span><span class="p">(</span><span class="n">job_id</span><span class="p">)</span>

    <span class="n">final_logs</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">get_job_logs</span><span class="p">(</span><span class="n">job_id</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;----------------------------------------------------&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[INFO]: Cluster </span><span class="si">{</span><span class="n">cluster_name</span><span class="si">}</span><span class="s2"> Logs: </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">final_logs</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;----------------------------------------------------&quot;</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">submit_jobs_to_clusters</span><span class="p">(</span><span class="n">jobs</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">clusters</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">dict</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Submit all jobs to their respective clusters, cycling through clusters if there are more jobs than clusters.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">clusters</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;No clusters available for job submission.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">jobs</span><span class="p">)</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">clusters</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;[INFO]: Less jobs than clusters, some clusters will not receive jobs&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">jobs</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">clusters</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;[INFO]: Exactly one job per cluster&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;[INFO]: More jobs than clusters, jobs submitted as clusters become available.&quot;</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">ThreadPoolExecutor</span><span class="p">()</span> <span class="k">as</span> <span class="n">executor</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">job_command</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">jobs</span><span class="p">):</span>
            <span class="c1"># Cycle through clusters using modulus to wrap around if there are more jobs than clusters</span>
            <span class="n">cluster</span> <span class="o">=</span> <span class="n">clusters</span><span class="p">[</span><span class="n">idx</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">clusters</span><span class="p">)]</span>
            <span class="n">executor</span><span class="o">.</span><span class="n">submit</span><span class="p">(</span><span class="n">submit_job</span><span class="p">,</span> <span class="n">cluster</span><span class="p">,</span> <span class="n">job_command</span><span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">&quot;Submit multiple GPU jobs to multiple Ray clusters.&quot;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--config_file&quot;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&quot;~/.cluster_config&quot;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;The cluster config path.&quot;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--aggregate_jobs&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
        <span class="n">nargs</span><span class="o">=</span><span class="n">argparse</span><span class="o">.</span><span class="n">REMAINDER</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;This should be last argument. The aggregate jobs to submit separated by the * delimiter.&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">aggregate_jobs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">jobs</span> <span class="o">=</span> <span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">aggregate_jobs</span><span class="p">)</span>
        <span class="n">formatted_jobs</span> <span class="o">=</span> <span class="n">jobs</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;*&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">formatted_jobs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Warning; Split jobs by cluster with the * delimiter&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">formatted_jobs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[INFO]: Isaac Ray Wrapper received jobs </span><span class="si">{</span><span class="n">formatted_jobs</span><span class="si">=}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">clusters</span> <span class="o">=</span> <span class="n">read_cluster_spec</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">config_file</span><span class="p">)</span>
    <span class="n">submit_jobs_to_clusters</span><span class="p">(</span><span class="n">formatted_jobs</span><span class="p">,</span> <span class="n">clusters</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details><p>3.) For tuning jobs, specify the tuning job / hyperparameter sweep as a <code class="xref py py-class docutils literal notranslate"><span class="pre">JobCfg</span></code> .
The included <code class="xref py py-class docutils literal notranslate"><span class="pre">JobCfg</span></code> only supports the <code class="docutils literal notranslate"><span class="pre">rl_games</span></code> workflow due to differences in
environment entrypoints and hydra arguments, although other workflows will work if provided a compatible
<code class="xref py py-class docutils literal notranslate"><span class="pre">JobCfg</span></code>.</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-icon"><svg version="1.1" width="1.0em" height="1.0em" class="sd-octicon sd-octicon-code" viewBox="0 0 16 16" aria-hidden="true"><path d="m11.28 3.22 4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734L13.94 8l-3.72-3.72a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215Zm-6.56 0a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042L2.06 8l3.72 3.72a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L.47 8.53a.75.75 0 0 1 0-1.06Z"></path></svg></span><span class="sd-summary-text">scripts/reinforcement_learning/ray/tuner.py (JobCfg definition)</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">JobCfg</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;To be compatible with :meth: invoke_tuning_run and :class:IsaacLabTuneTrainable,</span>
<span class="sd">    at a minimum, the tune job should inherit from this class.&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">:</span> <span class="nb">dict</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Runner args include command line arguments passed to the task.</span>
<span class="sd">        For example:</span>
<span class="sd">        cfg[&quot;runner_args&quot;][&quot;headless_singleton&quot;] = &quot;--headless&quot;</span>
<span class="sd">        cfg[&quot;runner_args&quot;][&quot;enable_cameras_singleton&quot;] = &quot;--enable_cameras&quot;</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="s2">&quot;runner_args&quot;</span> <span class="ow">in</span> <span class="n">cfg</span><span class="p">,</span> <span class="s2">&quot;No runner arguments specified.&quot;</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Task is the desired task to train on. For example:</span>
<span class="sd">        cfg[&quot;runner_args&quot;][&quot;--task&quot;] = tune.choice([&quot;Isaac-Cartpole-RGB-TheiaTiny-v0&quot;])</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="s2">&quot;--task&quot;</span> <span class="ow">in</span> <span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;runner_args&quot;</span><span class="p">],</span> <span class="s2">&quot;No task specified.&quot;</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Hydra args define the hyperparameters varied within the sweep. For example:</span>
<span class="sd">        cfg[&quot;hydra_args&quot;][&quot;agent.params.network.cnn.activation&quot;] = tune.choice([&quot;relu&quot;, &quot;elu&quot;])</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="s2">&quot;hydra_args&quot;</span> <span class="ow">in</span> <span class="n">cfg</span><span class="p">,</span> <span class="s2">&quot;No hyperparameters specified.&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cfg</span> <span class="o">=</span> <span class="n">cfg</span>
</pre></div>
</div>
</div>
</details><p>For example, see the following Cartpole Example configurations.</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-icon"><svg version="1.1" width="1.0em" height="1.0em" class="sd-octicon sd-octicon-code" viewBox="0 0 16 16" aria-hidden="true"><path d="m11.28 3.22 4.25 4.25a.75.75 0 0 1 0 1.06l-4.25 4.25a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734L13.94 8l-3.72-3.72a.749.749 0 0 1 .326-1.275.749.749 0 0 1 .734.215Zm-6.56 0a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042L2.06 8l3.72 3.72a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L.47 8.53a.75.75 0 0 1 0-1.06Z"></path></svg></span><span class="sd-summary-text">scripts/reinforcement_learning/ray/hyperparameter_tuning/vision_cartpole_cfg.py</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Copyright (c) 2022-2025, The Isaac Lab Project Developers.</span>
<span class="c1"># All rights reserved.</span>
<span class="c1">#</span>
<span class="c1"># SPDX-License-Identifier: BSD-3-Clause</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pathlib</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sys</span>

<span class="c1"># Allow for import of items from the ray workflow.</span>
<span class="n">CUR_DIR</span> <span class="o">=</span> <span class="n">pathlib</span><span class="o">.</span><span class="n">Path</span><span class="p">(</span><span class="vm">__file__</span><span class="p">)</span><span class="o">.</span><span class="n">parent</span>
<span class="n">UTIL_DIR</span> <span class="o">=</span> <span class="n">CUR_DIR</span><span class="o">.</span><span class="n">parent</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="nb">str</span><span class="p">(</span><span class="n">UTIL_DIR</span><span class="p">),</span> <span class="nb">str</span><span class="p">(</span><span class="n">CUR_DIR</span><span class="p">)])</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">util</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">vision_cfg</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ray</span><span class="w"> </span><span class="kn">import</span> <span class="n">tune</span>


<span class="k">class</span><span class="w"> </span><span class="nc">CartpoleRGBNoTuneJobCfg</span><span class="p">(</span><span class="n">vision_cfg</span><span class="o">.</span><span class="n">CameraJobCfg</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="p">{}):</span>
        <span class="n">cfg</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">populate_isaac_ray_cfg_args</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>
        <span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;runner_args&quot;</span><span class="p">][</span><span class="s2">&quot;--task&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">tune</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="s2">&quot;Isaac-Cartpole-RGB-v0&quot;</span><span class="p">])</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">cfg</span><span class="p">,</span> <span class="n">vary_env_count</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">vary_cnn</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">vary_mlp</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>


<span class="k">class</span><span class="w"> </span><span class="nc">CartpoleRGBCNNOnlyJobCfg</span><span class="p">(</span><span class="n">vision_cfg</span><span class="o">.</span><span class="n">CameraJobCfg</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="p">{}):</span>
        <span class="n">cfg</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">populate_isaac_ray_cfg_args</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>
        <span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;runner_args&quot;</span><span class="p">][</span><span class="s2">&quot;--task&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">tune</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="s2">&quot;Isaac-Cartpole-RGB-v0&quot;</span><span class="p">])</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">cfg</span><span class="p">,</span> <span class="n">vary_env_count</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">vary_cnn</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">vary_mlp</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>


<span class="k">class</span><span class="w"> </span><span class="nc">CartpoleRGBJobCfg</span><span class="p">(</span><span class="n">vision_cfg</span><span class="o">.</span><span class="n">CameraJobCfg</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="p">{}):</span>
        <span class="n">cfg</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">populate_isaac_ray_cfg_args</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>
        <span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;runner_args&quot;</span><span class="p">][</span><span class="s2">&quot;--task&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">tune</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="s2">&quot;Isaac-Cartpole-RGB-v0&quot;</span><span class="p">])</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">cfg</span><span class="p">,</span> <span class="n">vary_env_count</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">vary_cnn</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">vary_mlp</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>


<span class="k">class</span><span class="w"> </span><span class="nc">CartpoleResNetJobCfg</span><span class="p">(</span><span class="n">vision_cfg</span><span class="o">.</span><span class="n">ResNetCameraJob</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="p">{}):</span>
        <span class="n">cfg</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">populate_isaac_ray_cfg_args</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>
        <span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;runner_args&quot;</span><span class="p">][</span><span class="s2">&quot;--task&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">tune</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="s2">&quot;Isaac-Cartpole-RGB-ResNet18-v0&quot;</span><span class="p">])</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>


<span class="k">class</span><span class="w"> </span><span class="nc">CartpoleTheiaJobCfg</span><span class="p">(</span><span class="n">vision_cfg</span><span class="o">.</span><span class="n">TheiaCameraJob</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="p">{}):</span>
        <span class="n">cfg</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">populate_isaac_ray_cfg_args</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>
        <span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;runner_args&quot;</span><span class="p">][</span><span class="s2">&quot;--task&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">tune</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="s2">&quot;Isaac-Cartpole-RGB-TheiaTiny-v0&quot;</span><span class="p">])</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details><p>To view the tuning results, view the MLFlow dashboard of the server that you created.
For KubeRay, this can be done through port forwarding the MLFlow dashboard with the following.</p>
<p><code class="docutils literal notranslate"><span class="pre">kubectl</span> <span class="pre">port-forward</span> <span class="pre">service/isaacray-mlflow</span> <span class="pre">5000:5000</span></code></p>
<p>Then visit the following address in a browser.</p>
<p><code class="docutils literal notranslate"><span class="pre">localhost:5000</span></code></p>
<p>If the MLFlow port is forwarded like above, it can be converted into tensorboard logs with
this following command.</p>
<p><code class="docutils literal notranslate"><span class="pre">./isaaclab.sh</span> <span class="pre">-p</span> <span class="pre">scripts/reinforcement_learning/ray/mlflow_to_local_tensorboard.py</span> <span class="pre">\</span>
<span class="pre">--uri</span> <span class="pre">http://localhost:5000</span> <span class="pre">--experiment-name</span> <span class="pre">IsaacRay-&lt;CLASS_JOB_CFG&gt;-tune</span> <span class="pre">--download-dir</span> <span class="pre">test</span></code></p>
<section id="kubernetes-cluster-cleanup">
<h4><a class="toc-backref" href="#id11" role="doc-backlink">Kubernetes Cluster Cleanup</a><a class="headerlink" href="#kubernetes-cluster-cleanup" title="Permalink to this heading">#</a></h4>
<p>For the sake of conserving resources, and potentially freeing precious GPU resources for other people to use
on shared compute platforms, please destroy the Ray cluster after use. They can be easily
recreated! For KubeRay clusters, this can be done as follows.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>kubectl<span class="w"> </span>get<span class="w"> </span>raycluster<span class="w"> </span><span class="p">|</span><span class="w"> </span>egrep<span class="w"> </span><span class="s1">&#39;isaacray&#39;</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>awk<span class="w"> </span><span class="s1">&#39;{print $1}&#39;</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>xargs<span class="w"> </span>kubectl<span class="w"> </span>delete<span class="w"> </span>raycluster<span class="w"> </span><span class="o">&amp;&amp;</span>
kubectl<span class="w"> </span>get<span class="w"> </span>deployments<span class="w"> </span><span class="p">|</span><span class="w"> </span>egrep<span class="w"> </span><span class="s1">&#39;mlflow&#39;</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>awk<span class="w"> </span><span class="s1">&#39;{print $1}&#39;</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>xargs<span class="w"> </span>kubectl<span class="w"> </span>delete<span class="w"> </span>deployment<span class="w"> </span><span class="o">&amp;&amp;</span>
kubectl<span class="w"> </span>get<span class="w"> </span>services<span class="w"> </span><span class="p">|</span><span class="w"> </span>egrep<span class="w"> </span><span class="s1">&#39;mlflow&#39;</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>awk<span class="w"> </span><span class="s1">&#39;{print $1}&#39;</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>xargs<span class="w"> </span>kubectl<span class="w"> </span>delete<span class="w"> </span>service<span class="w"> </span><span class="o">&amp;&amp;</span>
kubectl<span class="w"> </span>get<span class="w"> </span>services<span class="w"> </span><span class="p">|</span><span class="w"> </span>egrep<span class="w"> </span><span class="s1">&#39;isaacray&#39;</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>awk<span class="w"> </span><span class="s1">&#39;{print $1}&#39;</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>xargs<span class="w"> </span>kubectl<span class="w"> </span>delete<span class="w"> </span>service
</pre></div>
</div>
</section>
</section>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="multi_gpu.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Multi-GPU and Multi-Node Training</p>
      </div>
    </a>
    <a class="right-next"
       href="reproducibility.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Reproducibility and Determinism</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#docker-based-local-quickstart">Docker-based Local Quickstart</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#remote-clusters">Remote Clusters</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#kuberay-setup">KubeRay Setup</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ray-clusters-without-kubernetes-setup">Ray Clusters (Without Kubernetes) Setup</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#shared-steps-between-kuberay-and-pure-ray-part-i">Shared Steps Between KubeRay and Pure Ray Part I</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#kuberay-clusters-only">KubeRay Clusters Only</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ray-clusters-only-without-kubernetes">Ray Clusters Only (Without Kubernetes)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dispatching-steps-shared-between-kuberay-and-pure-ray-part-ii">Dispatching Steps Shared Between KubeRay and Pure Ray Part II</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#kubernetes-cluster-cleanup">Kubernetes Cluster Cleanup</a></li>
</ul>
</li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By The Isaac Lab Project Developers.
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2022-2025, The Isaac Lab Project Developers..
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    <p class="last-updated">
  Last updated on Oct 17, 2025.
  <br/>
</p>
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>